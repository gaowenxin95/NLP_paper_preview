@article{radford2018improving,
  added-at = {2020-07-14T16:49:49.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {},
  timestamp = {2020-07-14T16:49:49.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}
@article{2017Attention,
  title={Attention Is All You Need},
  author={ Vaswani, Ashish  and  Shazeer, Noam  and  Parmar, Niki  and  Uszkoreit, Jakob  and  Jones, Llion  and  Gomez, Aidan N  and  Kaiser, Lukasz  and  Polosukhin, Illia },
  journal={arXiv},
  year={2017},
}
@online{DASOU,
url={https://www.bilibili.com/video/BV1Di4y1c7Zm?p=7},
title = {TRANSFORMER详解},
year=2021
}
@online{sliderSun,
url={https://blog.csdn.net/weixin_37947156/article/details/93035607},
title = {xlnet详解},
year=2019
}
@misc{yang2019xlnet,
  added-at = {2020-04-16T10:35:49.000+0200},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  biburl = {https://www.bibsonomy.org/bibtex/2b758258da935db4bc1a57b5f6c9d94c6/meinerscl},
  description = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  interhash = {cd85caa3241071a53ea5c86eadae8de8},
  intrahash = {b758258da935db4bc1a57b5f6c9d94c6},
  keywords = {machine-learning sequence-to-sequence},
  note = {cite arxiv:1906.08237Comment: Pretrained models and code are available at  https://github.com/zihangdai/xlnet},
  timestamp = {2020-04-16T10:35:49.000+0200},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url = {http://arxiv.org/abs/1906.08237},
  year = 2019
}
@online{zhangjunlin,
url={https://zhuanlan.zhihu.com/p/70257427},
title = {xlnet与bert的异同},
year=2019
}
@article{2019ALBERT,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={ Lan, Zhenzhong  and  Chen, Mingda  and  Goodman, Sebastian  and  Gimpel, Kevin  and  Sharma, Piyush  and  Soricut, Radu },
  year={2019},
}
@article{2019RoBERTa,
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
year = {2019},
month = {07},
pages = {},
title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach}
}
@article{2019Exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={ Raffel, Colin  and  Shazeer, Noam  and  Roberts, Adam  and  Lee, Katherine  and  Narang, Sharan  and  Matena, Michael  and  Zhou, Yanqi  and  Li, Wei  and  Liu, Peter J. },
  year={2019},
}
@article{article,
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
year = {2015},
month = {03},
pages = {},
title = {Distilling the Knowledge in a Neural Network}
}
@article{2019Distilling,
  title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},
  author={ Tang, R.  and  Lu, Y.  and  Liu, L.  and  Mou, L.  and  Vechtomova, O.  and  Lin, J. },
  year={2019},
}

