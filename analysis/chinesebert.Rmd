专门针对中文做的bert的优化

### 论文总览

主要idea

>汉字的最大特性有两个方面：一是字形，二是拼音。汉字是一种典型的意音文字，从其起源来看，它的字形本身就蕴含了一部分语义。比如，“江河湖泊”都有偏旁三点水，这表明它们都与水有关。

>从汉字本身的这两大特性出发，将汉字的字形与拼音信息融入到中文语料的预训练过程。一个汉字的字形向量由多个不同的字体形成，而拼音向量则由对应的罗马化的拼音字符序列得到。二者与字向量一起进行融合，得到最终的融合向量，作为预训练模型的输入。模型使用全词掩码（Whole Word Masking）和字掩码（Character Masking）两种策略训练，使模型更加综合地建立汉字、字形、读音与上下文之间的联系


其实看了源码很容易理解，就是embedding的部分增加了pinyin embedding和字形embedding，与bert的三个embedding一起相加



![](figs/chinesebert-1.png)

![](figs/chinesebert-2.png)

### 增加全词掩码

全词mask就是以词为单位进行mask，字mask就是以字为单位


创新点只有embedding部分，因为全词掩码之前哈工大的bert-zh已经提出了