RoBERTa在idea上面没有什么创新，可以认为是bert调参到最优的工程化实现。

挑着重点的一些简单看下

### INTRODUCTION

>We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include:[@2019RoBERTa]

提出了一项BERT预训练的复制研究(Devlin等人，2019)，其中包括对超参数调整和训练集大小的影响的仔细评估。结果发现BERT明显训练不足，并提出了一种改进的训练BERT模型RoBERTa，它可以匹配或超过所有后BERT方法的性能。主要优化方式包括

>
- (1)training the model longer, with bigger batches, over more data; 
- (2) removing the next sentence prediction objective;
- (3) training on longer sequences; and 
- (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects.

- batch size更大了，数据集更大了
- 去掉了NSP任务
- 序列更长了
- 动态MASK

>
When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on $4 / 9$ of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT's masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019 ).

当控制训练数据时,们改进的训练程序改进了GLUE和SQuAD上发表的BERT结果。经过更长时间的额外数据训练后，ROBERTA模型在公共GLUE排行榜上获得了准确率达88.5，与Yang等人(2019)报告的88.4分相匹配。roberta模型在$4 / 9$的胶水任务:MNLI, QNLI, RTE和STS-B上建立了一个新的最先进的模型。总的来说，重新建立了BERT的掩蔽语言模型训练目标与其他最近提出的训练目标，如扰动自回归语言建模(Yang et al.， 2019)竞争。

>
In summary, the contributions of this paper are:
(1) We present a set of important BERT design choices and training strategies and introduce
alternatives that lead to better downstream taskperformance; 
(2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; 
(3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017 ).

这篇文章的主要贡献如下：
(1)提出了一套重要的BERT设计选择和训练策略，下游任务的效果更好
(2)使用了一个新的数据集CCNEWS，并证实使用更多的数据进行预训练可以进一步提高下游任务的性能;
(3)训练改进表明，在正确的设计选择下，掩蔽语言模型预训练与其他最近发表的方法相比具有竞争力。发布了在PyTorch中实现的模型、预训练和微调代码(Paszke等人，2017)。


**比较新的idea应该是动态MASK**


### MLM

>
Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects $15 \%$ of the input tokens for possible replacement. Of the selected tokens, $80 \%$ are replaced with $[M A S K], 10 \%$ are left unchanged,and $10 \%$ are replaced by a randomly selected vocabulary token.
In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1 ).

(MLM)在输入序列中随机选取一个标记样本，并用特殊标记[MASK]替换。MLM目标是用交叉熵损失预测被mask的token。BERT统一选择$15 \%$的输入token进行可能的替换。在所选择的token中，$80 \%$被替换为$[M A S K]， 10 \%$保持不变，$10 \%$被随机选择的词汇表token替换。在原来的实现中，随机屏蔽和替换在开始时只执行一次，并保存到训练过程中，但在实际操作中，由于数据是重复的，所以每个训练句子的屏蔽并不总是相同的(见4.1节)。


###  Static vs. Dynamic Masking

>
As discussed in Section $2,$ BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets.

BERT依赖于随机mask和预测token。原始的BERT实现在数据预处理过程中执行一次mask，从而产生单个静态mask。在roberta中为了避免在每个epoch中对每个训练实例使用相同的掩码，，每条训练数据被重复10次，使每个序列在40个epoch中以10种不同的方式被掩码。因此，在训练过程中，每个训练序列都要进行相同的mask四次。将此策略与动态掩蔽策略进行了比较，**动态掩蔽策略是在每次向模型输入序列时生成掩蔽模式**。这在为更多步骤或更大的数据集进行预训练时变得至关重要。

下图是对比的结果，三个任务中有两个动态mask的效果更好。

![](figs/ROBERTA_1.png)


