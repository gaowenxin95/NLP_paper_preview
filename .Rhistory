a图是每两层学习教师模型的一次输出叫PKD-Skip,b图是学生网络学习教师模型最后6层,也叫PKD-last，Trm：transformer。
Learning from the hidden states of all the tokens is computationally expensive, and may introduce noise. In the original BERT implementation (Devlin et al., 2018), prediction is performed by only using the output from the last layer's [CLS ]
install.packages("tinytex", lib="D:/R/R-3.6.2/library")
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
