<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 模型蒸馏 | NLP相关paper阅读</title>
  <meta name="description" content="2 模型蒸馏 | NLP相关paper阅读" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="2 模型蒸馏 | NLP相关paper阅读" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 模型蒸馏 | NLP相关paper阅读" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2021-05-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="nlp-paper-preview.html"/>

<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html"><i class="fa fa-check"></i><b>1</b> NLP paper preview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#word2vec"><i class="fa fa-check"></i><b>1.1</b> word2vec</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#cbow"><i class="fa fa-check"></i><b>1.1.1</b> CBOW</a></li>
<li class="chapter" data-level="1.1.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#skip-gram"><i class="fa fa-check"></i><b>1.1.2</b> skip-gram</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#fasttext"><i class="fa fa-check"></i><b>1.2</b> fasttext</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#hierarchical-softmax"><i class="fa fa-check"></i><b>1.2.1</b> Hierarchical softmax</a></li>
<li class="chapter" data-level="1.2.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#n_gram-features"><i class="fa fa-check"></i><b>1.2.2</b> N_gram features</a></li>
<li class="chapter" data-level="1.2.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#fasttext与word2vec对比"><i class="fa fa-check"></i><b>1.2.3</b> fasttext与word2vec对比</a></li>
<li class="chapter" data-level="1.2.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#fasttext与cbow有两点不同"><i class="fa fa-check"></i><b>1.2.4</b> fasttext与CBOW有两点不同</a></li>
<li class="chapter" data-level="1.2.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验和结果分析"><i class="fa fa-check"></i><b>1.2.5</b> 实验和结果分析</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#doc2vec"><i class="fa fa-check"></i><b>1.3</b> Doc2vec</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验对比"><i class="fa fa-check"></i><b>1.3.2</b> 实验对比</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#skip-thoughts"><i class="fa fa-check"></i><b>1.4</b> skip-thoughts</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#结构"><i class="fa fa-check"></i><b>1.4.2</b> 结构</a></li>
<li class="chapter" data-level="1.4.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#encoder"><i class="fa fa-check"></i><b>1.4.3</b> encoder</a></li>
<li class="chapter" data-level="1.4.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#decoder"><i class="fa fa-check"></i><b>1.4.4</b> decoder</a></li>
<li class="chapter" data-level="1.4.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#目标函数"><i class="fa fa-check"></i><b>1.4.5</b> 目标函数</a></li>
<li class="chapter" data-level="1.4.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#词典的拓展"><i class="fa fa-check"></i><b>1.4.6</b> 词典的拓展</a></li>
<li class="chapter" data-level="1.4.7" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验部分"><i class="fa fa-check"></i><b>1.4.7</b> 实验部分</a></li>
<li class="chapter" data-level="1.4.8" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验部分-1"><i class="fa fa-check"></i><b>1.4.8</b> 实验部分</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#elmo"><i class="fa fa-check"></i><b>1.5</b> ELMO</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-2"><i class="fa fa-check"></i><b>1.5.1</b> introduction</a></li>
<li class="chapter" data-level="1.5.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#双向语言模型"><i class="fa fa-check"></i><b>1.5.2</b> 双向语言模型</a></li>
<li class="chapter" data-level="1.5.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#elmo结构"><i class="fa fa-check"></i><b>1.5.3</b> elmo结构</a></li>
<li class="chapter" data-level="1.5.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#evaluation"><i class="fa fa-check"></i><b>1.5.4</b> Evaluation</a></li>
<li class="chapter" data-level="1.5.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#analysis"><i class="fa fa-check"></i><b>1.5.5</b> analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>1.6</b> Attention is all you need</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-3"><i class="fa fa-check"></i><b>1.6.1</b> introduction</a></li>
<li class="chapter" data-level="1.6.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#input"><i class="fa fa-check"></i><b>1.6.2</b> input</a></li>
<li class="chapter" data-level="1.6.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#encoder-1"><i class="fa fa-check"></i><b>1.6.3</b> encoder</a></li>
<li class="chapter" data-level="1.6.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#self-attention"><i class="fa fa-check"></i><b>1.6.4</b> self-attention</a></li>
<li class="chapter" data-level="1.6.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#multi-head-attention"><i class="fa fa-check"></i><b>1.6.5</b> Multi-head attention</a></li>
<li class="chapter" data-level="1.6.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#残差网络的结构"><i class="fa fa-check"></i><b>1.6.6</b> 残差网络的结构</a></li>
<li class="chapter" data-level="1.6.7" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#decoder-1"><i class="fa fa-check"></i><b>1.6.7</b> decoder</a></li>
<li class="chapter" data-level="1.6.8" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#decoder-2"><i class="fa fa-check"></i><b>1.6.8</b> decoder</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#gpt"><i class="fa fa-check"></i><b>1.7</b> GPT</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-4"><i class="fa fa-check"></i><b>1.7.1</b> introduction</a></li>
<li class="chapter" data-level="1.7.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#framework"><i class="fa fa-check"></i><b>1.7.2</b> Framework</a></li>
<li class="chapter" data-level="1.7.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#unsupervised-pre-training"><i class="fa fa-check"></i><b>1.7.3</b> Unsupervised pre-training</a></li>
<li class="chapter" data-level="1.7.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#supervised-ﬁne-tuning"><i class="fa fa-check"></i><b>1.7.4</b> Supervised ﬁne-tuning</a></li>
<li class="chapter" data-level="1.7.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验部分-2"><i class="fa fa-check"></i><b>1.7.5</b> 实验部分</a></li>
<li class="chapter" data-level="1.7.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验分析"><i class="fa fa-check"></i><b>1.7.6</b> 实验分析</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#ulmfit"><i class="fa fa-check"></i><b>1.8</b> ULMFit</a></li>
<li class="chapter" data-level="1.9" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#bert"><i class="fa fa-check"></i><b>1.9</b> BERT</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-5"><i class="fa fa-check"></i><b>1.9.1</b> introduction</a></li>
<li class="chapter" data-level="1.9.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#bert结构"><i class="fa fa-check"></i><b>1.9.2</b> bert结构</a></li>
<li class="chapter" data-level="1.9.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#input-output"><i class="fa fa-check"></i><b>1.9.3</b> input output</a></li>
<li class="chapter" data-level="1.9.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#mlm"><i class="fa fa-check"></i><b>1.9.4</b> MLM</a></li>
<li class="chapter" data-level="1.9.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#next-sentence-prediction"><i class="fa fa-check"></i><b>1.9.5</b> Next Sentence Prediction</a></li>
<li class="chapter" data-level="1.9.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#fine-tuning-bert"><i class="fa fa-check"></i><b>1.9.6</b> Fine-tuning BERT</a></li>
<li class="chapter" data-level="1.9.7" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#实验部分-3"><i class="fa fa-check"></i><b>1.9.7</b> 实验部分</a></li>
<li class="chapter" data-level="1.9.8" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#ablation-studies"><i class="fa fa-check"></i><b>1.9.8</b> Ablation Studies</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#xlnet"><i class="fa fa-check"></i><b>1.10</b> XLNET</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#abstract"><i class="fa fa-check"></i><b>1.10.1</b> ABSTRACT</a></li>
<li class="chapter" data-level="1.10.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-6"><i class="fa fa-check"></i><b>1.10.2</b> introduction</a></li>
<li class="chapter" data-level="1.10.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#two-stream-self-attention"><i class="fa fa-check"></i><b>1.10.3</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="1.10.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#局部预测"><i class="fa fa-check"></i><b>1.10.4</b> 局部预测</a></li>
<li class="chapter" data-level="1.10.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#incorporating-ideas-from-transformer-x"><i class="fa fa-check"></i><b>1.10.5</b> Incorporating Ideas from Transformer-X</a></li>
<li class="chapter" data-level="1.10.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#modeling-multiple-segments"><i class="fa fa-check"></i><b>1.10.6</b> Modeling Multiple Segments</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#albert"><i class="fa fa-check"></i><b>1.11</b> ALBERT</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-7"><i class="fa fa-check"></i><b>1.11.1</b> introduction</a></li>
<li class="chapter" data-level="1.11.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#scaling-up-representation-learning-for-natural-language"><i class="fa fa-check"></i><b>1.11.2</b> SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE</a></li>
<li class="chapter" data-level="1.11.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#cross-layer-parameter-sharing"><i class="fa fa-check"></i><b>1.11.3</b> CROSS-LAYER PARAMETER SHARING</a></li>
<li class="chapter" data-level="1.11.4" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#sentence-ordering-objectives"><i class="fa fa-check"></i><b>1.11.4</b> SENTENCE ORDERING OBJECTIVES</a></li>
<li class="chapter" data-level="1.11.5" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#factorized-embedding-parameterization"><i class="fa fa-check"></i><b>1.11.5</b> Factorized embedding parameterization</a></li>
<li class="chapter" data-level="1.11.6" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#跨层参数共享"><i class="fa fa-check"></i><b>1.11.6</b> 跨层参数共享</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#roberta"><i class="fa fa-check"></i><b>1.12</b> RoBERTa</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-8"><i class="fa fa-check"></i><b>1.12.1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="1.12.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#mlm-1"><i class="fa fa-check"></i><b>1.12.2</b> MLM</a></li>
<li class="chapter" data-level="1.12.3" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#static-vs.-dynamic-masking"><i class="fa fa-check"></i><b>1.12.3</b> Static vs. Dynamic Masking</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#bertflow"><i class="fa fa-check"></i><b>1.13</b> bertFlow</a></li>
<li class="chapter" data-level="1.14" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#t5"><i class="fa fa-check"></i><b>1.14</b> T5</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#introduction-9"><i class="fa fa-check"></i><b>1.14.1</b> introduction</a></li>
<li class="chapter" data-level="1.14.2" data-path="nlp-paper-preview.html"><a href="nlp-paper-preview.html#model"><i class="fa fa-check"></i><b>1.14.2</b> MODEL</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="模型蒸馏.html"><a href="模型蒸馏.html"><i class="fa fa-check"></i><b>2</b> 模型蒸馏</a>
<ul>
<li class="chapter" data-level="2.1" data-path="模型蒸馏.html"><a href="模型蒸馏.html#fastbert"><i class="fa fa-check"></i><b>2.1</b> fastbert</a></li>
<li class="chapter" data-level="" data-path="模型蒸馏.html"><a href="模型蒸馏.html#参考文献"><i class="fa fa-check"></i>参考文献</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">NLP相关paper阅读</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="模型蒸馏" class="section level1" number="2">
<h1><span class="header-section-number">2</span> 模型蒸馏</h1>
<p>从WORD2VEC到T5,参数量级增加上亿倍，这种装备向来是资深玩家才买得起的装备。因此出现了模型蒸馏，尽量减少模型效果降低的前提下，对模型进行蒸馏，降低模型的参数。对于上面的albert实现了transformer各个block的参数共享，也算是蒸馏中的一种。</p>
<div id="fastbert" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> fastbert</h2>
</div>
<div id="参考文献" class="section level2 unnumbered" number="">
<h2>参考文献</h2>
<div id="refs" class="references hanging-indent">
<div>
<p>Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. “ALBERT: A Lite Bert for Self-Supervised Learning of Language Representations.”</p>
</div>
<div>
<p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized Bert Pretraining Approach,” July.</p>
</div>
<div>
<p>Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”</p>
</div>
<div>
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</p>
</div>
<div>
<p>“TRANSFORMER详解.” 2021. 2021. <a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=7">https://www.bilibili.com/video/BV1Di4y1c7Zm?p=7</a>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>arXiv</em>.</p>
</div>
<div>
<p>“Xlnet与bert的异同.” 2019. 2019. <a href="https://zhuanlan.zhihu.com/p/70257427">https://zhuanlan.zhihu.com/p/70257427</a>.</p>
</div>
<div>
<p>“Xlnet详解.” 2019. 2019. <a href="https://blog.csdn.net/weixin_37947156/article/details/93035607">https://blog.csdn.net/weixin_37947156/article/details/93035607</a>.</p>
</div>
<div>
<p>Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” <a href="http://arxiv.org/abs/1906.08237">http://arxiv.org/abs/1906.08237</a>.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nlp-paper-preview.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
