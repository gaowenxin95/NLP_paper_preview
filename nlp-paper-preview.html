<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>NLP相关paper阅读</title>
  <meta name="description" content="NLP相关paper阅读" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="NLP相关paper阅读" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="NLP相关paper阅读" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2020-12-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#nlp-paper-preview"><i class="fa fa-check"></i><b>1</b> NLP paper preview</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#fasttext"><i class="fa fa-check"></i><b>1.1</b> fasttext</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#hierarchical-softmax"><i class="fa fa-check"></i><b>1.1.1</b> Hierarchical softmax</a></li>
<li class="chapter" data-level="1.1.2" data-path=""><a href="#n_gram-features"><i class="fa fa-check"></i><b>1.1.2</b> N_gram features</a></li>
<li class="chapter" data-level="1.1.3" data-path=""><a href="#fasttext与word2vec对比"><i class="fa fa-check"></i><b>1.1.3</b> fasttext与word2vec对比</a></li>
<li class="chapter" data-level="1.1.4" data-path=""><a href="#fasttext与cbow有两点不同"><i class="fa fa-check"></i><b>1.1.4</b> fasttext与CBOW有两点不同</a></li>
<li class="chapter" data-level="1.1.5" data-path=""><a href="#实验和结果分析"><i class="fa fa-check"></i><b>1.1.5</b> 实验和结果分析</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#doc2vec"><i class="fa fa-check"></i><b>1.2</b> Doc2vec</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1.2.1</b> introduction</a></li>
<li class="chapter" data-level="1.2.2" data-path=""><a href="#实验对比"><i class="fa fa-check"></i><b>1.2.2</b> 实验对比</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#skip-thoughts"><i class="fa fa-check"></i><b>1.3</b> skip-thoughts</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path=""><a href="#introduction-1"><i class="fa fa-check"></i><b>1.3.1</b> introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path=""><a href="#结构"><i class="fa fa-check"></i><b>1.3.2</b> 结构</a></li>
<li class="chapter" data-level="1.3.3" data-path=""><a href="#encoder"><i class="fa fa-check"></i><b>1.3.3</b> encoder</a></li>
<li class="chapter" data-level="1.3.4" data-path=""><a href="#decoder"><i class="fa fa-check"></i><b>1.3.4</b> decoder</a></li>
<li class="chapter" data-level="1.3.5" data-path=""><a href="#目标函数"><i class="fa fa-check"></i><b>1.3.5</b> 目标函数</a></li>
<li class="chapter" data-level="1.3.6" data-path=""><a href="#词典的拓展"><i class="fa fa-check"></i><b>1.3.6</b> 词典的拓展</a></li>
<li class="chapter" data-level="1.3.7" data-path=""><a href="#实验部分"><i class="fa fa-check"></i><b>1.3.7</b> 实验部分</a></li>
<li class="chapter" data-level="1.3.8" data-path=""><a href="#实验部分-1"><i class="fa fa-check"></i><b>1.3.8</b> 实验部分</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">NLP相关paper阅读</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">NLP相关paper阅读</h1>
<p class="author"><em>高文欣</em></p>
<p class="date"><em>2020-12-29</em></p>
</div>
<div id="nlp-paper-preview" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NLP paper preview</h1>
<p>emmmmm…站在巨人的肩膀看世界~ 是件幸福的事情</p>
<div id="fasttext" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> fasttext</h2>
<p>结构中比较重要的几个点</p>
<div id="hierarchical-softmax" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Hierarchical softmax</h3>
<p>这个没什么好说的，和word2vec里面的是一样的。具体解释忘记之前从哪里找来得了但私以为解释的8错</p>
<blockquote>
<p>When the number of classes is large, computing the linear classifier
is computationally expensive. More precisely, the computational
complexity is <span class="math inline">\(O(k h)\)</span> where <span class="math inline">\(k\)</span> is the number of classes and <span class="math inline">\(h\)</span> the
dimension of the text representation. In order to improve our running
time, we use a hierarchical softmax (Goodman, 2001) based on the
Huffman coding tree (Mikolov et al., 2013 ). During training, the
computational complexity drops to <span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span>. The
hierarchical softmax is also advantageous at test time when searching
for the most likely class. Each node is associated with a probability
that is the probability of the path from the root to that node. If the
node is at depth <span class="math inline">\(l+1\)</span> with parents <span class="math inline">\(n_{1}, \ldots, n_{l},\)</span> its
probability is <span class="math display">\[
P\left(n_{l+1}\right)=\prod_{i=1}^{l} P\left(n_{i}\right)
\]</span></p>
</blockquote>
<blockquote>
<p>This means that the probability of a node is always lower than the one
of its parent. Exploring the tree with a depth first search and
tracking the maximum probability among the leaves allows us to discard
any branch associated with a small probability. In practice, we
observe a reduction of the complexity to
<span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span> at test time. This approach is further
extended to compute the <span class="math inline">\(T\)</span> -top targets at the cost of <span class="math inline">\(O(\log (T)),\)</span>
using a binary heap.</p>
</blockquote>
<p>主要就是利用霍夫曼树加快计算的速度</p>
<blockquote>
<p>Hierachical
Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树,
保证词频较大的词处于相对比较浅的层,
词频较低的词相应的处于Huffman树较深层的叶子节点,
每一个词都处于这棵Huffman树上的某个叶子节点;
第二，将原本的一个|V|分类问题变成了<span class="math inline">\(\log|V|\)</span> 次的二分类问题,
做法简单说来就是, 原先要计算 <span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span>的时候,
因为使用的 是普通的softmax,
势必要求词典中的每一个词的概率大小，为了减少这一步的计算量, 在
Hierachical Softmax中，同样是计算当前词 <span class="math inline">\(w_{t}\)</span>
在其上下文中的概率大小，只需要把它变成在
Huffman树中的路径预测问题就可以了，因为当前词 <span class="math inline">\(w_{t}\)</span>
在Huffman树中对应到一条路径, 这条 路径由这棵二叉树中从根节点开始,
经过一系列中间的父节点, 最终到达当前这个词的叶子节点而 组成,
那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而
Huffman树的构造过程保证了树的深度为 <span class="math inline">\(\log |V|,\)</span>
所以也就只需要做<span class="math inline">\(\log |V|\)</span>次二分类便可以 求得
<span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span> 的大小, 这相比原来|V|次的计算量,
已经大大减小了。</p>
</blockquote>
</div>
<div id="n_gram-features" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> N_gram features</h3>
<p><strong>这个是主要区别于word2vec的输入的部分了，为了更好的学习到上下文的语序特征</strong></p>
<p>从bag of word 变成了bag of features</p>
<blockquote>
<p>Bag of words is invariant to word order but taking explicitly this
order into account is often computationally very expensive. Instead,
we use a bag of n-grams as additional features to capture some partial
information about the local word order. This is very efficient in
practice while achieving comparable results to methods that explicitly
use the order (Wang and Manning, 2012 ). We maintain a fast and memory
efficient mapping of the n-grams by using the hashing trick
(Weinberger et al., 2009 ) with the same hashing function as in
Mikolov et al. (2011) and <span class="math inline">\(10 \mathrm{M}\)</span> bins if we only used
bigrams, and <span class="math inline">\(100 \mathrm{M}\)</span> otherwise.</p>
</blockquote>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
fasttext 文本分类的结构？</li>
</ul>
</div>
<div id="fasttext与word2vec对比" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> fasttext与word2vec对比</h3>
<p>感觉读paper的时候并没有很仔细的找出fasttext和word2vec的区别</p>
<blockquote>
<p>word2vec和GloVe都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText则是利用带有监督标记的文本分类数据完成训练，本质上没有什么特殊的，模型框架就是CBOW。</p>
</blockquote>
<p>因为是训练词向量的嘛，因此只需要文本就可以了，不需要标签。</p>
<p>fasttext和word2vec本质无区别都是单层的神经网络，CBOW的结构，通过上下文预测当前词。
word2vec是为了得到embedding的矩阵，word2vec本质是一个词袋模型:bag of
word。</p>
</div>
<div id="fasttext与cbow有两点不同" class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> fasttext与CBOW有两点不同</h3>
<blockquote>
<p>分别是输入数据和预测目标的不同 -
在输入数据上，CBOW输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而fastText为了利用更多的语序信息，将bag-of-words变成了bag-of-features，也就是下图中的输入x不再仅仅是一个词，还可以加上bigram或者是trigram的信息等等；</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>from gensim.models import FastText</span>
<span id="cb1-2"><a href="#cb1-2"></a>sentences =<span class="st"> </span>[[<span class="st">&quot;你&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;谁&quot;</span>], [<span class="st">&quot;我&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;中国人&quot;</span>]]</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>model =<span class="st"> </span><span class="kw">FastText</span>(sentences,  <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">window=</span><span class="dv">3</span>, <span class="dt">min_count=</span><span class="dv">1</span>, <span class="dt">iter=</span><span class="dv">10</span>,<span class="dt">min_n =</span> <span class="dv">3</span> , <span class="dt">max_n =</span> <span class="dv">6</span>,<span class="dt">word_ngrams =</span> <span class="dv">0</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>model[<span class="st">&#39;你&#39;</span>]  <span class="co"># 词向量获得的方式</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>model.wv[<span class="st">&#39;你&#39;</span>] <span class="co"># 词向量获得的方式</span></span></code></pre></div>
<p>所以在训练fasttext的词向量时候，参数word_ngrams =
0时候，是等价于word2vec的。</p>
<blockquote>
<p>第二个不同在于，CBOW预测目标是语境中的一个词，而fastText预测目标是当前这段输入文本的类别，正因为需要这个文本类别，因此才说fastText是一个监督模型。而相同点在于，fastText的网络结构和CBOW基本一致，同时在输出层的分类上也使用了Hierachical
Softmax技巧来加速训练。</p>
</blockquote>
<p>两者本质的不同，体现在 h-softmax的使用：</p>
<p>Wordvec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的
h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。
fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</p>
</div>
<div id="实验和结果分析" class="section level3" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> 实验和结果分析</h3>
<p><em>情感分析实验</em></p>
<p>在8个数据集上面acc对比，对比了6个模型，可以看出在绝大部分的数据集上面fasttext的acc是最好的。
加入bgram的效果要优于不加的</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-87AA0A90.png" /></p>
<p>运行时间对比fasttext的速度绝了</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-099A1FC8.png" /></p>
<p>对比不同模型的acc，fasttext略高</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-283D5547.png" /></p>
<p>在标签预测上的测试时间，fasttext非常快</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-77B0769C.png" /></p>
</div>
</div>
<div id="doc2vec" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Doc2vec</h2>
<div id="introduction" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> introduction</h3>
<blockquote>
<p>However, the bag-of-words (BOW) has many disadvantages. The word order
is lost, and thus different sentences can have exactly the same
representation, as long as the same words are used. Even though
bag-of-n-grams considers the word order in short context, it suffers
from data sparsity and high dimensionality. Bag-of-words and
bagof-n-grams have very little sense about the semantics of the words
or more formally the distances between the words. This means that
words “powerful,” “strong” and “Paris” are equally distant despite the
fact that semantically, “powerful” should be closer to “strong” than
“Paris.”</p>
</blockquote>
<ul>
<li>词袋模型的缺点是没有考虑词序，学习不到语义；</li>
<li>bag-of-n-grams模型即使在短文本中也是存在高维稀疏问题的；</li>
<li>二者都无法学习到语义</li>
</ul>
<blockquote>
<p>In this paper, we propose Paragraph Vector, an unsupervised framework
that learns continuous distributed vector representations for pieces
of texts. The texts can be of variable-length, ranging from sentences
to documents. The name Paragraph Vector is to emphasize the fact that
the method can be applied to variable-length pieces of texts, anything
from a phrase or sentence to a large document.</p>
</blockquote>
<p>DOC2vec中提出了一种句向量的思想。文本是由不同长度的句子组成的，句向量可以学习到不同长度的短语和句子的embedding</p>
<blockquote>
<p>In our model, the vector representation is trained to be useful for
predicting words in a paragraph. More precisely, we concatenate the
paragraph vector with several word vectors from a paragraph and
predict the following word in the given context. Both word vectors and
paragraph vectors are trained by the stochastic gradient descent and
backpropagation (Rumelhart et al., 1986 ). While paragraph vectors are
unique among paragraphs, the word vectors are shared. At prediction
time, the paragraph vectors are inferred by fixing the word vectors
and training the new paragraph vector until convergence.</p>
</blockquote>
<p>说简单点就是在原有词向量的基础上concat上了句向量，同时学习词向量和句向量的语义。</p>
<p>个人感觉句向量的作用其实是增加了一个上下文的position，句向量的大小可以自定义。</p>
<p>Doc2vec同样具有2种结构</p>
<blockquote>
<p>The above method considers the concatenation of the paragraph vector
with the word vectors to predict the next word in a text window.
Another way is to ignore the context words in the input, but force the
model to predict words randomly sampled from the paragraph in the
output. In reality, what this means is that at each iteration of
stochastic gradient descent, we sample a text window, then sample a
random word from the text window and form a classification task given
the Paragraph Vector. This technique is shown in Figure <span class="math inline">\(3 .\)</span> We name
this version the Distributed Bag of Words version of Paragraph Vector
(PV-DBOW), as opposed to Distributed Memory version of Paragraph
Vector (PV-DM) in previous section.</p>
</blockquote>
<p>PV-DBOW是Distributed Bag of Words version of Paragraph
Vector，和Skip-gram类似，通过文档来预测文档内的词，训练的时候，随机采样一些文本片段，然后再从这个片段中采样一个词，让PV-DBOW模型来预测这个词，以此分类任务作为训练方法，说白了本质上和Skip-gram是一样的。这个方法有个致命的弱点，就是为了获取新文档的向量，还得继续走一遍训练流程，并且由于模型主要是针对文档向量预测词向量的过程进行建模，其实很难去表征词语之间的更丰富的语义结构。</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-CEE075C9.png" /></p>
<p>PV-DM的全称是Distributed Memory Model of Paragraph
Vectors，和CBOW类似，也是通过上下文预测下一个词，不过在输入层的时候，同时也维护了一个文档ID映射到一个向量的look-up
table，模型的目的便是将当前文档的向量以及上下文向量联合输入模型，并让模型预测下一个词，训练结束后，对于现有的文档，便可以直接通过查表的方式快速得到该文档的向量，而对于新的一篇文档，那么则需要将已有的look-up
table添加相应的列，然后重新走一遍训练流程，只不过此时固定好其他的参数，只调整look-up
table，收敛后便可以得到新文档对应的向量了。</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-64854E74.png" /></p>
</div>
<div id="实验对比" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> 实验对比</h3>
<p>Treebank Dataset 情感分析结果对比 基本都是不同长度的句子</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-CE415F36.png" /></p>
<p>情感分析结果对比，段落和文章上</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-EC1185C3.png" /></p>
<p>计算句间的距离对比</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-C86443E1.png" /></p>
<p>实际跑下来，Doc的效果并不如word2vec的效果好，是不是和样本数据量有关，亦或者和fasttext一样适用于英文。</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Doc2VEC是否可以直接使用word2vec的结果？</li>
</ul>
<p>个人人为应该可以使用，但是这里的句向量的计算的方式应该挺多的，计算出来句向量直接concat到词向量上面，理论上来说就是该篇文章的思想。</p>
</div>
</div>
<div id="skip-thoughts" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> skip-thoughts</h2>
<blockquote>
<p>Using the continuity of text from books, we train an encoderdecoder
model that tries to reconstruct the surrounding sentences of an
encoded passage. Sentences that share semantic and syntactic
properties are thus mapped to similar vector representations. We next
introduce a simple vocabulary expansion method to encode words that
were not seen as part of training, allowing us to expand our
vocabulary to a million words.</p>
</blockquote>
<p>skip-thoughts也是一种encoder-decoder结构，直接根据当前句预测上下文。</p>
<p>skip-gram是根据当前词预测上下文的词。目标不同。</p>
<div id="introduction-1" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> introduction</h3>
<blockquote>
<p>In this paper we abstract away from the composition methods themselves
and consider an alternative loss function that can be applied with any
composition operator. We consider the following question: is there a
task and a corresponding loss that will allow us to learn highly
generic sentence representations? We give evidence for this by
proposing a model for learning high-quality sentence vectors without a
particular supervised task in mind. Using word vector learning as
inspiration, we propose an objective function that abstracts the
skip-gram model of [8] to the sentence level. That is, instead of
using a word to predict its surrounding context, we instead encode a
sentence to predict the sentences around it. Thus, any composition
operator can be substituted as a sentence encoder and only the
objective function becomes modified. Figure 1 illustrates the model.
We call our model skip-thoughts and vectors induced by our model are
called skip-thought vectors.</p>
</blockquote>
<p>也是在强调直接学习句向量，根据当前的句子预测上下文的句子向量。
<eos>是每个句子的结尾</p>
<p><img src="figs/skip-thoughts.png" /></p>
<p>当前句<span class="math inline">\(s_{i}\)</span>预测上下句<span class="math inline">\(s_{i-1}\)</span>和<span class="math inline">\(s_{i+1}\)</span></p>
</div>
<div id="结构" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> 结构</h3>
<blockquote>
<p>We treat skip-thoughts in the framework of encoder-decoder models
<span class="math inline">\(1 .\)</span> That is, an encoder maps words to a sentence vector and a
decoder is used to generate the surrounding sentences. Encoderdecoder
models have gained a lot of traction for neural machine translation.
In this setting, an encoder is used to map e.g. an English sentence
into a vector. The decoder then conditions on this vector to generate
a translation for the source English sentence. Several choices of
encoder-decoder pairs have been explored, including ConvNet-RNN [10],
RNN-RNN [11] and LSTM-LSTM [12]. The source sentence representation
can also dynamically change through the use of an attention mechanism
[13] to take into account only the relevant words for translation at
any given time. In our model, we use an RNN encoder with GRU [14]
activations and an RNN decoder with a conditional GRU. This model
combination is nearly identical to the RNN encoder-decoder of [11]
used in neural machine translation. GRU has been shown to perform as
well as LSTM [2] on sequence modelling tasks [14] while being
conceptually simpler. GRU units have only 2 gates and do not require
the use of a cell. While we use RNNs for our model, any encoder and
decoder can be used so long as we can backpropagate through it.</p>
</blockquote>
<p>常见的编码器和解码器的结构有ConvNet-RNN , RNN-RNN and LSTM-LSTM</p>
<p>Skip-Though模型希望通过编码中间的句子来预测其前一个句子和后一个句子，前一个句子和后一个句子分别用不同的解码器进行解码，也就是根据中间句子的句向量表示进行自回归的Decoder把句子解码出来，这借鉴了机器翻译中的思想。</p>
<p>使用两个独立的Decoder分别建模前一句和后一句是为了用独立的语义去编码前一句和后一句。</p>
<p>skip-thought模型的神经网络结构是在机器翻译中最常用的 Encoder-Decoder 结构，而在 Encoder-Decoder 架构中所使用的模型是GRU模型。因此在训练句子向量时同样要使用到词向量，编码器输出的结果为句子中最后一个词所输出的向量。
GRU对比LSTM从速度上面来说是应该是更快的，效果上来看，实际数据中差不多~</p>
</div>
<div id="encoder" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> encoder</h3>
<p>Skip-Thought模型的编码器部分使用GRU进行Encoder, GRU中有更新门和重置门，更新门对应 <span class="math inline">\(z^{t},\)</span> 重置门对应 <span class="math inline">\(r^{t}\)</span> 。更新门用于控制前一 个时刻的信息被带入当前时刻的程度, 更新门的值越大, 说明前一时刻的信息带入当前时刻越多。重置门控制的是前一时刻有多少信息被 写入到当前时刻的候选集。</p>
<blockquote>
<p>Encoder. Let <span class="math inline">\(w_{i}^{1}, \ldots, w_{i}^{N}\)</span> be the words in sentence
<span class="math inline">\(s_{i}\)</span> where <span class="math inline">\(N\)</span> is the number of words in the sentence. At each time
step, the encoder produces a hidden state <span class="math inline">\(\mathbf{h}_{i}^{t}\)</span> which
can be interpreted as the representation of the sequence
<span class="math inline">\(w_{i}^{1}, \ldots, w_{i}^{t} .\)</span> The hidden state <span class="math inline">\(\mathbf{h}_{i}^{N}\)</span>
thus represents the full sentence. To encode a sentence, we iterate
the following sequence of equations (dropping the subscript <span class="math inline">\(i\)</span> ):</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{x}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}\right) \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{x}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}\right) \\
\overline{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W} \mathbf{x}^{t}+\mathbf{U}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)\right) \\
\mathbf{h}^{t} &amp;=\left(1-\mathbf{z}^{t}\right) \odot \mathbf{h}^{t-1}+\mathbf{z}^{t} \odot \overline{\mathbf{h}}^{t}
\end{aligned}
\]</span> where <span class="math inline">\(\overline{\mathbf{h}}^{t}\)</span> is the proposed state update at
time <span class="math inline">\(t, \mathbf{z}^{t}\)</span> is the update gate, <span class="math inline">\(\mathbf{r}_{t}\)</span> is the
reset gate <span class="math inline">\((\odot)\)</span> denotes a component-wise product. Both update gates
takes values between zero and one.</p>
<p>encoder部分就是一个GRU的结构进行特征选择</p>
<p>编码器的作用：编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量
c，并在该背景变量中编码输⼊序列信息。</p>
</div>
<div id="decoder" class="section level3" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> decoder</h3>
<blockquote>
<p>Decoder. The decoder is a neural language model which conditions on
the encoder output <span class="math inline">\(\mathbf{h}_{i} .\)</span> The computation is similar to
that of the encoder except we introduce matrices
<span class="math inline">\(\mathbf{C}_{z}, \mathbf{C}_{r}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> that are used to
bias the update gate, reset gate and hidden state computation by the
sentence vector. One decoder is used for the next sentence <span class="math inline">\(s_{i+1}\)</span>
while a second decoder is used for the previous sentence <span class="math inline">\(s_{i-1}\)</span>.
Separate parameters are used for each decoder with the exception of
the vocabulary matrix <span class="math inline">\(\mathbf{V}\)</span> which is the weight matrix
connecting the decoder’s hidden state for computing a distribution
over words. In what follows we describe the decoder for the next
sentence <span class="math inline">\(s_{i+1}\)</span> although an analogous computation is used for the
previous sentence <span class="math inline">\(s_{i-1}\)</span>. Let <span class="math inline">\(\mathbf{h}_{i+1}^{t}\)</span> denote the
hidden state of the decoder at time <span class="math inline">\(t .\)</span> Decoding involves iterating
through the following sequence of equations (dropping the subscript
<span class="math inline">\(i+1\)</span> ):</p>
</blockquote>
<p>decoder的输入是encoder的输出，两个解码器分别对当前句的上下句进行解码。
下面给出了预测 <span class="math inline">\(s_{i+1}\)</span>的，预测 <span class="math inline">\(s_{i-1}\)</span>同上</p>
<p>Decoder部分使用的同样是GRU，Decoder部分的GRU是带有条件信息的，也就是编码器得到的中间句子的编码信息<span class="math inline">\(h_{i}\)</span>，从而使得Encoder部分的GRU每次都能携带中间句子的信息做出决策。</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r}^{d} \mathbf{x}^{t-1}+\mathbf{U}_{r}^{d} \mathbf{h}^{t-1}+\mathbf{C}_{r} \mathbf{h}_{i}\right) \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z}^{d} \mathbf{x}^{t-1}+\mathbf{U}_{z}^{d} \mathbf{h}^{t-1}+\mathbf{C}_{z} \mathbf{h}_{i}\right) \\
\overline{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}^{d} \mathbf{x}^{t-1}+\mathbf{U}^{d}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{C h}_{i}\right) \\
\mathbf{h}_{i+1}^{t} &amp;=\left(1-\mathbf{z}^{t}\right) \odot \mathbf{h}^{t-1}+\mathbf{z}^{t} \odot \overline{\mathbf{h}}^{t}
\end{aligned}
\]</span></p>
<p>Given <span class="math inline">\(\mathbf{h}_{i+1}^{t},\)</span> the probability of word <span class="math inline">\(w_{i+1}^{t}\)</span>
given the previous <span class="math inline">\(t-1\)</span> words and the encoder vector is <span class="math display">\[
P\left(w_{i+1}^{t} \mid w_{i+1}^{&lt;t}, \mathbf{h}_{i}\right) \propto \exp \left(\mathbf{v}_{w_{i+1}^{t}} \mathbf{h}_{i+1}^{t}\right)
\]</span> where <span class="math inline">\(\mathbf{v}_{w_{i+1}^{t}}\)</span> denotes the row of <span class="math inline">\(\mathbf{V}\)</span>
corresponding to the word of <span class="math inline">\(w_{i+1}^{t} .\)</span> An analogous computation is
performed for the previous sentence <span class="math inline">\(s_{i-1}\)</span>.</p>
<p>解码器部分使用的网络结构也是GRU</p>
</div>
<div id="目标函数" class="section level3" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> 目标函数</h3>
<blockquote>
<p>Objective. Given a tuple <span class="math inline">\(\left(s_{i-1}, s_{i}, s_{i+1}\right),\)</span> the
objective optimized is the sum of the log-probabilities for the
forward and backward sentences conditioned on the encoder
representation: <span class="math display">\[
\sum_{t} \log P\left(w_{i+1}^{t} \mid w_{i+1}^{&lt;t}, \mathbf{h}_{i}\right)+\sum_{t} \log P\left(w_{i-1}^{t} \mid w_{i-1}^{&lt;t}, \mathbf{h}_{i}\right)
\]</span></p>
</blockquote>
<p>预测上下句的损失函数之和。</p>
</div>
<div id="词典的拓展" class="section level3" number="1.3.6">
<h3><span class="header-section-number">1.3.6</span> 词典的拓展</h3>
<blockquote>
<p>We now describe how to expand our encoder’s vocabulary to words it has not seen during training. Suppose we have a model that was trained to induce word representations, such as word2vec. Let <span class="math inline">\(V_{w 2 v}\)</span> denote the word embedding space of these word representations and let <span class="math inline">\(V_{r n n}\)</span> denote the <span class="math inline">\(\mathrm{RNN}\)</span> word embedding space. We assume the vocabulary of <span class="math inline">\(\mathcal{V}_{w 2 v}\)</span> is much larger than that of <span class="math inline">\(\mathcal{V}_{r n n}\)</span>. Our goal is to construct a mapping <span class="math inline">\(f: \mathcal{V}_{w 2 v} \rightarrow \mathcal{V}_{r n n}\)</span> parameterized by a matrix <span class="math inline">\(\mathbf{W}\)</span> such that <span class="math inline">\(\mathbf{v}^{\prime}=\mathbf{W} \mathbf{v}\)</span> for <span class="math inline">\(\mathbf{v} \in \mathcal{V}_{w 2 v}\)</span> and <span class="math inline">\(\mathbf{v}^{\prime} \in \mathcal{V}_{r n n} .\)</span> Inspired by [15] , which learned linear mappings between translation word spaces, we solve an un-regularized L2 linear regression loss for the matrix <span class="math inline">\(\mathbf{W}\)</span>. Thus, any word from <span class="math inline">\(\mathcal{V}_{w 2 v}\)</span> can now be mapped into <span class="math inline">\(\mathcal{V}_{r n n}\)</span> for encoding sentences. Table 3 shows examples of nearest neighbour words for queries that did not appear in our training vocabulary.</p>
</blockquote>
<p>对于encoder部分，如何对词库中未出现的词进行编码。</p>
<ol style="list-style-type: decimal">
<li>用 <span class="math inline">\(V_{w 2 v}\)</span> 表示训练的词向量空间, 用 <span class="math inline">\(V_{r n n}\)</span> 表示模型中的词向量空间，在这里 <span class="math inline">\(V_{w 2 v}\)</span> 词的数量是远远大于 <span class="math inline">\(V_{r n n}\)</span> 的。</li>
<li>引入一个矩阵 <span class="math inline">\(W\)</span> 来构建一个映射函数: <span class="math inline">\(\mathrm{f}: V_{r n n}-&gt;V_{w 2 v}\)</span> 。使得有 <span class="math inline">\(v \prime=W v,\)</span> 其中 <span class="math inline">\(v \in V_{w 2 v}, v \prime \in V_{r n n}\)</span> 。</li>
<li>通过映射函数就可以将任何在 <span class="math inline">\(V_{w 2 v}\)</span> 中的词映射到 <span class="math inline">\(V_{r n n}\)</span> 中。</li>
</ol>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
从w2vec词向量的词表中做一个映射到rnn的词表上，得到一个映射的矩阵</li>
</ul>
<p>这个是说加载预训练模型作为第一层的embedding吗？</p>
<blockquote>
<p>We note that there are alternate strategies for solving the vocabulary problem. One alternative is to initialize the RNN embedding space to that of pre-trained word vectors. This would require a more sophisticated softmax for decoding, or clipping the vocabulary of the decoder as it would be too computationally expensive to naively decode with vocabularies of hundreds of thousands of words. An alternative strategy is to avoid words altogether and train at the character level.</p>
</blockquote>
<p>以词作为预训练的词向量，需要非常复杂的softmax做解码，这样计算的代码比较大，确实。。每次算词频都需要遍历整个词库，计算量很大。。很慢。
因此有了以字符为基本单位的可代替方案</p>
<p>这一段的意义何在？skip-thoughts中还是以词为基本做的预训练，没有用到字符啊</p>
</div>
<div id="实验部分" class="section level3" number="1.3.7">
<h3><span class="header-section-number">1.3.7</span> 实验部分</h3>
<blockquote>
<p>In our experiments, we evaluate the capability of our encoder as a generic feature extractor after training on the BookCorpus dataset. Our experimentation setup on each task is as follows:</p>
</blockquote>
<p>在BookCorpus数据集上进行训练，每个任务如下</p>
<blockquote>
<p>Using the learned encoder as a feature extractor, extract skip-thought vectors for all sentences.</p>
</blockquote>
<p>encoder部分：使用skip-th-vec提取所有句子特征</p>
<blockquote>
<p>If the task involves computing scores between pairs of sentences, compute component-wise features between pairs. This is described in more detail specifically for each experiment.</p>
</blockquote>
<p>若需要计算两个句子之间的得分，则计算它们之间的成分特征。</p>
<blockquote>
<p>Train a linear classifier on top of the extracted features, with no additional fine-tuning or backpropagation through the skip-thoughts model.</p>
</blockquote>
<p>在提取的特征上面训练一个线性的分类器，无需额外的微调和反向传播</p>
<blockquote>
<p>We restrict ourselves to linear classifiers for two reasons. The first is to directly evaluate the representation quality of the computed vectors. It is possible that additional performance gains can be made throughout our experiments with non-linear models but this falls out of scope of our goal. Furthermore, it allows us to better analyze the strengths and weaknesses of the learned representations. The second reason is that reproducibility now becomes very straightforward.</p>
</blockquote>
<p>严格使用线性分类器有2个原因：</p>
<ul>
<li>第一种是直接评估计算出的向量的表征能力。对非线性模型的实验，有可能获得额外的性能提高，此外能更好地分析表征学习的优缺点。</li>
<li>第二个原因是再现性变得非常直接。（这一点没有明白）</li>
</ul>
</div>
<div id="实验部分-1" class="section level3" number="1.3.8">
<h3><span class="header-section-number">1.3.8</span> 实验部分</h3>
<p><img src="figs/skip-th-vec-exp1.png" /></p>
<p>左边的部门是计算相似性的实验，评价指标是皮尔森相关系数和斯皮尔曼相关系数，无监督的实验。
从结果上看skip-vec这个的结果不是最优的，属于中等偏上了。</p>
<p>右边的是一个二分类的实验，评价指标是ACC和F1.</p>
<p>还有计算句子相似语义的实验和情感分析等的实验，可以参考原文，此除略。</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
