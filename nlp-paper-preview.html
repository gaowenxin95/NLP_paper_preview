<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>NLP相关paper阅读</title>
  <meta name="description" content="NLP相关paper阅读" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="NLP相关paper阅读" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="NLP相关paper阅读" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2021-01-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#nlp-paper-preview"><i class="fa fa-check"></i><b>1</b> NLP paper preview</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#word2vec"><i class="fa fa-check"></i><b>1.1</b> word2vec</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#cbow"><i class="fa fa-check"></i><b>1.1.1</b> CBOW</a></li>
<li class="chapter" data-level="1.1.2" data-path=""><a href="#skip-gram"><i class="fa fa-check"></i><b>1.1.2</b> skip-gram</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#fasttext"><i class="fa fa-check"></i><b>1.2</b> fasttext</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path=""><a href="#hierarchical-softmax"><i class="fa fa-check"></i><b>1.2.1</b> Hierarchical softmax</a></li>
<li class="chapter" data-level="1.2.2" data-path=""><a href="#n_gram-features"><i class="fa fa-check"></i><b>1.2.2</b> N_gram features</a></li>
<li class="chapter" data-level="1.2.3" data-path=""><a href="#fasttext与word2vec对比"><i class="fa fa-check"></i><b>1.2.3</b> fasttext与word2vec对比</a></li>
<li class="chapter" data-level="1.2.4" data-path=""><a href="#fasttext与cbow有两点不同"><i class="fa fa-check"></i><b>1.2.4</b> fasttext与CBOW有两点不同</a></li>
<li class="chapter" data-level="1.2.5" data-path=""><a href="#实验和结果分析"><i class="fa fa-check"></i><b>1.2.5</b> 实验和结果分析</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#doc2vec"><i class="fa fa-check"></i><b>1.3</b> Doc2vec</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1.3.1</b> introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path=""><a href="#实验对比"><i class="fa fa-check"></i><b>1.3.2</b> 实验对比</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#skip-thoughts"><i class="fa fa-check"></i><b>1.4</b> skip-thoughts</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path=""><a href="#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path=""><a href="#结构"><i class="fa fa-check"></i><b>1.4.2</b> 结构</a></li>
<li class="chapter" data-level="1.4.3" data-path=""><a href="#encoder"><i class="fa fa-check"></i><b>1.4.3</b> encoder</a></li>
<li class="chapter" data-level="1.4.4" data-path=""><a href="#decoder"><i class="fa fa-check"></i><b>1.4.4</b> decoder</a></li>
<li class="chapter" data-level="1.4.5" data-path=""><a href="#目标函数"><i class="fa fa-check"></i><b>1.4.5</b> 目标函数</a></li>
<li class="chapter" data-level="1.4.6" data-path=""><a href="#词典的拓展"><i class="fa fa-check"></i><b>1.4.6</b> 词典的拓展</a></li>
<li class="chapter" data-level="1.4.7" data-path=""><a href="#实验部分"><i class="fa fa-check"></i><b>1.4.7</b> 实验部分</a></li>
<li class="chapter" data-level="1.4.8" data-path=""><a href="#实验部分-1"><i class="fa fa-check"></i><b>1.4.8</b> 实验部分</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#elmo"><i class="fa fa-check"></i><b>1.5</b> ELMO</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path=""><a href="#introduction-2"><i class="fa fa-check"></i><b>1.5.1</b> introduction</a></li>
<li class="chapter" data-level="1.5.2" data-path=""><a href="#双向语言模型"><i class="fa fa-check"></i><b>1.5.2</b> 双向语言模型</a></li>
<li class="chapter" data-level="1.5.3" data-path=""><a href="#elmo结构"><i class="fa fa-check"></i><b>1.5.3</b> elmo结构</a></li>
<li class="chapter" data-level="1.5.4" data-path=""><a href="#evaluation"><i class="fa fa-check"></i><b>1.5.4</b> Evaluation</a></li>
<li class="chapter" data-level="1.5.5" data-path=""><a href="#analysis"><i class="fa fa-check"></i><b>1.5.5</b> analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#attention-is-all-you-need"><i class="fa fa-check"></i><b>1.6</b> Attention is all you need</a></li>
<li class="chapter" data-level="1.7" data-path=""><a href="#gpt"><i class="fa fa-check"></i><b>1.7</b> GPT</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path=""><a href="#introduction-3"><i class="fa fa-check"></i><b>1.7.1</b> introduction</a></li>
<li class="chapter" data-level="1.7.2" data-path=""><a href="#framework"><i class="fa fa-check"></i><b>1.7.2</b> Framework</a></li>
<li class="chapter" data-level="1.7.3" data-path=""><a href="#unsupervised-pre-training"><i class="fa fa-check"></i><b>1.7.3</b> Unsupervised pre-training</a></li>
<li class="chapter" data-level="1.7.4" data-path=""><a href="#supervised-ﬁne-tuning"><i class="fa fa-check"></i><b>1.7.4</b> Supervised ﬁne-tuning</a></li>
<li class="chapter" data-level="1.7.5" data-path=""><a href="#实验部分-2"><i class="fa fa-check"></i><b>1.7.5</b> 实验部分</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">NLP相关paper阅读</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">NLP相关paper阅读</h1>
<p class="author"><em>高文欣</em></p>
<p class="date"><em>2021-01-12</em></p>
</div>
<div id="nlp-paper-preview" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NLP paper preview</h1>
<p>emmmmm…站在巨人的肩膀看世界~ 是件幸福的事情</p>
<div id="word2vec" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> word2vec</h2>
<p>这个工具包毕竟是梯子级别的~
主要包括两个方法CBOW和SKIP-gram</p>
<div id="cbow" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> CBOW</h3>
<p>Continuous Bag-of-Words 连续词袋模型</p>
<p>目标函数</p>
<p><span class="math display">\[
\begin{array}{c}
\frac{1}{T} \sum_{t=1}^{T} \log P\left(w_{t} \mid c_{t}\right) \\
P\left(w_{t} \mid c_{t}\right)=\frac{\exp \left(e^{\prime}\left(w_{t}\right)^{T} x\right)}{\sum_{i=1}^{|V|} \exp \left(e^{\prime}\left(w_{i}\right)^{T} x\right)}, x=\sum_{i \in c} e\left(w_{i}\right)
\end{array}
\]</span></p>
<blockquote>
<p>CBOW没有隐藏层，本质上只有两层结构，输入层将目标词语境c中的每一个词向量简单求和（当然，也可以求平均）后得到语境向量，然后直接与目标词的输出向量求点积，目标函数也就是要让这个与目标词向量的点积取得最大值，对应的与非目标词的点积尽量取得最小值。从这可以看出，CBOW的第一个特点是取消了NNLM中的隐藏层，直接将输入层和输出层相连；第二个特点便是在求语境context向量时候，语境内的词序已经丢弃（这个是名字中Continuous的来源）；第三，因为最终的目标函数仍然是语言模型的目标函数，所以需要顺序遍历语料中的每一个词（这个是名字中Bag-of-Words的来源）。因此有了这些特点（尤其是第二点和第三点），Mikolov才把这个简单的模型取名叫做CBOW，简单却有效的典范。
需要注意的是这里每个词对应到两个词向量, 在上面的公式中都有体现, 其中 <span class="math inline">\(e\left(w_{t}\right)\)</span> 是词的输入 向量, 而 <span class="math inline">\(e^{\prime}\left(w_{t}\right)\)</span> 则是词的输出向量, 或者更准确的来讲, 前者是CBOW输入层中跟词 <span class="math inline">\(w_{t}\)</span> 所在 位置相连的所有边的权值 (其实这就是词向量) 组合成的向量, 而是输出层中与词 <span class="math inline">\(w_{t}\)</span> 所在位置 相连的所有边的权值组合成的向量, 所以把这一向量叫做输出向量。</p>
</blockquote>
</div>
<div id="skip-gram" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> skip-gram</h3>
<p>同样地, 和CBOW对应, Skip-gram的模型基本思想和CBOW非常类似, 只是换了一个方向： CBOW是让目标词的输出向量 <span class="math inline">\(e^{\prime}\left(w_{t}\right)\)</span> 拟合语境的向量 <span class="math inline">\(x\)</span>; 而Skip-gram则是让语境中每个词 的输出向量尽量拟合当前输入词的向量 <span class="math inline">\(e\left(w_{t}\right),\)</span> 和CBOW的方向相反, 因此它的目标函数如下：</p>
<p><span class="math display">\[
\begin{array}{c}
\frac{1}{T} \sum_{t=1}^{T} \sum_{j \in c} \log P\left(w_{j} \mid w_{t}\right) \\
P\left(w_{j} \mid w_{t}\right)=\frac{\exp \left(e^{\prime}\left(w_{j}\right)^{T} e\left(w_{t}\right)\right)}{\sum_{i=1}^{|V|} \exp \left(e^{\prime}\left(w_{i}\right)^{T} e\left(w_{t}\right)\right)}
\end{array}
\]</span></p>
<p>可以看出目标函数中有两个求和符号, 最里面的求和符号的意义便是让当前的输入词分别和该词对 应语境中的每一个词都尽量接近, 从而便可以表现为该词与其上下文尽量接近。</p>
<p>CBOW和skip-gram的两个目标函数是需要记得的，但是也要记得如果这么计算，去遍历每个词，那么一旦涉及大量的文本任务，其计算量会非常的大~</p>
<p>因此有了优化算法Hierachical Softmax和负采样。前者是霍夫曼树的思想。后者具体解释下~</p>
<p>负采样思想也是受了C&amp;W模型中构造负样本方法启发, 同时参考了Noise Contrastive Estimation (NCE)的思想, 用CBOW的框架简单来讲就是, 负采样 每遍历到一个目标词, 为了使得目标词的概率 <span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span> 最大，根据softmax函数的概率公式, 也 就是让分子中的 <span class="math inline">\(e^{\prime}\left(w_{t}\right)^{T} x\)</span> 最大, 而分母中其他非目标词的 <span class="math inline">\(e^{\prime}\left(w_{i}\right)^{T} x\)</span> 最小, 普通softmax的 计算量太大就是因为它把词典中所有其他非目标词都当做负例了，而负采样的思想特别简单，就是 每次按照一定概率随机采样一些词当做负例，从而就只需要计算这些负采样出来的负例了，那么概 率公式便相应变为</p>
<p><span class="math display">\[
P\left(w_{t} \mid c_{t}\right)=\frac{\exp \left(e^{\prime}\left(w_{t}\right)^{T} x\right)}{\sum_{i=1}^{K} \exp \left(e^{\prime}\left(w_{i}\right)^{T} x\right)}, x=\sum_{i \in c} e\left(w_{i}\right)
\]</span>
仔细和普通softmax进行比较便会发现，将原来的|V|分类问题变成了K分类问题, 这便把词典大小 对时间复杂度的影响变成了一个常数项, 而改动又非常的微小，不可谓不巧妙。</p>
<p>非常的巧妙~</p>
<p>跳词模型也有非常多的变形~后面的skip-thoughts就源于此</p>
</div>
</div>
<div id="fasttext" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> fasttext</h2>
<p>结构中比较重要的几个点</p>
<div id="hierarchical-softmax" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Hierarchical softmax</h3>
<p>这个没什么好说的，和word2vec里面的是一样的。具体解释忘记之前从哪里找来得了但私以为解释的8错</p>
<blockquote>
<p>When the number of classes is large, computing the linear classifier
is computationally expensive. More precisely, the computational
complexity is <span class="math inline">\(O(k h)\)</span> where <span class="math inline">\(k\)</span> is the number of classes and <span class="math inline">\(h\)</span> the
dimension of the text representation. In order to improve our running
time, we use a hierarchical softmax (Goodman, 2001) based on the
Huffman coding tree (Mikolov et al., 2013 ). During training, the
computational complexity drops to <span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span>. The
hierarchical softmax is also advantageous at test time when searching
for the most likely class. Each node is associated with a probability
that is the probability of the path from the root to that node. If the
node is at depth <span class="math inline">\(l+1\)</span> with parents <span class="math inline">\(n_{1}, \ldots, n_{l},\)</span> its
probability is <span class="math display">\[
P\left(n_{l+1}\right)=\prod_{i=1}^{l} P\left(n_{i}\right)
\]</span></p>
</blockquote>
<blockquote>
<p>This means that the probability of a node is always lower than the one
of its parent. Exploring the tree with a depth first search and
tracking the maximum probability among the leaves allows us to discard
any branch associated with a small probability. In practice, we
observe a reduction of the complexity to
<span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span> at test time. This approach is further
extended to compute the <span class="math inline">\(T\)</span> -top targets at the cost of <span class="math inline">\(O(\log (T)),\)</span>
using a binary heap.</p>
</blockquote>
<p>主要就是利用霍夫曼树加快计算的速度</p>
<blockquote>
<p>Hierachical
Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树,
保证词频较大的词处于相对比较浅的层,
词频较低的词相应的处于Huffman树较深层的叶子节点,
每一个词都处于这棵Huffman树上的某个叶子节点;
第二，将原本的一个|V|分类问题变成了<span class="math inline">\(\log|V|\)</span> 次的二分类问题,
做法简单说来就是, 原先要计算 <span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span>的时候,
因为使用的 是普通的softmax,
势必要求词典中的每一个词的概率大小，为了减少这一步的计算量, 在
Hierachical Softmax中，同样是计算当前词 <span class="math inline">\(w_{t}\)</span>
在其上下文中的概率大小，只需要把它变成在
Huffman树中的路径预测问题就可以了，因为当前词 <span class="math inline">\(w_{t}\)</span>
在Huffman树中对应到一条路径, 这条 路径由这棵二叉树中从根节点开始,
经过一系列中间的父节点, 最终到达当前这个词的叶子节点而 组成,
那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而
Huffman树的构造过程保证了树的深度为 <span class="math inline">\(\log |V|,\)</span>
所以也就只需要做<span class="math inline">\(\log |V|\)</span>次二分类便可以 求得
<span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span> 的大小, 这相比原来|V|次的计算量,
已经大大减小了。</p>
</blockquote>
</div>
<div id="n_gram-features" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> N_gram features</h3>
<p><strong>这个是主要区别于word2vec的输入的部分了，为了更好的学习到上下文的语序特征</strong></p>
<p>从bag of word 变成了bag of features</p>
<blockquote>
<p>Bag of words is invariant to word order but taking explicitly this
order into account is often computationally very expensive. Instead,
we use a bag of n-grams as additional features to capture some partial
information about the local word order. This is very efficient in
practice while achieving comparable results to methods that explicitly
use the order (Wang and Manning, 2012 ). We maintain a fast and memory
efficient mapping of the n-grams by using the hashing trick
(Weinberger et al., 2009 ) with the same hashing function as in
Mikolov et al. (2011) and <span class="math inline">\(10 \mathrm{M}\)</span> bins if we only used
bigrams, and <span class="math inline">\(100 \mathrm{M}\)</span> otherwise.</p>
</blockquote>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
fasttext 文本分类的结构？</li>
</ul>
</div>
<div id="fasttext与word2vec对比" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> fasttext与word2vec对比</h3>
<p>感觉读paper的时候并没有很仔细的找出fasttext和word2vec的区别</p>
<blockquote>
<p>word2vec和GloVe都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText则是利用带有监督标记的文本分类数据完成训练，本质上没有什么特殊的，模型框架就是CBOW。</p>
</blockquote>
<p>因为是训练词向量的嘛，因此只需要文本就可以了，不需要标签。</p>
<p>fasttext和word2vec本质无区别都是单层的神经网络，CBOW的结构，通过上下文预测当前词。
word2vec是为了得到embedding的矩阵，word2vec本质是一个词袋模型:bag of
word。</p>
</div>
<div id="fasttext与cbow有两点不同" class="section level3" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> fasttext与CBOW有两点不同</h3>
<blockquote>
<p>分别是输入数据和预测目标的不同 -
在输入数据上，CBOW输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而fastText为了利用更多的语序信息，将bag-of-words变成了bag-of-features，也就是下图中的输入x不再仅仅是一个词，还可以加上bigram或者是trigram的信息等等；</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>from gensim.models import FastText</span>
<span id="cb1-2"><a href="#cb1-2"></a>sentences =<span class="st"> </span>[[<span class="st">&quot;你&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;谁&quot;</span>], [<span class="st">&quot;我&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;中国人&quot;</span>]]</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>model =<span class="st"> </span><span class="kw">FastText</span>(sentences,  <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">window=</span><span class="dv">3</span>, <span class="dt">min_count=</span><span class="dv">1</span>, <span class="dt">iter=</span><span class="dv">10</span>,<span class="dt">min_n =</span> <span class="dv">3</span> , <span class="dt">max_n =</span> <span class="dv">6</span>,<span class="dt">word_ngrams =</span> <span class="dv">0</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>model[<span class="st">&#39;你&#39;</span>]  <span class="co"># 词向量获得的方式</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>model.wv[<span class="st">&#39;你&#39;</span>] <span class="co"># 词向量获得的方式</span></span></code></pre></div>
<p>所以在训练fasttext的词向量时候，参数word_ngrams =
0时候，是等价于word2vec的。</p>
<blockquote>
<p>第二个不同在于，CBOW预测目标是语境中的一个词，而fastText预测目标是当前这段输入文本的类别，正因为需要这个文本类别，因此才说fastText是一个监督模型。而相同点在于，fastText的网络结构和CBOW基本一致，同时在输出层的分类上也使用了Hierachical
Softmax技巧来加速训练。</p>
</blockquote>
<p>两者本质的不同，体现在 h-softmax的使用：</p>
<p>Wordvec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的
h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。
fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</p>
<ul>
<li>fasttext为什么快？</li>
</ul>
<blockquote>
<p>其实fasttext使用的模型与word2vec的模型在结构上是一样的，拿cbow来说，不同的只是在于word2vec cbow的目标是通过当前词的前后N个词来预测当前词，在使用层次softmax的时候，huffman树叶子节点处是训练语料里所有词的向量。</p>
</blockquote>
<p>下面这个解释原文中在hf的论述中并无很向下的介绍</p>
<blockquote>
<p>而fasttext在进行文本分类时，huffmax树叶子节点处是每一个类别标签的词向量，在训练的过程中，训练语料的每一个词也会得到对应的词向量，输入为一个window内的词对应的词向量，hidden layer为这几个词的线性相加，相加的结果作为该文档的向量，再通过层次softmax得到预测标签，结合文档的真实标签计算loss，梯度与迭代更新词向量。</p>
</blockquote>
<p>这样的话树的深度会小，因此遍历时间短了？</p>
<blockquote>
<p>fasttext有别于word2vec的另一点是加了ngram切分这个trick，将长词再通过ngram切分为几个短词，这样对于未登录词也可以通过切出来的ngram词向量合并为一个词。由于中文的词大多比较短，这对英文语料的用处会比中文语料更大。</p>
</blockquote>
</div>
<div id="实验和结果分析" class="section level3" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> 实验和结果分析</h3>
<p><em>情感分析实验</em></p>
<p>在8个数据集上面acc对比，对比了6个模型，可以看出在绝大部分的数据集上面fasttext的acc是最好的。
加入bgram的效果要优于不加的</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-87AA0A90.png" /></p>
<p>运行时间对比fasttext的速度绝了</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-099A1FC8.png" /></p>
<p>对比不同模型的acc，fasttext略高</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-283D5547.png" /></p>
<p>在标签预测上的测试时间，fasttext非常快</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-77B0769C.png" /></p>
</div>
</div>
<div id="doc2vec" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Doc2vec</h2>
<div id="introduction" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> introduction</h3>
<blockquote>
<p>However, the bag-of-words (BOW) has many disadvantages. The word order
is lost, and thus different sentences can have exactly the same
representation, as long as the same words are used. Even though
bag-of-n-grams considers the word order in short context, it suffers
from data sparsity and high dimensionality. Bag-of-words and
bagof-n-grams have very little sense about the semantics of the words
or more formally the distances between the words. This means that
words “powerful,” “strong” and “Paris” are equally distant despite the
fact that semantically, “powerful” should be closer to “strong” than
“Paris.”</p>
</blockquote>
<ul>
<li>词袋模型的缺点是没有考虑词序，学习不到语义；</li>
<li>bag-of-n-grams模型即使在短文本中也是存在高维稀疏问题的；</li>
<li>二者都无法学习到语义</li>
</ul>
<blockquote>
<p>In this paper, we propose Paragraph Vector, an unsupervised framework
that learns continuous distributed vector representations for pieces
of texts. The texts can be of variable-length, ranging from sentences
to documents. The name Paragraph Vector is to emphasize the fact that
the method can be applied to variable-length pieces of texts, anything
from a phrase or sentence to a large document.</p>
</blockquote>
<p>DOC2vec中提出了一种句向量的思想。文本是由不同长度的句子组成的，句向量可以学习到不同长度的短语和句子的embedding</p>
<blockquote>
<p>In our model, the vector representation is trained to be useful for
predicting words in a paragraph. More precisely, we concatenate the
paragraph vector with several word vectors from a paragraph and
predict the following word in the given context. Both word vectors and
paragraph vectors are trained by the stochastic gradient descent and
backpropagation (Rumelhart et al., 1986 ). While paragraph vectors are
unique among paragraphs, the word vectors are shared. At prediction
time, the paragraph vectors are inferred by fixing the word vectors
and training the new paragraph vector until convergence.</p>
</blockquote>
<p>说简单点就是在原有词向量的基础上concat上了句向量，同时学习词向量和句向量的语义。</p>
<p>个人感觉句向量的作用其实是增加了一个上下文的position，句向量的大小可以自定义。</p>
<p>Doc2vec同样具有2种结构</p>
<blockquote>
<p>The above method considers the concatenation of the paragraph vector
with the word vectors to predict the next word in a text window.
Another way is to ignore the context words in the input, but force the
model to predict words randomly sampled from the paragraph in the
output. In reality, what this means is that at each iteration of
stochastic gradient descent, we sample a text window, then sample a
random word from the text window and form a classification task given
the Paragraph Vector. This technique is shown in Figure <span class="math inline">\(3 .\)</span> We name
this version the Distributed Bag of Words version of Paragraph Vector
(PV-DBOW), as opposed to Distributed Memory version of Paragraph
Vector (PV-DM) in previous section.</p>
</blockquote>
<p>PV-DBOW是Distributed Bag of Words version of Paragraph
Vector，和Skip-gram类似，通过文档来预测文档内的词，训练的时候，随机采样一些文本片段，然后再从这个片段中采样一个词，让PV-DBOW模型来预测这个词，以此分类任务作为训练方法，说白了本质上和Skip-gram是一样的。这个方法有个致命的弱点，就是为了获取新文档的向量，还得继续走一遍训练流程，并且由于模型主要是针对文档向量预测词向量的过程进行建模，其实很难去表征词语之间的更丰富的语义结构。</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-CEE075C9.png" /></p>
<p>PV-DM的全称是Distributed Memory Model of Paragraph
Vectors，和CBOW类似，也是通过上下文预测下一个词，不过在输入层的时候，同时也维护了一个文档ID映射到一个向量的look-up
table，模型的目的便是将当前文档的向量以及上下文向量联合输入模型，并让模型预测下一个词，训练结束后，对于现有的文档，便可以直接通过查表的方式快速得到该文档的向量，而对于新的一篇文档，那么则需要将已有的look-up
table添加相应的列，然后重新走一遍训练流程，只不过此时固定好其他的参数，只调整look-up
table，收敛后便可以得到新文档对应的向量了。</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-64854E74.png" /></p>
</div>
<div id="实验对比" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> 实验对比</h3>
<p>Treebank Dataset 情感分析结果对比 基本都是不同长度的句子</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-CE415F36.png" /></p>
<p>情感分析结果对比，段落和文章上</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-EC1185C3.png" /></p>
<p>计算句间的距离对比</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-C86443E1.png" /></p>
<p>实际跑下来，Doc的效果并不如word2vec的效果好，是不是和样本数据量有关，亦或者和fasttext一样适用于英文。</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Doc2VEC是否可以直接使用word2vec的结果？</li>
</ul>
<p>个人人为应该可以使用，但是这里的句向量的计算的方式应该挺多的，计算出来句向量直接concat到词向量上面，理论上来说就是该篇文章的思想。</p>
</div>
</div>
<div id="skip-thoughts" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> skip-thoughts</h2>
<blockquote>
<p>Using the continuity of text from books, we train an encoderdecoder
model that tries to reconstruct the surrounding sentences of an
encoded passage. Sentences that share semantic and syntactic
properties are thus mapped to similar vector representations. We next
introduce a simple vocabulary expansion method to encode words that
were not seen as part of training, allowing us to expand our
vocabulary to a million words.</p>
</blockquote>
<p>skip-thoughts也是一种encoder-decoder结构，直接根据当前句预测上下文。</p>
<p>skip-gram是根据当前词预测上下文的词。目标不同。</p>
<div id="introduction-1" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> introduction</h3>
<blockquote>
<p>In this paper we abstract away from the composition methods themselves
and consider an alternative loss function that can be applied with any
composition operator. We consider the following question: is there a
task and a corresponding loss that will allow us to learn highly
generic sentence representations? We give evidence for this by
proposing a model for learning high-quality sentence vectors without a
particular supervised task in mind. Using word vector learning as
inspiration, we propose an objective function that abstracts the
skip-gram model of [8] to the sentence level. That is, instead of
using a word to predict its surrounding context, we instead encode a
sentence to predict the sentences around it. Thus, any composition
operator can be substituted as a sentence encoder and only the
objective function becomes modified. Figure 1 illustrates the model.
We call our model skip-thoughts and vectors induced by our model are
called skip-thought vectors.</p>
</blockquote>
<p>也是在强调直接学习句向量，根据当前的句子预测上下文的句子向量。
<eos>是每个句子的结尾</p>
<p><img src="figs/skip-thoughts.png" /></p>
<p>当前句<span class="math inline">\(s_{i}\)</span>预测上下句<span class="math inline">\(s_{i-1}\)</span>和<span class="math inline">\(s_{i+1}\)</span></p>
</div>
<div id="结构" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> 结构</h3>
<blockquote>
<p>We treat skip-thoughts in the framework of encoder-decoder models
<span class="math inline">\(1 .\)</span> That is, an encoder maps words to a sentence vector and a
decoder is used to generate the surrounding sentences. Encoderdecoder
models have gained a lot of traction for neural machine translation.
In this setting, an encoder is used to map e.g. an English sentence
into a vector. The decoder then conditions on this vector to generate
a translation for the source English sentence. Several choices of
encoder-decoder pairs have been explored, including ConvNet-RNN [10],
RNN-RNN [11] and LSTM-LSTM [12]. The source sentence representation
can also dynamically change through the use of an attention mechanism
[13] to take into account only the relevant words for translation at
any given time. In our model, we use an RNN encoder with GRU [14]
activations and an RNN decoder with a conditional GRU. This model
combination is nearly identical to the RNN encoder-decoder of [11]
used in neural machine translation. GRU has been shown to perform as
well as LSTM [2] on sequence modelling tasks [14] while being
conceptually simpler. GRU units have only 2 gates and do not require
the use of a cell. While we use RNNs for our model, any encoder and
decoder can be used so long as we can backpropagate through it.</p>
</blockquote>
<p>常见的编码器和解码器的结构有ConvNet-RNN , RNN-RNN and LSTM-LSTM</p>
<p>Skip-Though模型希望通过编码中间的句子来预测其前一个句子和后一个句子，前一个句子和后一个句子分别用不同的解码器进行解码，也就是根据中间句子的句向量表示进行自回归的Decoder把句子解码出来，这借鉴了机器翻译中的思想。</p>
<p>使用两个独立的Decoder分别建模前一句和后一句是为了用独立的语义去编码前一句和后一句。</p>
<p>skip-thought模型的神经网络结构是在机器翻译中最常用的 Encoder-Decoder 结构，而在 Encoder-Decoder 架构中所使用的模型是GRU模型。因此在训练句子向量时同样要使用到词向量，编码器输出的结果为句子中最后一个词所输出的向量。
GRU对比LSTM从速度上面来说是应该是更快的，效果上来看，实际数据中差不多~</p>
</div>
<div id="encoder" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> encoder</h3>
<p>Skip-Thought模型的编码器部分使用GRU进行Encoder, GRU中有更新门和重置门，更新门对应 <span class="math inline">\(z^{t},\)</span> 重置门对应 <span class="math inline">\(r^{t}\)</span> 。更新门用于控制前一 个时刻的信息被带入当前时刻的程度, 更新门的值越大, 说明前一时刻的信息带入当前时刻越多。重置门控制的是前一时刻有多少信息被 写入到当前时刻的候选集。</p>
<blockquote>
<p>Encoder. Let <span class="math inline">\(w_{i}^{1}, \ldots, w_{i}^{N}\)</span> be the words in sentence
<span class="math inline">\(s_{i}\)</span> where <span class="math inline">\(N\)</span> is the number of words in the sentence. At each time
step, the encoder produces a hidden state <span class="math inline">\(\mathbf{h}_{i}^{t}\)</span> which
can be interpreted as the representation of the sequence
<span class="math inline">\(w_{i}^{1}, \ldots, w_{i}^{t} .\)</span> The hidden state <span class="math inline">\(\mathbf{h}_{i}^{N}\)</span>
thus represents the full sentence. To encode a sentence, we iterate
the following sequence of equations (dropping the subscript <span class="math inline">\(i\)</span> ):</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{x}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}\right) \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{x}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}\right) \\
\overline{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W} \mathbf{x}^{t}+\mathbf{U}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)\right) \\
\mathbf{h}^{t} &amp;=\left(1-\mathbf{z}^{t}\right) \odot \mathbf{h}^{t-1}+\mathbf{z}^{t} \odot \overline{\mathbf{h}}^{t}
\end{aligned}
\]</span> where <span class="math inline">\(\overline{\mathbf{h}}^{t}\)</span> is the proposed state update at
time <span class="math inline">\(t, \mathbf{z}^{t}\)</span> is the update gate, <span class="math inline">\(\mathbf{r}_{t}\)</span> is the
reset gate <span class="math inline">\((\odot)\)</span> denotes a component-wise product. Both update gates
takes values between zero and one.</p>
<p>encoder部分就是一个GRU的结构进行特征选择</p>
<p>编码器的作用：编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量
c，并在该背景变量中编码输⼊序列信息。</p>
</div>
<div id="decoder" class="section level3" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> decoder</h3>
<blockquote>
<p>Decoder. The decoder is a neural language model which conditions on
the encoder output <span class="math inline">\(\mathbf{h}_{i} .\)</span> The computation is similar to
that of the encoder except we introduce matrices
<span class="math inline">\(\mathbf{C}_{z}, \mathbf{C}_{r}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> that are used to
bias the update gate, reset gate and hidden state computation by the
sentence vector. One decoder is used for the next sentence <span class="math inline">\(s_{i+1}\)</span>
while a second decoder is used for the previous sentence <span class="math inline">\(s_{i-1}\)</span>.
Separate parameters are used for each decoder with the exception of
the vocabulary matrix <span class="math inline">\(\mathbf{V}\)</span> which is the weight matrix
connecting the decoder’s hidden state for computing a distribution
over words. In what follows we describe the decoder for the next
sentence <span class="math inline">\(s_{i+1}\)</span> although an analogous computation is used for the
previous sentence <span class="math inline">\(s_{i-1}\)</span>. Let <span class="math inline">\(\mathbf{h}_{i+1}^{t}\)</span> denote the
hidden state of the decoder at time <span class="math inline">\(t .\)</span> Decoding involves iterating
through the following sequence of equations (dropping the subscript
<span class="math inline">\(i+1\)</span> ):</p>
</blockquote>
<p>decoder的输入是encoder的输出，两个解码器分别对当前句的上下句进行解码。
下面给出了预测 <span class="math inline">\(s_{i+1}\)</span>的，预测 <span class="math inline">\(s_{i-1}\)</span>同上</p>
<p>Decoder部分使用的同样是GRU，Decoder部分的GRU是带有条件信息的，也就是编码器得到的中间句子的编码信息<span class="math inline">\(h_{i}\)</span>，从而使得Encoder部分的GRU每次都能携带中间句子的信息做出决策。</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r}^{d} \mathbf{x}^{t-1}+\mathbf{U}_{r}^{d} \mathbf{h}^{t-1}+\mathbf{C}_{r} \mathbf{h}_{i}\right) \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z}^{d} \mathbf{x}^{t-1}+\mathbf{U}_{z}^{d} \mathbf{h}^{t-1}+\mathbf{C}_{z} \mathbf{h}_{i}\right) \\
\overline{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}^{d} \mathbf{x}^{t-1}+\mathbf{U}^{d}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{C h}_{i}\right) \\
\mathbf{h}_{i+1}^{t} &amp;=\left(1-\mathbf{z}^{t}\right) \odot \mathbf{h}^{t-1}+\mathbf{z}^{t} \odot \overline{\mathbf{h}}^{t}
\end{aligned}
\]</span></p>
<p>Given <span class="math inline">\(\mathbf{h}_{i+1}^{t},\)</span> the probability of word <span class="math inline">\(w_{i+1}^{t}\)</span>
given the previous <span class="math inline">\(t-1\)</span> words and the encoder vector is <span class="math display">\[
P\left(w_{i+1}^{t} \mid w_{i+1}^{&lt;t}, \mathbf{h}_{i}\right) \propto \exp \left(\mathbf{v}_{w_{i+1}^{t}} \mathbf{h}_{i+1}^{t}\right)
\]</span> where <span class="math inline">\(\mathbf{v}_{w_{i+1}^{t}}\)</span> denotes the row of <span class="math inline">\(\mathbf{V}\)</span>
corresponding to the word of <span class="math inline">\(w_{i+1}^{t} .\)</span> An analogous computation is
performed for the previous sentence <span class="math inline">\(s_{i-1}\)</span>.</p>
<p>解码器部分使用的网络结构也是GRU</p>
</div>
<div id="目标函数" class="section level3" number="1.4.5">
<h3><span class="header-section-number">1.4.5</span> 目标函数</h3>
<blockquote>
<p>Objective. Given a tuple <span class="math inline">\(\left(s_{i-1}, s_{i}, s_{i+1}\right),\)</span> the
objective optimized is the sum of the log-probabilities for the
forward and backward sentences conditioned on the encoder
representation: <span class="math display">\[
\sum_{t} \log P\left(w_{i+1}^{t} \mid w_{i+1}^{&lt;t}, \mathbf{h}_{i}\right)+\sum_{t} \log P\left(w_{i-1}^{t} \mid w_{i-1}^{&lt;t}, \mathbf{h}_{i}\right)
\]</span></p>
</blockquote>
<p>预测上下句的损失函数之和。</p>
</div>
<div id="词典的拓展" class="section level3" number="1.4.6">
<h3><span class="header-section-number">1.4.6</span> 词典的拓展</h3>
<blockquote>
<p>We now describe how to expand our encoder’s vocabulary to words it has not seen during training. Suppose we have a model that was trained to induce word representations, such as word2vec. Let <span class="math inline">\(V_{w 2 v}\)</span> denote the word embedding space of these word representations and let <span class="math inline">\(V_{r n n}\)</span> denote the <span class="math inline">\(\mathrm{RNN}\)</span> word embedding space. We assume the vocabulary of <span class="math inline">\(\mathcal{V}_{w 2 v}\)</span> is much larger than that of <span class="math inline">\(\mathcal{V}_{r n n}\)</span>. Our goal is to construct a mapping <span class="math inline">\(f: \mathcal{V}_{w 2 v} \rightarrow \mathcal{V}_{r n n}\)</span> parameterized by a matrix <span class="math inline">\(\mathbf{W}\)</span> such that <span class="math inline">\(\mathbf{v}^{\prime}=\mathbf{W} \mathbf{v}\)</span> for <span class="math inline">\(\mathbf{v} \in \mathcal{V}_{w 2 v}\)</span> and <span class="math inline">\(\mathbf{v}^{\prime} \in \mathcal{V}_{r n n} .\)</span> Inspired by [15] , which learned linear mappings between translation word spaces, we solve an un-regularized L2 linear regression loss for the matrix <span class="math inline">\(\mathbf{W}\)</span>. Thus, any word from <span class="math inline">\(\mathcal{V}_{w 2 v}\)</span> can now be mapped into <span class="math inline">\(\mathcal{V}_{r n n}\)</span> for encoding sentences. Table 3 shows examples of nearest neighbour words for queries that did not appear in our training vocabulary.</p>
</blockquote>
<p>对于encoder部分，如何对词库中未出现的词进行编码。</p>
<ol style="list-style-type: decimal">
<li>用 <span class="math inline">\(V_{w 2 v}\)</span> 表示训练的词向量空间, 用 <span class="math inline">\(V_{r n n}\)</span> 表示模型中的词向量空间，在这里 <span class="math inline">\(V_{w 2 v}\)</span> 词的数量是远远大于 <span class="math inline">\(V_{r n n}\)</span> 的。</li>
<li>引入一个矩阵 <span class="math inline">\(W\)</span> 来构建一个映射函数: <span class="math inline">\(\mathrm{f}: V_{r n n}-&gt;V_{w 2 v}\)</span> 。使得有 <span class="math inline">\(v \prime=W v,\)</span> 其中 <span class="math inline">\(v \in V_{w 2 v}, v \prime \in V_{r n n}\)</span> 。</li>
<li>通过映射函数就可以将任何在 <span class="math inline">\(V_{w 2 v}\)</span> 中的词映射到 <span class="math inline">\(V_{r n n}\)</span> 中。</li>
</ol>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
从w2vec词向量的词表中做一个映射到rnn的词表上，得到一个映射的矩阵</li>
</ul>
<p>这个是说加载预训练模型作为第一层的embedding吗？</p>
<blockquote>
<p>We note that there are alternate strategies for solving the vocabulary problem. One alternative is to initialize the RNN embedding space to that of pre-trained word vectors. This would require a more sophisticated softmax for decoding, or clipping the vocabulary of the decoder as it would be too computationally expensive to naively decode with vocabularies of hundreds of thousands of words. An alternative strategy is to avoid words altogether and train at the character level.</p>
</blockquote>
<p>以词作为预训练的词向量，需要非常复杂的softmax做解码，这样计算的代码比较大，确实。。每次算词频都需要遍历整个词库，计算量很大。。很慢。
因此有了以字符为基本单位的可代替方案</p>
<p>这一段的意义何在？skip-thoughts中还是以词为基本做的预训练，没有用到字符啊</p>
</div>
<div id="实验部分" class="section level3" number="1.4.7">
<h3><span class="header-section-number">1.4.7</span> 实验部分</h3>
<blockquote>
<p>In our experiments, we evaluate the capability of our encoder as a generic feature extractor after training on the BookCorpus dataset. Our experimentation setup on each task is as follows:</p>
</blockquote>
<p>在BookCorpus数据集上进行训练，每个任务如下</p>
<blockquote>
<p>Using the learned encoder as a feature extractor, extract skip-thought vectors for all sentences.</p>
</blockquote>
<p>encoder部分：使用skip-th-vec提取所有句子特征</p>
<blockquote>
<p>If the task involves computing scores between pairs of sentences, compute component-wise features between pairs. This is described in more detail specifically for each experiment.</p>
</blockquote>
<p>若需要计算两个句子之间的得分，则计算它们之间的成分特征。</p>
<blockquote>
<p>Train a linear classifier on top of the extracted features, with no additional fine-tuning or backpropagation through the skip-thoughts model.</p>
</blockquote>
<p>在提取的特征上面训练一个线性的分类器，无需额外的微调和反向传播</p>
<blockquote>
<p>We restrict ourselves to linear classifiers for two reasons. The first is to directly evaluate the representation quality of the computed vectors. It is possible that additional performance gains can be made throughout our experiments with non-linear models but this falls out of scope of our goal. Furthermore, it allows us to better analyze the strengths and weaknesses of the learned representations. The second reason is that reproducibility now becomes very straightforward.</p>
</blockquote>
<p>严格使用线性分类器有2个原因：</p>
<ul>
<li>第一种是直接评估计算出的向量的表征能力。对非线性模型的实验，有可能获得额外的性能提高，此外能更好地分析表征学习的优缺点。</li>
<li>第二个原因是再现性变得非常直接。（这一点没有明白）</li>
</ul>
</div>
<div id="实验部分-1" class="section level3" number="1.4.8">
<h3><span class="header-section-number">1.4.8</span> 实验部分</h3>
<p><img src="figs/skip-th-vec-exp1.png" /></p>
<p>左边的部门是计算相似性的实验，评价指标是皮尔森相关系数和斯皮尔曼相关系数，无监督的实验。
从结果上看skip-vec这个的结果不是最优的，属于中等偏上了。</p>
<p>右边的是一个二分类的实验，评价指标是ACC和F1.</p>
<p>还有计算句子相似语义的实验和情感分析等的实验，可以参考原文，略。</p>
</div>
</div>
<div id="elmo" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> ELMO</h2>
<div id="introduction-2" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> introduction</h3>
<blockquote>
<p>Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017 ), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.</p>
</blockquote>
<p>ELMo是双向的语言模型：两个BILSTM，ELMO学习了在每个结束任务的每个输入词之上堆叠的向量的线性组合，这比仅仅使用LSTM顶层显著提高了性能。</p>
<blockquote>
<p>Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modiﬁcation to perform well on supervised word sense disambiguation tasks) while lower-level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). Simultaneously exposing all of these signals is highly beneﬁcial, allowing the learned models select the types of semi-supervision that are most useful for each end task</p>
</blockquote>
<p>以这种方式组合内部状态可以实现非常丰富的单词表示。使用内在的评价,高级LSTM捕获词义的上下文相关的方面(例如,他们可以使用不需要修改监督词义消歧任务上的表现良好)虽然低级状态模型方面的语法(例如,他们可以用来做词性标注)。同时暴露所有这些信号是非常有益的，允许学习模型选择对每个最终任务最有用的半监督类型。</p>
<ul class="task-list">
<li><p><input type="checkbox" disabled="" />
产生上下文相关的词向量，怎么产生的？</p></li>
<li><p><input type="checkbox" disabled="" />
这里提到了消岐任务，但是具体是如何做到消岐的？</p></li>
</ul>
</div>
<div id="双向语言模型" class="section level3" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> 双向语言模型</h3>
<blockquote>
<p>Given a sequence of <span class="math inline">\(N\)</span> tokens, <span class="math inline">\(\left(t_{1}, t_{2}, \ldots, t_{N}\right),\)</span> a forward language model computes the probability of the sequence by modeling the probability of token <span class="math inline">\(t_{k}\)</span> given the history <span class="math inline">\(\left(t_{1}, \ldots, t_{k-1}\right)\)</span></p>
</blockquote>
<p>给定一个含有N个tokens的序列，根据上文计算当前的token，前向的表示为</p>
<p><span class="math display">\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right)
\]</span></p>
<blockquote>
<p>Recent state-of-the-art neural language models (Józefowicz et al., <span class="math inline">\(2016 ;\)</span> Melis et al., <span class="math inline">\(2017 ;\)</span> Merity et al., 2017 ) compute a context-independent token representation <span class="math inline">\(\mathrm{x}_{k}^{L M}\)</span> (via token embeddings or a CNN over characters) then pass it through <span class="math inline">\(L\)</span> layers of forward LSTMs. At each position <span class="math inline">\(k,\)</span> each LSTM layer outputs a context-dependent representation <span class="math inline">\(\overrightarrow{\mathbf{h}}_{k, j}^{L M}\)</span> where <span class="math inline">\(j=1, \ldots, L .\)</span> The top layer LSTM output, <span class="math inline">\(\overrightarrow{\mathbf{h}}_{k, L}^{L M},\)</span> is used to predict the next token <span class="math inline">\(t_{k+1}\)</span> with a Softmax layer.</p>
</blockquote>
<p>输入的token是<span class="math inline">\({x}_{k}^{L M}\)</span>,L是lstm的层数，在每一个位置 k ，每一个LSTM 层都输出相应的context-dependent的表征<span class="math inline">\(\overrightarrow{\mathbf{h}}_{k, j}^{L M}\)</span>，j=1,2….L,通过Softmax layer预测下一个<span class="math inline">\(t_{k+1}\)</span></p>
<blockquote>
<p>A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context:</p>
</blockquote>
<p>后向语言模型：学习下文的知识，后向的表示为</p>
<p><span class="math display">\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
\]</span></p>
<blockquote>
<p>It can be implemented in an analogous way to a forward LM, with each backward LSTM layer <span class="math inline">\(j\)</span> in a <span class="math inline">\(L\)</span> layer deep model producing representations <span class="math inline">\(\overleftarrow{\mathbf{h}}_{k, j}^{L M}\)</span> of <span class="math inline">\(t_{k}\)</span> given <span class="math inline">\(\left(t_{k+1}, \ldots, t_{N}\right)\)</span></p>
</blockquote>
<p>前向的语言模型是一个lstm网络层，后向的也是一个lstm网络层，相当于两个lstm做stacking</p>
<blockquote>
<p>A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions:</p>
</blockquote>
<p>目标函数就是前向后向语言模型的最大似然函数之和</p>
<p><span class="math display">\[
\begin{array}{l}
\sum_{k=1}^{N}\left(\log p\left(t_{k} \mid t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)\right. \\
\left.\quad+\log p\left(t_{k} \mid t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)
\end{array}
\]</span>
We tie the parameters for both the token representation <span class="math inline">\(\left(\Theta_{x}\right)\)</span> and Softmax layer <span class="math inline">\(\left(\Theta_{s}\right)\)</span> in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction. Overall, this formulation is similar to the approach of Peters et al. ( 2017 ), with the exception that we share some weights between directions instead of using completely independent parameters. In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers.</p>
<p>两个lstm层的参数不共享，单独训练
但是在两个lstm之间会有一些共享权重参数</p>
</div>
<div id="elmo结构" class="section level3" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> elmo结构</h3>
<blockquote>
<p>ELMo is a task speciﬁc combination of the intermediate layer representations in the biLM. For each token <span class="math inline">\(t_{k},\)</span> a <span class="math inline">\(L\)</span> -layer biLM computes a set of <span class="math inline">\(2 L+1\)</span> representations</p>
</blockquote>
<p>ELMo是biLM中中间层表示的特定于任务的组合.对于每个token，一个L层的biLM要计算出 2L+1 个表征</p>
<p><span class="math display">\[
\begin{aligned}
R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overline{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\
&amp;=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{h}_{k=0}^{L M}\)</span> is the token layer and <span class="math inline">\(\mathbf{h}_{k, j}^{L M}=\)</span> <span class="math inline">\(\left[\overrightarrow{\mathbf{h}}_{k, j}^{L M} ; \overleftarrow{\mathbf{h}}_{k, j}^{L M}\right],\)</span> for each biLSTM layer.</p>
<p>在上面 <span class="math inline">\(X_{k}^{L M}\)</span> 等于 <span class="math inline">\(h_{k, j}^{L M},\)</span> 表示的是token层的值。</p>
<p>For inclusion in a downstream model, ELMo collapses all layers in <span class="math inline">\(R\)</span> into a single vector, <span class="math inline">\(\mathbf{E L M o}_{k}=E\left(R_{k} ; \Theta_{e}\right) .\)</span> In the simplest case ELMo just selects the top layer, <span class="math inline">\(E\left(R_{k}\right)=\mathbf{h}_{k, L}^{L M}\)</span> as in TagLM (Peters et al., 2017 ) and CoVe (McCann et al., 2017 ). More generally, we compute a task specific weighting of all biLM layers:</p>
<p>在下游的任务中, ELMo把所有层的R压缩在一起形成一个向量。(在最简单的情况下, 可以只保留最后一层的 <span class="math inline">\(h_{k, L}^{L M}\)</span> 。 <span class="math inline">\()\)</span></p>
<p><span class="math display">\[
\mathbf{E L M o}_{k}^{\text {task}}=E\left(R_{k} ; \Theta^{\text {task}}\right)=\gamma^{\text {task}} \sum_{j=0}^{L} s_{j}^{\text {task}} \mathbf{h}_{k, j}^{L M}
\]</span>
In (1), s <span class="math inline">\(^{\text {task }}\)</span> are softmax-normalized weights and the scalar parameter <span class="math inline">\(\gamma^{\text {task}}\)</span> allows the task model to scale the entire ELMo vector. <span class="math inline">\(\gamma\)</span> is of practical importance to aid the optimization process (see supplemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016 ) to each biLM layer before weighting.</p>
<p>e</p>
<p>其中 } <span class="math inline">\(s_{j}^{task }\)</span>是softmax标准化权重,<span class="math inline">\(\gamma^{\text {task}}\)</span>是缩放系数，允许任务模型去缩放整个ELMO向量。</p>
<div id="using-bilms-for-supervised-nlp-tasks" class="section level4" number="1.5.3.1">
<h4><span class="header-section-number">1.5.3.1</span> Using biLMs for supervised NLP tasks</h4>
<p>双向语言模型在有监督NLP的任务上如何做representations</p>
<blockquote>
<p>Given a pretrained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model. We simply run the biLM and record all of the layer representations for each word. Then, we let the end task model learn a linear combination of these representations, as described below.</p>
</blockquote>
<p>给定一个预先训练好的biLM和一个目标NLP任务的监督架构，使用biLM来改进任务模型是一个简单的过程。只需运行biLM并记录每个单词的所有层表示。然后让最终任务模型学习这些表示的线性组合。</p>
<blockquote>
<p>First consider the lowest layers of the supervised model without the biLM. Most supervised NLP models share a common architecture at the
lowest layers, allowing us to add ELMo in a consistent, unified manner. Given a sequence of tokens <span class="math inline">\(\left(t_{1}, \ldots, t_{N}\right),\)</span> it is standard to form a context-independent token representation <span class="math inline">\(\mathbf{x}_{k}\)</span> for each token position using pre-trained word embeddings and optionally character-based representations. Then, the model forms a context-sensitive representation <span class="math inline">\(\mathbf{h}_{k},\)</span> typically using either bidirectional RNNs, CNNs, or feed forward networks.</p>
</blockquote>
<blockquote>
<p>To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo <span class="math inline">\(_{k}^{\text {task}}\)</span> with <span class="math inline">\(\mathrm{x}_{k}\)</span> and pass the ELMo enhanced representation <span class="math inline">\(\left[\mathrm{x}_{k} ;\right.\)</span> ELMo <span class="math inline">\(\left._{k}^{\text {task }}\right]\)</span> into the task RNN. For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing <span class="math inline">\(\mathbf{h}_{k}\)</span> with <span class="math inline">\(\left[\mathbf{h}_{k} ; \mathbf{E L M o}_{k}^{\text {task}}\right] .\)</span> As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.</p>
</blockquote>
<p>使用预训练的ELMO embedding</p>
<blockquote>
<p>Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al.,
2014) and in some cases to regularize the ELMo weights by adding <span class="math inline">\(\lambda\|\mathbf{w}\|_{2}^{2}\)</span> to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.</p>
</blockquote>
<p>在bilm后增加dropout正则化方式</p>
<p>这对ELMo权重施加了一种诱导偏差，以保持接近所有biLM层的平均值。</p>
</div>
<div id="pre-trained-bidirectional-language-model-architecture" class="section level4" number="1.5.3.2">
<h4><span class="header-section-number">1.5.3.2</span> Pre-trained bidirectional language model architecture</h4>
<p>预训练的双向语言模型</p>
<blockquote>
<p>The pre-trained biLMs in this paper are similar to the architectures in Józefowicz et al. (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers. We focus on large scale biLMs in this work, as Peters et al. (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training.</p>
</blockquote>
<ul>
<li><p>产生pre-trained biLM模型。模型由两层bi-LSTM组成，之间用residual connection连接起来。</p></li>
<li><p>在任务语料上(注意是语料，忽略label)fine tuning上一步得到的biLM模型。可以把这一步看为biLM的domain transfer。</p></li>
<li><p>利用ELMo的word embedding来对任务进行训练。通常的做法是把它们作为输入加到已有的模型中，一般能够明显的提高原模型的表现。</p></li>
</ul>
</div>
</div>
<div id="evaluation" class="section level3" number="1.5.4">
<h3><span class="header-section-number">1.5.4</span> Evaluation</h3>
<p><img src="figs/ELMO_ACC.png" /></p>
<p>对比在几个数据集上之前stoa的acc和f1,从结果来看，效果都有刷新之前的sota
SQuAD是斯坦福的一个问答的数据集</p>
<p><img src="figs/ELMO-RE.png" /></p>
<p>表2是增加了正则化参数之后</p>
</div>
<div id="analysis" class="section level3" number="1.5.5">
<h3><span class="header-section-number">1.5.5</span> analysis</h3>
<blockquote>
<p>since adding ELMo improves task performance over word vectors alone, the biLM’s contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors. Intuitively, the biLM must be disambiguating the meaning of words using their context. Consider “play”, a highly polysemous word. The top of Table 4 lists nearest neighbors to “play” using GloVe vectors. They are spread across several parts of speech (e.g., “played”, “playing” as verbs, and “player”, “game” as nouns) but concentrated in the sportsrelated senses of “play”. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM’s context representation of “play” in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence.
intrinsic evaluation of the contextual representations similar to Belinkov et al. (2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers.
Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1 nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0 , our training corpus (Miller et al., 1994 ), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.</p>
</blockquote>
<p>由于添加ELMo比单独使用词向量提高了任务性能，因此biLM的上下文表示必须编码在词向量中没有捕捉到的通常对NLP任务有用的信息。直观地说，biLM必须使用上下文来消除词语的歧义。以“play”为例，这是一个高度多义词。表4的顶部列出了使用手套矢量“玩”的最近邻居。它们分布在几个词性中(例如，“玩”、“玩”作为动词，“玩家”、“游戏”作为名词)，但集中在与体育运动相关的“玩”含义中。相比之下，底部两行显示SemCor数据集(见下)中使用源句子中“play”的biLM上下文表示的最近的句子。在这些情况下，biLM能够消除源句子中的词性和词义的歧义。与Belinkov等人类似，上下文表征的内在评估。为了隔离由biLM编码的信息，表示用于直接预测细粒度词义消歧(WSD)任务和词性标记任务。使用这种方法，还可以跨每个单独的层与CoVe进行比较。单词词义消歧对于一个句子，可以使用biLM表示来预测目标单词的意义，使用简单的1最近邻方法，类似于Melamud等人。为此，首先使用biLM来计算训练语料库SemCor 3.0中所有单词的表示，然后取每种SENSE的平均表示。在测试时，再次使用biLM来计算给定目标词的表示，并从训练集中获取最近邻的意义，对于训练期间未观察到的引理，则返回到WordNet的第一个意义。</p>
<p>因为ELMO能学习到上下文的语义信息，使用上下文的语义进行消岐</p>
<blockquote>
<p>Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et
al. (2017a). Overall, the biLM top layer representations have <span class="math inline">\(\mathrm{F}_{1}\)</span> of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016 ) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.</p>
</blockquote>
<p><img src="figs/ELMO-DS.png" /></p>
<p>表5比较了Raganato et al. 使用Raganato et al. 的评估框架在Raganato et al. 的同一套四组测试集上的WSD结果。总的来说，biLM顶层表示的<span class="math inline">\(\mathrm{F}_{1}\)</span>为69.0，在WSD方面优于第一层。这与使用手工制作特性的最先进的特定于wsd的监督模型(Iacobacci等人)和使用辅助粗粒度语义标签和POS标签训练的特定于任务的biLSTM 竞争。</p>
</div>
</div>
<div id="attention-is-all-you-need" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Attention is all you need</h2>
<blockquote>
<p>the Transformer,based solely on attention mechanisms, dispensing with
recurrence and convolutions entirely.</p>
</blockquote>
<p>transformer只依靠attention机制，舍弃了之前的rnn和cnn的结构</p>
<blockquote>
<p>Our model achieves 28.4 BLEU on the WMT 2014 English-to-German
translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French
translation task,our model establishes a new single-model
state-of-the-art BLEU score of 41.8 after training for 3.5 days on
eight GPUs, a small fraction of the training costs of the best models
from the literature.</p>
</blockquote>
<p>非常的消耗算力，因此后面的很多学者研究模型压缩</p>
<blockquote>
<p>Recurrent models typically factor computation along the symbol
positions of the input and output sequences. Aligning the positions to
steps in computation time, they generate a sequence of hidden states
<span class="math inline">\(h_{t},\)</span> as a function of the previous hidden state <span class="math inline">\(h_{t-1}\)</span> and the
input for position <span class="math inline">\(t .\)</span> This inherently sequential nature precludes
parallelization within training examples, which becomes critical at
longer sequence lengths, as memory constraints limit batching across
examples.</p>
</blockquote>
<p>RNN的<span class="math inline">\(h_t\)</span>是同时接受<span class="math inline">\(x_t\)</span>和<span class="math inline">\(h_{t-1}\)</span>的影响的</p>
<p>但是RNN相关算法只能从左向右依次计算或者从右向左依次计算缺少全局的依赖
但是还是短距离依赖，没法解决梯度消失，长距离依赖的问题
因此出现了lstm和gru</p>
<blockquote>
<p>Attention mechanisms have become an integral part of compelling
sequence modeling and transduction models in various tasks, allowing
modeling of dependencies without regard to their distance in the input
or output sequences .</p>
</blockquote>
<p>attention在序列模型传导机制中允许对依赖项进行建模而<strong>无需考虑它们之间的输入距离或输出序列</strong></p>
<blockquote>
<p>The goal of reducing sequential computation also forms the foundation
of the Extended Neural GPU |16], ByteNet [18] and ConvS2S [9], all of
which use convolutional neural networks as basic building block,
computing hidden representations in parallel for all input and output
positions. In these models, the number of operations required to
relate signals from two arbitrary input or output positions grows in
the distance between positions, linearly for ConvS2S and
logarithmically for ByteNet. This makes it more difficult to learn
dependencies between distant positions <span class="math inline">\([12] .\)</span> In the Transformer
this is reduced to a constant number of operations, albeit at the cost
of reduced effective resolution due to averaging attention-weighted
positions, an effect we counteract with Multi-Head Attention</p>
</blockquote>
<p>前人的研究有使用卷积神经进行序列建模建立block结构（卷积核？）并行计算所有输入和输出位置的隐藏表示，在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数随位置之间的距离而增加，对于ConvS2S的参数呈线性增长，而对于ByteNet参数则对数增长。
这使得学习远位置之间的依赖关系变得更加困难。</p>
<p>在Transformer中，讲参数减少到一个固定的维度，尽管这是由于平均注意力加权位置而导致有效分辨率降低的结果，可以使用多头注意力抵消这种影响~</p>
<p><strong>所以多头注意力机制是为了限制参数增长的？解决这个问题之前先知道cnn是怎样让参数爆炸的？</strong></p>
<blockquote>
<p>Self-attention, sometimes called intra-attention is an attention
mechanism relating different positions of a single sequence in order
to compute a representation of the sequence.</p>
</blockquote>
<p>自我注意（有时称为内部注意）是一种<strong>与单个序列的不同位置相关的注意力机制</strong>，目的是计算序列的表示形式。</p>
<p>这里看下之前的注意力机制的讲解<a href="https://www.cnblogs.com/gaowenxingxing/p/12674810.html">attention</a></p>
<blockquote>
<p>Transformer is the first transduction model relying entirely on
self-attention to compute representations of its input and output
without using sequencealigned RNNs or convolution.</p>
</blockquote>
<p>Transformer是第一个完全依靠自我注意力来计算其输入和输出表示的转导模型，而无需使用序列对齐的RNN或卷积</p>
<blockquote>
<p>The Transformer follows this overall architecture using stacked
self-attention and point-wise, fully connected layers for both the
encoder and decoder, shown in the left and right halves of Figure
1,respectively.</p>
</blockquote>
<p>下面介绍的transfomer都是基于自注意力elf-attention和
point-wise（计算注意力时候用的是点积的形式:可以理解为逐点扫描就像kernel
size为1的卷积操作,对输出的每一个位置做同样的变化?)</p>
<p><img src="https://img2020.cnblogs.com/blog/1365906/202008/1365906-20200815115042282-1192701119.png" /></p>
<p>transformer的结构也是由encoder和decoder组成</p>
<blockquote>
<p><strong>encoder</strong>: The encoder is composed of a stack of <span class="math inline">\(N=6\)</span> identical
layers. Each layer has two sub-layers. The first is a multi-head
self-attention mechanism, and the second is a simple, positionwise
fully connected feed-forward network. We employ a residual connection
[11] around each of the two sub-layers, followed by layer
normalization [1]. That is, the output of each sub-layer is LayerNorm
<span class="math inline">\((x+\)</span> Sublayer <span class="math inline">\((x)),\)</span> where Sublayer <span class="math inline">\((x)\)</span> is the function
implemented by the sub-layer itself. To facilitate these residual
connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension <span class="math inline">\(d_{\text {model }}=512\)</span>.</p>
</blockquote>
<p>encoder部分是由6个相同的堆网络层组成的，每一层有2个子网络：第一个子网络是多头注意力和自注意力机制，第二个子网络是一个位置全连接前馈神经网络（这个咋理解？）</p>
<p>在两个子网络周围用残差网络连接（也就是没两个子网络之间用到了残差网络）LayerNorm（这个需要查下：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对边长的输入sequence的normalize操作。）然后进行图1画的还是很直观的。</p>
<p>因此每个子层的输出就是正则化后的<span class="math inline">\((x+\)</span> Sublayer
<span class="math inline">\((x)),\)</span>sub-layers和embedding
layers的输出维度设置为512，这样是为了更好的进行残差连接（补下残差连接)</p>
<blockquote>
<p><strong>Decoder:</strong> The decoder is also composed of a stack of <span class="math inline">\(N=6\)</span>
identical layers. In addition to the two sub-layers in each encoder
layer, the decoder inserts a third sub-layer, which performs
multi-head attention over the output of the encoder stack. Similar to
the encoder, we employ residual connections around each of the
sub-layers, followed by layer normalization.
decoder部分也是由6个相同的块结构组成，除了每个编码器层中的两个子层之外，解码器还插入一个第三子层，该子层对编码器堆栈的输出执行多头关注，在每个sub-layers之间同样使用了残差神经网络。</p>
</blockquote>
<blockquote>
<p>We also modify the self-attention sub-layer in the decoder stack to
prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by
one position, ensures that the predictions for position <span class="math inline">\(i\)</span> can depend
only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</p>
</blockquote>
<p>修改了解码器堆栈中的自我注意子层，以防止位置关注后续位置。
这种掩盖，加上输出嵌入被一个位置偏移的事实，确保了对位置$ i
<span class="math inline">\(的预测只能依赖于位置小于\)</span> i
$的已知输出。(这里感觉用到了HMM的齐次一阶马尔可夫？）</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of
key-value pairs to an output,where the query, keys, values, and output
are all vectors. The output is computed as a weighted sum of the
values, where the weight assigned to each value is computed by a
compatibility function of the query with the corresponding key</p>
</blockquote>
<p>attention可以描述为将查询和一组键值对映射到输出，其中查询，键，值和输出都是向量。
<strong>将输出计算为值的加权总和，其中分配给每个值的权重是通过查询与相应键的兼容性函数来计算</strong></p>
<blockquote>
<p>We call our particular attention “Scaled Dot-Product Attention”
(Figure 2). The input consists of queries and keys of dimension
<span class="math inline">\(d_{k},\)</span> and values of dimension <span class="math inline">\(d_{v} .\)</span> We compute the dot products
of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_{k}}\)</span>, and apply a
softmax function to obtain the weights on the values.</p>
</blockquote>
<p>Scaled Dot-Product
Attention:输入是<span class="math inline">\(d_{k},\)</span>维的键值和<span class="math inline">\(d_{v} .\)</span>维的值，用所有的k计算查询的点积，将每个QK的除以$
 sqrt {d_ {k}} $，然后应用softmax函数来获得值的权重。</p>
<p><img src="https://img2020.cnblogs.com/blog/1365906/202008/1365906-20200814154338891-1651205929.png" /></p>
<blockquote>
<p>In practice, we compute the attention function on a set of queries
simultaneously, packed together into a matrix <span class="math inline">\(Q .\)</span> The keys and
values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We compute
the matrix of outputs as:</p>
</blockquote>
<p>查询,键值分别对应Q,K,V三个矩阵，因此attention的矩阵运算如下</p>
<p><span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\]</span></p>
<blockquote>
<p>Transformer会在三个地方使用multi-head attention： 1. encoder-decoder
attention：输入为encoder的输出和decoder的self-attention输出，其中encoder的self-attention作为
key and value，decoder的self-attention作为query。 2. encoder
self-attention：输入的Q、K、V都是encoder的input embedding and
positional embedding。 3. decoder
self-attention：在decoder的self-attention层中，deocder
都能够访问当前位置前面的位置，输入的Q、K、V都是decoder的input
embedding and positional embedding。 <strong>Note:</strong>
在一般的attention模型中，Q就是decoder的隐层，K就是encoder的隐层，V也是encoder的隐层。所谓的self-attention就是取Q，K，V相同，均为encoder或者decoder的input
embedding and positional
embedding，更具体为“网络输入是三个相同的向量q, k和v，是word
embedding和position
embedding相加得到的结果”。<a href="https://blog.csdn.net/Sakura55/article/details/86679826">csdn</a></p>
</blockquote>
<blockquote>
<p>The two most commonly used attention functions are additive attention
[2], and dot-product (multiplicative) attention. Dot-product attention
is identical to our algorithm, except for the scaling factor of
<span class="math inline">\(\frac{1}{\sqrt{d_{k}}} .\)</span> Additive attention computes the
compatibility function using a feed-forward network with a single
hidden layer. While the two are similar in theoretical complexity,
dot-product attention is much faster and more space-efficient in
practice, since it can be implemented using highly optimized matrix
multiplication code.</p>
</blockquote>
<p>计算attention的方式有2种，一种是点积的形式，另一种是求和的形式这里可以看下参考文献2，transformer中用的是点积的形式，此外还多了一个标准化的<span class="math inline">\(\frac{1}{\sqrt{d_{k}}} .\)</span>
求和形式的注意力使用具有单个隐藏层的前馈网络来计算兼容性函数.实际中点积形式的会更快更省内存</p>
<blockquote>
<p>While for small values of <span class="math inline">\(d_{k}\)</span> the two mechanisms perform
similarly, additive attention outperforms dot product attention
without scaling for larger values of <span class="math inline">\(d_{k}[3] .\)</span> We suspect that for
large values of <span class="math inline">\(d_{k},\)</span> the dot products grow large in magnitude,
pushing the softmax function into regions where it has extremely small
gradients <span class="math inline">\({ }^{4} .\)</span> To counteract this effect, we scale the dot
products by <span class="math inline">\(\frac{1}{\sqrt{d_{k}}}\)</span></p>
</blockquote>
<p>虽然对于$ d_ {k} <span class="math inline">\(较小的，这两种机制的执行方式相似，但是对于\)</span> d_ {k}
较大的，加法注意的性能优于点积注意，而无需缩放。<span class="math inline">\(我们怀疑对于\)</span> d_
{k的较大值， }，<span class="math inline">\(点积的幅度增大，将softmax函数推入梯度极小的区域\)</span> {} ^
{4}。<span class="math inline">\(为了抵消这种影响，我们用\)</span>  frac {1} {  sqrt {d_ {k}}} $</p>
<p>这里不知所云？</p>
<blockquote>
<p><strong>Multi-Head Attention</strong>:Instead of performing a single attention
function with <span class="math inline">\(d_{\text {model }}\)</span> -dimensional keys, values and
queries, we found it beneficial to linearly project the queries, keys
and values <span class="math inline">\(h\)</span> times with different, learned linear projections to
<span class="math inline">\(d_{k}, d_{k}\)</span> and <span class="math inline">\(d_{v}\)</span> dimensions, respectively. On each of these
projected versions of queries, keys and values we then perform the
attention function in parallel, yielding <span class="math inline">\(d_{v}\)</span> -dimensional output
values. These are concatenated and once again projected, resulting in
the final values, as depicted in Figure 2 .</p>
</blockquote>
<p>与使用<span class="math inline">\(d _ {\ text {model}}\)</span>维的键，值和查询执行单个注意功能相比，multi-head
attention则是通过h个不同的线性变换对Q，K，V进行投影，最后将不同的attention结果拼接起来再次训练，有点像cnn有咩有。。</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information
from different representation subspaces at different positions. With a
single attention head, averaging inhibits this.</p>
</blockquote>
<p>多头注意力使模型共同关注来自不同位置的不同表示子空间的信息。
对于一个注意力集中的头部，平均抑制了这一点。</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &amp;=\operatorname{Concat}\left(\operatorname{head}_{1}, \ldots, \operatorname{head}_{\mathrm{h}}\right) W^{O} \\
\text { where head }_{\mathrm{i}} &amp;=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
\]</span></p>
<blockquote>
<p>Where the projections are parameter matrices
<span class="math inline">\(W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{v}}\)</span>
and <span class="math inline">\(W^{O} \in \mathbb{R}^{h d_{v} \times d_{\text {model }}}\)</span></p>
</blockquote>
<blockquote>
<p>In this work we employ <span class="math inline">\(h=8\)</span> parallel attention layers, or heads. For
each of these we use <span class="math inline">\(d_{k}=d_{v}=d_{\text {model }} / h=64 .\)</span> Due to
the reduced dimension of each head, the total computational cost is
similar to that of single-head attention with full dimensionality.</p>
</blockquote>
<p>这里multi-head的头部个数8</p>
<blockquote>
<p><strong>Position-wise Feed-Forward Networks</strong>：In addition to attention
sub-layers, each of the layers in our encoder and decoder contains a
fully connected feed-forward network, which is applied to each
position separately and identically. This consists of two linear
transformations with a ReLU activation in between.</p>
</blockquote>
<p>Position-wise Feed-Forward Networks：位置全连接前馈神经网络</p>
<p><span class="math display">\[
\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
\]</span> &gt;While the linear transformations are the same across different
positions, they use different parameters from layer to layer. Another
way of describing this is as two convolutions with kernel size 1 The
dimensionality of input and output is <span class="math inline">\(d_{\text {model }}=512,\)</span> and the
inner-layer has dimensionality <span class="math inline">\(d_{f f}=2048\)</span></p>
<blockquote>
<p>位置全链接前馈网络一一MLP变形。之所以是position-wise (i/o维度一样)
是因为处理的attention输出是某一个
位置i的attention输出。hidden_size变化为：768-&gt;3072-&gt;768（或者512-&gt;2048-&gt;512）。</p>
</blockquote>
<blockquote>
<p>Position-wise feed forward network<strong>其实就是一个MLP
网络</strong>(多层感知机）, i的输出中, 每个<span class="math inline">\(d_model\)</span>维向量 x 在此先由
<span class="math inline">\(\mathrm{xW}_{-} 1+\mathrm{b}_{-} 1\)</span> 变为 <span class="math inline">\(\mathrm{d}_{1}\)</span> 维的
<span class="math inline">\(\mathrm{x}^{\prime},\)</span> 再经过max
<span class="math inline">\(\left(0, \mathrm{x}^{\prime}\right) \mathrm{W}_{2} +\mathrm{b}_{-2} 2\)</span>
回归 <span class="math inline">\(\mathrm{d}_{model}\)</span> 维。 Feed Forward Neural
Network全连接有两层dense,
第一层的激活函数是ReLU(或者其更平滑的版本Gaussian Error Linear
Unit-gelu), 第二层是一个线性激活函数, 如果multi-head输出表示为Z,
则FFN可以表示为： <span class="math display">\[
\mathrm{FFN}(Z)=\max \left(0, Z W_{1}+b_{1}\right) W_{2}+b_{2}
\]</span> 之后就是对hidden层进行dropout,
最后加一个resnet并normalization（tensor的最后一维, 即feature维进行）。
Transformer通过对输入的文本不断进行这样的注意力机制层和普通的非线性层交叠来得到最终的文本表达。<a href="https://blog.csdn.net/Sakura55/article/details/86679826">csdn</a></p>
</blockquote>
<p>那这样我就明白了，也就是input是经过attention层和普通的全连接层（使用的激活函数是relu）</p>
<blockquote>
<p><strong>Embeddings and Softmax:</strong>Similarly to other sequence transduction
models, we use learned embeddings to convert the input tokens and
output tokens to vectors of dimension <span class="math inline">\(d_{\text {model. }}\)</span>. We also
use the usual learned linear transformation and softmax function to
convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding
layers and the pre-softmax linear transformation, similar to [30]. In
the embedding layers, we multiply those weights by
<span class="math inline">\(\sqrt{d_{\text {model }}}\)</span>.</p>
</blockquote>
<p>有embedding层，decoder到output时用到了线性转移和softmax，在模型里面embedding层是共享参数的</p>
<blockquote>
<p>Positional Encoding:Since our model contains no recurrence and no
convolution, in order for the model to make use of the order of the
sequence, we must inject some information about the relative or
absolute position of the tokens in the sequence. To this end, we add
“positional encodings” to the input embeddings at the bottoms of the
encoder and decoder stacks. The positional encodings have the same
dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and fixed.</p>
</blockquote>
<p>Transformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional
Encoding的方法，将Positional
Encoding后的数据与输入embedding数据求和，加入了相对位置信息。</p>
<p>两种Positional Encoding方法：</p>
<ul>
<li>用不同频率的sine和cosine函数直接计算</li>
<li>学习出一份positional embedding。学习时注意，每个batch的pos
emb都一样，即在batch维度进行broadcast。
经过实验发现两者的结果一样，所以最后选择了第一种方法，公式如下：</li>
</ul>
<p><span class="math display">\[\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)
\end{aligned}\]</span></p>
<blockquote>
<p>任意位置的 $PE_{pos+k} $都可以被 $PE_{pos}
$的线性函数表示。考虑到在NLP任务中，除了单词的绝对位置，单词的相对位置也非常重要。根据公式
<span class="math inline">\(sin(\alpha+\beta) = sin \alpha cos \beta + cos \alpha sin\beta 以及cos(\alpha + \beta) = cos \alpha cos \beta - sin \alpha sin\beta，\)</span>这表明位置
$k+p $的位置向量可以表示为位置 k
的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li>如果是学习到的positional
embedding，可能会像词向量一样受限于词典大小。也就是只能学习到“位置2对应的向量是(1,1,1,2)”这样的表示。所以用三角公式明显不受序列长度的限制，也就是可以对
比所遇到序列的更长的序列 进行表示。</li>
</ol>
<blockquote>
<p>Transformer注意力机制有效的解释：Transformer所使用的注意力机制的核心思想是去计算一句话中的每个词对于这句话中所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，<strong>因此和单纯的词向量相比是一个更加全局的表达</strong>。使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量。
Attention之后还有一个线性的dense层，即multi-head
attention_output经过一个hidden_size为768的dense层，然后对hidden层进行dropout，最后加上resnet并进行normalization（tensor的最后一维，即feature维进行）。</p>
</blockquote>
<p>总结的很到位</p>
<p>OOV就是out-of-vocabulary，不在词库里的意思。</p>
<p>tranformer这里最近看了一篇还不错的分享，等空了把重点内容，之前理解没到位的记录下
这里先放一个链接</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247521353&amp;idx=6&amp;sn=5c1d369457feb0e85d2dc0ce15013551&amp;chksm=ebb7ac9ddcc0258b844f4c3e96e0a4e6092908eae6101582998d9023472fda4f853195d0483d&amp;mpshare=1&amp;scene=23&amp;srcid=0103cJP102hkdnRNQfXSAOuQ&amp;sharer_sharetime=1609723757487&amp;sharer_shareid=df15c342306c746b0423e3fd6ee52a86#rd">万能的transformer</a></p>
<p>上面这个文章中回答了一些在看attention is all you need 论文中的没有讲清楚的几个问题</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
transformer中Q,K,V的作用是什么？</li>
</ul>
<p>Transformer 中采用的是多头自注意力机制。在 Encoder 中，Q、K、V均为输入的序列编码，而多头的线性层可以方便地扩展模型参数。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>Q  =<span class="st"> </span>x <span class="op">*</span><span class="st"> </span>Wq</span>
<span id="cb2-2"><a href="#cb2-2"></a>K  =<span class="st"> </span>x <span class="op">*</span><span class="st"> </span>Wk</span>
<span id="cb2-3"><a href="#cb2-3"></a>V  =<span class="st"> </span>x <span class="op">*</span><span class="st"> </span>Wv</span></code></pre></div>
<p>此处应该有个图</p>
<p>x对应信息V的注意力权重 与 Q*K.tranpose 成正比
等于说：x的注意力权重，由x自己来决定，所以叫自注意力。
Wq,Wk,Wv会根据任务目标更新变化，保证了自注意力机制的效果。
<a href="https://www.zhihu.com/question/427629601/answer/1545963545">赵明明</a></p>
<p>换一个通俗易懂的解释</p>
<blockquote>
<p>你有一个问题Q，然后去搜索引擎里面搜，搜索引擎里面有好多文章，每个文章V有一个能代表其正文内容的标题K，然后搜索引擎用你的问题Q和那些文章V的标题K进行一个匹配，看看相关度（QK —&gt;attention值），然后你想用这些检索到的不同相关度的文章V来表示你的问题，就用这些相关度将检索的文章V做一个加权和，那么你就得到了一个新的Q’，这个Q’融合了相关性强的文章V更多信息，而融合了相关性弱的文章V较少的信息。这就是注意力机制，注意力度不同，重点关注（权值大）与你想要的东西相关性强的部分，稍微关注（权值小）相关性弱的部分。<a href="https://www.zhihu.com/question/427629601/answer/1558216827">Leetuter</a></p>
</blockquote>
<ul>
<li>多头注意力机制为什么能解决长依赖的问题？</li>
</ul>
<blockquote>
<p>同一序列中词的关系直接由 score 值计算得到，不会因为时序、传播的限制而造成信息损失，有利于模拟长程依赖，但也有工作通过实验表明[7]，多头注意力机制解决长程依赖是由于参数量的增加，若在同规模下对比，相对CNN，RNN并未有明显提升。</p>
</blockquote>
<ul>
<li>下游任务是什么?</li>
</ul>
<blockquote>
<p>预训练技术是指预先设计多层网络结构，将编码后的数据输入到网络结构中进行训练，增加模型的泛化能力。预先设计的网络结构通常被称为预训练模型，它的作用是初始化下游任务。将预训练好的模型参数应用到后续的其他特定任务上，这些特定任务通常被称为“下游任务”</p>
</blockquote>
<p>现在非常多的任务都是预训练+微调这种方式进行的，相当于冻结预训练部分的参数，将预训练模型应用到小的数据集上，只需要微调下游任务的参数即可。word2vec，glooe，elmo，bert等都是这种形式。</p>
<p>同样的这些也都是无监督的模型。fastext作为预训练模型的时候也是无监督的，用作分类时时有监督的</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
补充bibtex 养成好的习惯蛤！！</li>
</ul>
</div>
<div id="gpt" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> GPT</h2>
<p>单项语言模型,准确的来说是单项的transformer，只用到了transformer中的decodder</p>
<div id="introduction-3" class="section level3" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> introduction</h3>
<blockquote>
<p>We employ a two-stage training procedure. First, we use a language modeling objective onthe unlabeled data to learn the initial parameters of a neural network model. Subsequently, we adapt these parameters to a target task using the corresponding supervised objective.</p>
</blockquote>
<p>GPT是一个包含两阶段的训练的过程</p>
<ul>
<li><p>在无标签的海量数据中训练语言模型，学习神经网络模型的参数。</p></li>
<li><p>阶段一训练完成模型参数用相关标签数据训练target task。</p></li>
</ul>
</div>
<div id="framework" class="section level3" number="1.7.2">
<h3><span class="header-section-number">1.7.2</span> Framework</h3>
<p>这里主要是看下GPT的结构</p>
</div>
<div id="unsupervised-pre-training" class="section level3" number="1.7.3">
<h3><span class="header-section-number">1.7.3</span> Unsupervised pre-training</h3>
<p>无监督的预训练的过程</p>
<p>首先是无监督的预训练部分</p>
<p>Given an unsupervised corpus of tokens <span class="math inline">\(\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\},\)</span> we use a standard language modeling objective to maximize the following likelihood:</p>
<p>语言模型的最大似然函数</p>
<p><span class="math display">\[
L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)
\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the size of the context window, and the conditional probability <span class="math inline">\(P\)</span> is modeled using a neural network with parameters <span class="math inline">\(\Theta\)</span>. These parameters are trained using stochastic gradient descent [51] .</p>
<p>In our experiments, we use a multi-layer Transformer decoder [34] for the language model, which is a variant of the transformer <span class="math inline">\([62] .\)</span> This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:</p>
<p>使用transformer中的decoder来建立模型</p>
<p>模型的输入上下文使用的是多头注意力机制，然后对位置前馈全连接层进行应用，以在目标token上产生输出分布：</p>
<p><span class="math display">\[
\begin{aligned}
h_{0} &amp;=U W_{e}+W_{p} \\
h_{l} &amp;=\text { transformer_block }\left(h_{l-1}\right) \forall i \in[1, n] \\
P(u) &amp;=\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(U=\left(u_{-k}, \ldots, u_{-1}\right)\)</span> is the context vector of tokens, <span class="math inline">\(n\)</span> is the number of layers, <span class="math inline">\(W_{e}\)</span> is the token embedding matrix, and <span class="math inline">\(W_{p}\)</span> is the position embedding matrix.</p>
<p><span class="math inline">\(U=\left(u_{-k}, \ldots, u_{-1}\right)\)</span>是上下文向量的token，<span class="math inline">\(n\)</span>是层数，<span class="math inline">\(W_{e}\)</span>是词向量的嵌入矩阵，<span class="math inline">\(W_{p}\)</span>是位置embedding的矩阵，其他部分都是transformer中的decoder的结构。</p>
</div>
<div id="supervised-ﬁne-tuning" class="section level3" number="1.7.4">
<h3><span class="header-section-number">1.7.4</span> Supervised ﬁne-tuning</h3>
<p>有监督的微调的阶段</p>
<blockquote>
<p>After training the model with the objective in Eq. 1 , we adapt the parameters to the supervised target task. We assume a labeled dataset <span class="math inline">\(\mathcal{C},\)</span> where each instance consists of a sequence of input tokens, <span class="math inline">\(x^{1}, \ldots, x^{m},\)</span> along with a label <span class="math inline">\(y .\)</span> The inputs are passed through our pre-trained model to obtain the final transformer block’s activation <span class="math inline">\(h_{l}^{m},\)</span> which is then fed into an added linear output layer with parameters <span class="math inline">\(W_{y}\)</span> to predict <span class="math inline">\(y:\)</span></p>
</blockquote>
<p>在公式1的目标函数训练的模型,将训练获得的参数应用于有监督的目标任务。假定有带标签的数据集C，包含的每个实例是词序列, 如 <span class="math inline">\(x^{1}, \ldots, x^{m},\)</span> 带有标签y, 首先作为输入通过已经预训练好的pre-trained model获得最终transformer block’s activation <span class="math inline">\(h_{l}^{m}\)</span>,然后输入带有 Wy的线性输出层来预测y。</p>
<p>简单的来说就是将第一阶段无监督预训练的词向量的作为embedding层。</p>
<p>在最后一个transformer block后跟一层全连接和softmax构成task classifier，预测每个类别的概率。</p>
<p><span class="math display">\[
P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)
\]</span></p>
<p>This gives us the following objective to maximize:</p>
<p>在输入所有的token后，预测true类别的概率最大，目标函数为</p>
<p><span class="math display">\[
L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)
\]</span></p>
<p>We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning by (a) improving generalization of the supervised model, and (b) accelerating convergence. This is in line with prior work [50,43] , who also observed improved performance with such an auxiliary objective. Specifically, we optimize the following objective (with weight <span class="math inline">\(\lambda\)</span> ):</p>
<p>为了更好的fine-tuning分类器，更快的收敛，修改目标函数为task classifier和text prediction相结合：</p>
<p><span class="math display">\[
L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda * L_{1}(\mathcal{C})
\]</span>
Overall, the only extra parameters we require during fine-tuning are <span class="math inline">\(W_{y}\)</span>, and embeddings for delimiter tokens (described below in Section 3.3</p>
<p><img src="figs/gpt_1.png" /></p>
<blockquote>
<p>For some tasks, like text classification, we can directly fine-tune our model as described above. Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks. Previous work proposed learning task specific architectures on top of transferred representations [44] . Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components. Instead, we use a traversal-style approach <span class="math inline">\([52],\)</span> where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks. We provide a brief description of these input transformations below and Figure <span class="math inline">\(\square\)</span> provides a visual illustration. All transformations include adding randomly initialized start and end tokens <span class="math inline">\((\langle s\rangle,\langle e\rangle)\)</span>.</p>
</blockquote>
<p>figure1是是GPT针对不同的任务选择的不同的transformer结构。</p>
<ul>
<li><p>文本分类是1层transformer+liner层</p></li>
<li><p>文本相似度分析是2层transformer+liner层</p></li>
</ul>
<p>不同的任务处理方式不同。</p>
<p>通过提升参数量来提升任务的效果，不知道这种代价在实际应用中是否划算。</p>
<blockquote>
<p>Textual entailment For entailment tasks, we concatenate the premise <span class="math inline">\(p\)</span> and hypothesis <span class="math inline">\(h\)</span> token sequences, with a delimiter token ($) in between.</p>
</blockquote>
<p>entailment：因输入有前提和假说两个句子，那就在两个句子中加入分隔符（delim）连接两条句子，作为输入，经过语言模型送入分类器。</p>
<blockquote>
<p>Similarity For similarity tasks, there is no inherent ordering of the two sentences being compared. To reflect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations <span class="math inline">\(h_{l}^{m}\)</span> which are added element-wise before being fed into the linear output layer.</p>
</blockquote>
<p>similarity：因两条句子的顺序不影响结果，就按两种顺序分别放入语言模型得到各自的hidden state，将两种hidden state相加，送入分类器。</p>
<blockquote>
<p>Question Answering and Commonsense Reasoning For these tasks, we are given a context document <span class="math inline">\(z,\)</span> a question <span class="math inline">\(q,\)</span> and a set of possible answers <span class="math inline">\(\left\{a_{k}\right\} .\)</span> We concatenate the document context and question with each possible answer, adding a delimiter token in between to get <span class="math inline">\(\left[z ; q ; \$ ; a_{k}\right]\)</span>. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.</p>
</blockquote>
<p>multiple choice：对于每个答案，都将context、问题、该答案以分隔符隔开连接起来，作为输入，经过语言模型送入分类器得到一个向量，将所有答案的向量送入softmax。</p>
</div>
<div id="实验部分-2" class="section level3" number="1.7.5">
<h3><span class="header-section-number">1.7.5</span> 实验部分</h3>
<blockquote>
<p>Unsupervised pre-training We use the BooksCorpus dataset [71] for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. An alternative dataset, the <span class="math inline">\(1 \mathrm{~B}\)</span> Word Benchmark, which is used by a similar approach, ELMo [44] , is approximately the same size</p>
</blockquote>
<p>预料库是BooksCorpus数据集，elmo也是这个</p>
<blockquote>
<p>but is shufﬂed at a sentence level - destroying long-range structure. Our language model achieves a very low token level perplexity of 18.4 on this corpus.</p>
</blockquote>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
token级别的困惑度是什么?</li>
</ul>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
