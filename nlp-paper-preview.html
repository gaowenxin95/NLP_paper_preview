<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>NLP相关paper阅读</title>
  <meta name="description" content="NLP相关paper阅读" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="NLP相关paper阅读" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="NLP相关paper阅读" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2020-12-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#nlp-paper-preview"><i class="fa fa-check"></i><b>1</b> NLP paper preview</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#fasttext"><i class="fa fa-check"></i><b>1.1</b> fasttext</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#hierarchical-softmax"><i class="fa fa-check"></i><b>1.1.1</b> Hierarchical softmax</a></li>
<li class="chapter" data-level="1.1.2" data-path=""><a href="#n_gram-features"><i class="fa fa-check"></i><b>1.1.2</b> N_gram features</a></li>
<li class="chapter" data-level="1.1.3" data-path=""><a href="#fasttext与word2vec对比"><i class="fa fa-check"></i><b>1.1.3</b> fasttext与word2vec对比</a></li>
<li class="chapter" data-level="1.1.4" data-path=""><a href="#fasttext与cbow有两点不同"><i class="fa fa-check"></i><b>1.1.4</b> fasttext与CBOW有两点不同</a></li>
<li class="chapter" data-level="1.1.5" data-path=""><a href="#实验和结果分析"><i class="fa fa-check"></i><b>1.1.5</b> 实验和结果分析</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#doc2vec"><i class="fa fa-check"></i><b>1.2</b> Doc2vec</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#section"><i class="fa fa-check"></i><b>1.3</b> </a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">NLP相关paper阅读</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">NLP相关paper阅读</h1>
<p class="author"><em>高文欣</em></p>
<p class="date"><em>2020-12-23</em></p>
</div>
<div id="nlp-paper-preview" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NLP paper preview</h1>
<p>emmmmm…站在巨人的肩膀看世界~ 是件幸福的事情</p>
<div id="fasttext" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> fasttext</h2>
<p>结构中比较重要的几个点</p>
<div id="hierarchical-softmax" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Hierarchical softmax</h3>
<p>这个没什么好说的，和word2vec里面的是一样的。具体解释忘记之前从哪里找来得了但私以为解释的8错</p>
<blockquote>
<p>When the number of classes is large, computing the linear classifier
is computationally expensive. More precisely, the computational
complexity is <span class="math inline">\(O(k h)\)</span> where <span class="math inline">\(k\)</span> is the number of classes and <span class="math inline">\(h\)</span> the
dimension of the text representation. In order to improve our running
time, we use a hierarchical softmax (Goodman, 2001) based on the
Huffman coding tree (Mikolov et al., 2013 ). During training, the
computational complexity drops to <span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span>. The
hierarchical softmax is also advantageous at test time when searching
for the most likely class. Each node is associated with a probability
that is the probability of the path from the root to that node. If the
node is at depth <span class="math inline">\(l+1\)</span> with parents <span class="math inline">\(n_{1}, \ldots, n_{l},\)</span> its
probability is <span class="math display">\[
P\left(n_{l+1}\right)=\prod_{i=1}^{l} P\left(n_{i}\right)
\]</span></p>
</blockquote>
<blockquote>
<p>This means that the probability of a node is always lower than the one
of its parent. Exploring the tree with a depth first search and
tracking the maximum probability among the leaves allows us to discard
any branch associated with a small probability. In practice, we
observe a reduction of the complexity to
<span class="math inline">\(O\left(h \log _{2}(k)\right)\)</span> at test time. This approach is further
extended to compute the <span class="math inline">\(T\)</span> -top targets at the cost of <span class="math inline">\(O(\log (T)),\)</span>
using a binary heap.</p>
</blockquote>
<p>主要就是利用霍夫曼树加快计算的速度</p>
<blockquote>
<p>Hierachical
Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树,
保证词频较大的词处于相对比较浅的层,
词频较低的词相应的处于Huffman树较深层的叶子节点,
每一个词都处于这棵Huffman树上的某个叶子节点;
第二，将原本的一个|V|分类问题变成了<span class="math inline">\(\log|V|\)</span> 次的二分类问题,
做法简单说来就是, 原先要计算 <span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span>的时候,
因为使用的 是普通的softmax,
势必要求词典中的每一个词的概率大小，为了减少这一步的计算量, 在
Hierachical Softmax中，同样是计算当前词 <span class="math inline">\(w_{t}\)</span>
在其上下文中的概率大小，只需要把它变成在
Huffman树中的路径预测问题就可以了，因为当前词 <span class="math inline">\(w_{t}\)</span>
在Huffman树中对应到一条路径, 这条 路径由这棵二叉树中从根节点开始,
经过一系列中间的父节点, 最终到达当前这个词的叶子节点而 组成,
那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而
Huffman树的构造过程保证了树的深度为 <span class="math inline">\(\log |V|,\)</span>
所以也就只需要做<span class="math inline">\(\log |V|\)</span>次二分类便可以 求得
<span class="math inline">\(P\left(w_{t} \mid c_{t}\right)\)</span> 的大小, 这相比原来|V|次的计算量,
已经大大减小了。</p>
</blockquote>
</div>
<div id="n_gram-features" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> N_gram features</h3>
<p><strong>这个是主要区别于word2vec的输入的部分了，为了更好的学习到上下文的语序特征</strong></p>
<p>从bag of word 变成了bag of features</p>
<blockquote>
<p>Bag of words is invariant to word order but taking explicitly this
order into account is often computationally very expensive. Instead,
we use a bag of n-grams as additional features to capture some partial
information about the local word order. This is very efficient in
practice while achieving comparable results to methods that explicitly
use the order (Wang and Manning, 2012 ). We maintain a fast and memory
efficient mapping of the n-grams by using the hashing trick
(Weinberger et al., 2009 ) with the same hashing function as in
Mikolov et al. (2011) and <span class="math inline">\(10 \mathrm{M}\)</span> bins if we only used
bigrams, and <span class="math inline">\(100 \mathrm{M}\)</span> otherwise.</p>
</blockquote>
</div>
<div id="fasttext与word2vec对比" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> fasttext与word2vec对比</h3>
<p>感觉读paper的时候并没有很仔细的找出fasttext和word2vec的区别</p>
<blockquote>
<p>word2vec和GloVe都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText则是利用带有监督标记的文本分类数据完成训练，本质上没有什么特殊的，模型框架就是CBOW。</p>
</blockquote>
<p>因为是训练词向量的嘛，因此只需要文本就可以了，不需要标签。</p>
<p>fasttext和word2vec本质无区别都是单层的神经网络，CBOW的结构，通过上下文预测当前词。
word2vec是为了得到embedding的矩阵，word2vec本质是一个词袋模型:bag of
word。</p>
</div>
<div id="fasttext与cbow有两点不同" class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> fasttext与CBOW有两点不同</h3>
<blockquote>
<p>分别是输入数据和预测目标的不同 -
在输入数据上，CBOW输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而fastText为了利用更多的语序信息，将bag-of-words变成了bag-of-features，也就是下图中的输入x不再仅仅是一个词，还可以加上bigram或者是trigram的信息等等；</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>from gensim.models import FastText</span>
<span id="cb1-2"><a href="#cb1-2"></a>sentences =<span class="st"> </span>[[<span class="st">&quot;你&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;谁&quot;</span>], [<span class="st">&quot;我&quot;</span>, <span class="st">&quot;是&quot;</span>, <span class="st">&quot;中国人&quot;</span>]]</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>model =<span class="st"> </span><span class="kw">FastText</span>(sentences,  <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">window=</span><span class="dv">3</span>, <span class="dt">min_count=</span><span class="dv">1</span>, <span class="dt">iter=</span><span class="dv">10</span>,<span class="dt">min_n =</span> <span class="dv">3</span> , <span class="dt">max_n =</span> <span class="dv">6</span>,<span class="dt">word_ngrams =</span> <span class="dv">0</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>model[<span class="st">&#39;你&#39;</span>]  <span class="co"># 词向量获得的方式</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>model.wv[<span class="st">&#39;你&#39;</span>] <span class="co"># 词向量获得的方式</span></span></code></pre></div>
<p>所以在训练fasttext的词向量时候，参数word_ngrams =
0时候，是等价于word2vec的。</p>
<blockquote>
<p>第二个不同在于，CBOW预测目标是语境中的一个词，而fastText预测目标是当前这段输入文本的类别，正因为需要这个文本类别，因此才说fastText是一个监督模型。而相同点在于，fastText的网络结构和CBOW基本一致，同时在输出层的分类上也使用了Hierachical
Softmax技巧来加速训练。</p>
</blockquote>
<p>两者本质的不同，体现在 h-softmax的使用：</p>
<p>Wordvec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的
h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。
fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</p>
</div>
<div id="实验和结果分析" class="section level3" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> 实验和结果分析</h3>
<p><em>情感分析实验</em></p>
<p>在8个数据集上面acc对比，对比了6个模型，可以看出在绝大部分的数据集上面fasttext的acc是最好的。
加入bgram的效果要优于不加的</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-87AA0A90.png" /></p>
<p>运行时间对比fasttext的速度绝了~</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-099A1FC8.png" /></p>
<p>对比不同模型的acc，fasttext略高~</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-283D5547.png" /></p>
<p>在标签预测上的测试时间，fasttext非常快~</p>
<p><img src="C:/Users/gaowenxin95/AppData/Local/RStudio/tmp/paste-77B0769C.png" /></p>
</div>
</div>
<div id="doc2vec" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Doc2vec</h2>
</div>
<div id="section" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> </h2>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
