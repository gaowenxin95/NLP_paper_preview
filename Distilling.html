<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>模型蒸馏笔记</title>
  <meta name="description" content="模型蒸馏笔记" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="模型蒸馏笔记" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="模型蒸馏笔记" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2021-03-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="0.1" data-path=""><a href="#distilling-the-knowledge-in-a-neural-network"><i class="fa fa-check"></i><b>0.1</b> Distilling the Knowledge in a Neural Network</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path=""><a href="#文章知识点总览"><i class="fa fa-check"></i><b>0.1.1</b> 文章知识点总览</a></li>
<li class="chapter" data-level="0.1.2" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>0.1.2</b> introduction</a></li>
<li class="chapter" data-level="0.1.3" data-path=""><a href="#distillation"><i class="fa fa-check"></i><b>0.1.3</b> Distillation</a></li>
<li class="chapter" data-level="0.1.4" data-path=""><a href="#matching-logits-is-a-special-case-of-distillation"><i class="fa fa-check"></i><b>0.1.4</b> Matching logits is a special case of distillation</a></li>
<li class="chapter" data-level="0.1.5" data-path=""><a href="#experiment"><i class="fa fa-check"></i><b>0.1.5</b> Experiment</a></li>
<li class="chapter" data-level="0.1.6" data-path=""><a href="#结论"><i class="fa fa-check"></i><b>0.1.6</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path=""><a href="#distilling-task-speciﬁc-knowledge-from-bert-into-simple-neural-networks"><i class="fa fa-check"></i><b>0.2</b> Distilling Task-Speciﬁc Knowledge from BERT into Simple Neural Networks</a>
<ul>
<li class="chapter" data-level="0.2.1" data-path=""><a href="#文章知识点总览-1"><i class="fa fa-check"></i><b>0.2.1</b> 文章知识点总览</a></li>
<li class="chapter" data-level="0.2.2" data-path=""><a href="#abstract"><i class="fa fa-check"></i><b>0.2.2</b> abstract</a></li>
<li class="chapter" data-level="0.2.3" data-path=""><a href="#introduction-1"><i class="fa fa-check"></i><b>0.2.3</b> Introduction</a></li>
<li class="chapter" data-level="0.2.4" data-path=""><a href="#approach"><i class="fa fa-check"></i><b>0.2.4</b> Approach</a></li>
<li class="chapter" data-level="0.2.5" data-path=""><a href="#model-architecture"><i class="fa fa-check"></i><b>0.2.5</b> Model Architecture</a></li>
<li class="chapter" data-level="0.2.6" data-path=""><a href="#distillation-objective"><i class="fa fa-check"></i><b>0.2.6</b> Distillation Objective</a></li>
<li class="chapter" data-level="0.2.7" data-path=""><a href="#data-augmentation-for-distillation"><i class="fa fa-check"></i><b>0.2.7</b> Data Augmentation for Distillation</a></li>
<li class="chapter" data-level="0.2.8" data-path=""><a href="#对照实验部分"><i class="fa fa-check"></i><b>0.2.8</b> 对照实验部分</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path=""><a href="#patient-knowledge-distillation-for-bert-model-compression"><i class="fa fa-check"></i><b>0.3</b> Patient Knowledge Distillation for BERT Model Compression</a>
<ul>
<li class="chapter" data-level="0.3.1" data-path=""><a href="#文章知识点总览-2"><i class="fa fa-check"></i><b>0.3.1</b> 文章知识点总览</a></li>
<li class="chapter" data-level="0.3.2" data-path=""><a href="#abstract-1"><i class="fa fa-check"></i><b>0.3.2</b> abstract</a></li>
<li class="chapter" data-level="0.3.3" data-path=""><a href="#introduction-2"><i class="fa fa-check"></i><b>0.3.3</b> Introduction</a></li>
<li class="chapter" data-level="0.3.4" data-path=""><a href="#patient-knowledge-distillation"><i class="fa fa-check"></i><b>0.3.4</b> Patient Knowledge Distillation</a></li>
<li class="chapter" data-level="0.3.5" data-path=""><a href="#实验部分"><i class="fa fa-check"></i><b>0.3.5</b> 实验部分</a></li>
</ul></li>
<li class="chapter" data-level="0.4" data-path=""><a href="#distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter"><i class="fa fa-check"></i><b>0.4</b> DistilBERT a distilled version of BERT smaller faster cheaper and lighter</a>
<ul>
<li class="chapter" data-level="0.4.1" data-path=""><a href="#文章知识点总览-3"><i class="fa fa-check"></i><b>0.4.1</b> 文章知识点总览</a></li>
<li class="chapter" data-level="0.4.2" data-path=""><a href="#abstract-2"><i class="fa fa-check"></i><b>0.4.2</b> abstract</a></li>
<li class="chapter" data-level="0.4.3" data-path=""><a href="#introdiction"><i class="fa fa-check"></i><b>0.4.3</b> introdiction</a></li>
<li class="chapter" data-level="0.4.4" data-path=""><a href="#distilbert-a-distilled-version-of-bert"><i class="fa fa-check"></i><b>0.4.4</b> DistilBERT a distilled version of BERT</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#参考文献"><i class="fa fa-check"></i>参考文献</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">模型蒸馏笔记</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">模型蒸馏笔记</h1>
<p class="author"><em>高文欣</em></p>
<p class="date"><em>2021-03-25</em></p>
</div>
<p>KD一个比较<a href="Wanghttps://www.zhihu.com/question/50519680/answer/136363665">靠谱的解释是Naiyan的这篇</a></p>
<p>这里我直接粘过来</p>
<blockquote>
<p>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布（soft softmax prob)，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</p>
</blockquote>
<div id="distilling-the-knowledge-in-a-neural-network" class="section level2" number="0.1">
<h2><span class="header-section-number">0.1</span> Distilling the Knowledge in a Neural Network</h2>
<p>原文：<a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a></p>
<div id="文章知识点总览" class="section level3" number="0.1.1">
<h3><span class="header-section-number">0.1.1</span> 文章知识点总览</h3>
<p>主要内容</p>
<p>介绍模型蒸馏的概念
模型蒸馏一共有两种方式</p>
<ul>
<li><p>蒸馏集成模型</p></li>
<li><p>蒸馏复杂的大的网络模型（一般指参数过亿的）</p></li>
</ul>
<p>新概念</p>
<ul>
<li><p>提出了soft softmax prob</p></li>
<li><p>为了平滑soft softmax prob 的结果指定一个高温数T，没什么特别的含义，一个超参数</p></li>
</ul>
</div>
<div id="introduction" class="section level3" number="0.1.2">
<h3><span class="header-section-number">0.1.2</span> introduction</h3>
<blockquote>
<p>It is generally accepted that the objective function used for training should reflect the true objective of the user as closely as possible. Despite this, models are usually trained to optimize performance on the training data when the real objective is to generalize well to new data. It would clearly be better to train models to generalize well, but this requires information about the correct way to generalize and this information is not normally available. When we are distilling the knowledge from a large model into a small one, however, we can train the small model to generalize in the same way as the large model. If the cumbersome model generalizes well because, for example, it is the average of a large ensemble of different models, a small model trained to generalize in the same way will typically do much better on test data than a small model that is trained in the normal way on the same training set as was used to train the ensemble.<span class="citation">(Hinton, Vinyals, and Dean 2015)</span></p>
</blockquote>
<p>一般认为，用于训练的目标函数应该尽可能地反映用户的真实目标。训练模型通常是为了提高模型的泛化能力，能应用到更多的新的数据上面。显然，训练模型来很好地泛化会更好，但这需要关于正确泛化方式的信息，而这些信息通常是不可用的。然而，当将知识从一个大模型提炼成一个小模型时，可以训练小模型以与大模型相同的方式进行归纳。这句话简单来说就是不让小模型去学习数据的分布，而是让小模型去学习大模型的性能。</p>
<p>例如,对于一个复杂的大模型来说,用同样的方法训练一般化的小模型在测试数据上的表现通常要比在用于训练集成的相同训练集上以正常方式训练的小模型好得多。</p>
<ul>
<li>大模型一般直参数量级非常大：bert，xlnet,T5等这种。</li>
</ul>
<blockquote>
<p>An obvious way to transfer the generalization ability of the cumbersome model to a small model is to use the class probabilities produced by the cumbersome model as “soft targets” for training the small model. For this transfer stage, we could use the same training set or a separate “transfer” set. When the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or geometric mean of their individual predictive distributions as the soft targets. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model and using a much higher learning rate.</p>
</blockquote>
<p>将大模型的泛化能力迁移到小模型的一种明显的方法是用大网络模型产生的类概率作为小模型的“soft targets”进行训练。对于这个转移阶段，可以使用相同的训练集或单独的“transfer” set
。当大的网络模型是简单模型的大集合时(也就是集成模型)，可以使用它们各自预测分布的算术或几何均值作为soft targets。当soft targets有很高的熵,它们为每个训练案例提供了比硬目标更多的信息，并且训练案例之间梯度的方差也更小，因此小模型通常可以在比原始复杂模型少得多的数据上进行训练，并且使用更高的学习率。</p>
<p>梯度方差更小，说明蒸馏得到的小模型是比较平稳的更加容易收敛的。</p>
<blockquote>
<p>For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high confidence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of <span class="math inline">\(10^{-6}\)</span> of being a 3 and <span class="math inline">\(10^{-9}\)</span> of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2 ’s look like 3 ’s and which look like 7 ’s) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero. Caruana and his collaborators circumvent this problem by using the logits (the inputs to the final softmax) rather than the probabilities produced by the softmax as the targets for learning the small model and they minimize the squared difference between the logits produced by the cumbersome model and the logits produced by the small model. Our more general solution, called “distillation”, is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. We then use the same high temperature when training the small model to match these soft targets. We show later that matching the logits of the cumbersome model is actually a special case of distillation.</p>
</blockquote>
<p>对于像MNIST这样的任务，复杂的大模型几乎总是以很高的置信度得出正确分类，关于学习函数的大部分信息都存在于软目标中非常小的概率比率中。例如，在MNIST数据中，对于某个2的输入，对于2 的预测概率会很高, 而对于2类似的数字，例如3和7的预测概率为 <span class="math inline">\(10^{-6}\)</span> 和 <span class="math inline">\(10^{-9}\)</span> 。这样的话, teacher网络学到数据的相似信息（例如数字2和3，7很类似) 很难传达给student网络。由于它们 的概率值接近0。因此，Caruana等人通过使用logits作为目标解决了这个问题，而不是softmax产生的概率作为学习小模型的目标，它们最小化了复杂大模型产生的对数和小模型产生的对数之间的差的平方。于是有了soft target.后文有详细的介绍。</p>
<p>hard target：以二分类为例：样本预测为0/1，只属于0/1其中一个，也就是样本原有的标签。
soft targets:样本预测为0/1的概率，是0-1范围内的一个概率数值。</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
soft targets 和label smoothing什么关系？</li>
</ul>
<p>个人感觉label smoothing是soft targets的特殊情况，权重分别取0和1时候是等价的</p>
<p>因此可以给出蒸馏的定义：</p>
<p>也是迁移学习的范畴，但是这里的迁移不是把训练好的预训练模型去做下游任务的fine-tune,而是利用小模型去学习大模型的性能。把大模型的学习性能蒸馏给了小模型。利用小模型学习大模型得到的soft-target的分布。</p>
<p>参考李rumor的一个总结就是：</p>
<blockquote>
<p>蒸馏这个概念之所以work，核心思想是因为好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据。所以蒸馏的目标是让学生模型学习到教师模型的泛化能力，理论上得到的结果会比单纯拟合训练数据的学生模型要好。<a href="https://mp.weixin.qq.com/s/tKfHq49heakvjM0EVQPgHw">李rumor</a></p>
</blockquote>
</div>
<div id="distillation" class="section level3" number="0.1.3">
<h3><span class="header-section-number">0.1.3</span> Distillation</h3>
<p>蒸馏具体的做法</p>
<p>Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, <span class="math inline">\(z_{i}\)</span>, computed for each class into a probability, <span class="math inline">\(q_{i}\)</span>, by comparing <span class="math inline">\(z_{i}\)</span> with the other logits.</p>
<p>下面是计算soft softmax概率值的公式。于基本的softmax不同的地方在于引入了一个参数T</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
softmax做二分类和sigmoid做二分类谁的效果更好一些？并给出原因</li>
</ul>
<p><span class="math display">\[
q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
\]</span></p>
<blockquote>
<p>where <span class="math inline">\(T\)</span> is a temperature that is normally set to <span class="math inline">\(1 .\)</span> Using a higher value for <span class="math inline">\(T\)</span> produces a softer probability distribution over classes.</p>
</blockquote>
<p>T是一个温度值，通常设置为1，当T取值较高的时候会得到一个“soft”概率分布的值。</p>
<p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。
可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失：</p>
<blockquote>
<p>In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of <span class="math inline">\(1 .\)</span></p>
</blockquote>
<p><strong>简单的蒸馏方式，从模型迁移的角度去理解</strong></p>
<p>在蒸馏的最简单形式中，知识转移到蒸馏模型，方法是在一个转移集上训练它，并对转移集中的每个情况使用软目标分布，这是使用softmax中带有较大T的大模型产生的。训练蒸馏模型时使用相同的T，但训练后使用T=1。简单来说就是蒸馏模型和原模型使用相同的T。</p>
<p>简单的来说其中的一种蒸馏方式是根据“迁移模型”的方式，大模型和蒸馏模型使用的是相同的T，将</p>
<ul>
<li>T一个小模型学习大模型的超参数，温度函数</li>
</ul>
<blockquote>
<p>When the correct labels are known for all or some of the transfer set, this method can be significantly improved by also training the distilled model to produce the correct labels. One way to do this is to use the correct labels to modify the soft targets, but we found that a better way is to simply use a weighted average of two different objective functions. The first objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model. The second objective function is the cross entropy with the correct labels. This is computed using exactly the same logits in softmax of the distilled model but at a temperature of <span class="math inline">\(1 .\)</span> We found that the best results were generally obtained by using a condiderably lower weight on the second objective function. Since the magnitudes of the gradients produced by the soft targets scale as <span class="math inline">\(1 / T^{2}\)</span> it is important to multiply them by <span class="math inline">\(T^{2}\)</span> when using both hard and soft targets. This ensures that the relative contributions of the hard and soft targets remain roughly unchanged if the temperature used for distillation is changed while experimenting with meta-parameters.</p>
</blockquote>
<p>第二种目标函数：
当所有或部分传输集都知道正确的标签时，通过训练蒸馏模型产生正确的标签，可以显著改进这种方法。一种方法是使用正确的标签来修改软目标，一个更好的方法是简单地使用两个不同目标函数的加权平均值。</p>
<ul>
<li><p>第1种目标函数是与软目标的交叉熵，该交叉熵的计算使用的是蒸馏模型的softmax中的high temperature，与从繁琐的模型生成软目标时使用的T相同。</p></li>
<li><p>第2种目标函数于正确标签的交叉熵,此时T取值为1。</p></li>
</ul>
<p><span class="math inline">\(L=(1-\alpha) C E(y, p)+\alpha C E(q, p) \cdot T^{2}\)</span></p>
<p>最终效果较好的实验是给第二个目标函数赋予一个较低的权重。因为梯度的范围是通过软目标产生的，会变为原来的<span class="math inline">\(1 / T^{2}\)</span>，当同时使用hard and soft targets时候,软目标函数需要乘上<span class="math inline">\(T^{2}\)</span>。这就保证了在使用meta-parameters进行实验时，如果用于蒸馏的温度发生变化,硬目标和软目标的相对贡献大致保持不变。</p>
</div>
<div id="matching-logits-is-a-special-case-of-distillation" class="section level3" number="0.1.4">
<h3><span class="header-section-number">0.1.4</span> Matching logits is a special case of distillation</h3>
<p>Each case in the transfer set contributes a cross-entropy gradient, <span class="math inline">\(d C / d z_{i}\)</span> with respect to each logit, <span class="math inline">\(z_{i}\)</span> of the distilled model. If the cumbersome model has logits <span class="math inline">\(v_{i}\)</span> which produce soft target probabilities <span class="math inline">\(p_{i}\)</span> and the transfer training is done at a temperature of <span class="math inline">\(T,\)</span> this gradient is given by:</p>
<p>每个样本会得到一个交叉熵的梯度，<span class="math inline">\(d C / d z_{i}\)</span>表示低i个logit，<span class="math inline">\(z_{i}\)</span>表示蒸馏模型。复杂模型在某个温度T下得到梯度计算公式如下</p>
<p><span class="math display">\[
\frac{\partial C}{\partial z_{i}}=\frac{1}{T}\left(q_{i}-p_{i}\right)=\frac{1}{T}\left(\frac{e^{z_{i} / T}}{\sum_{j} e^{z_{j} / T}}-\frac{e^{v_{i} / T}}{\sum_{j} e^{v_{j} / T}}\right)
\]</span></p>
<blockquote>
<p>If the temperature is high compared with the magnitude of the logits, we can approximate:</p>
</blockquote>
<p>温度比对数的大小高，可以近似得到</p>
<p><span class="math display">\[
\frac{\partial C}{\partial z_{i}} \approx \frac{1}{T}\left(\frac{1+z_{i} / T}{N+\sum_{j} z_{j} / T}-\frac{1+v_{i} / T}{N+\sum_{j} v_{j} / T}\right)
\]</span>
If we now assume that the logits have been zero-meaned separately for each transfer case so that <span class="math inline">\(\sum_{j} z_{j}=\sum_{j} v_{j}=0\)</span> Eq. 3 simplifies to:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial z_{i}} \approx \frac{1}{N T^{2}}\left(z_{i}-v_{i}\right)
\]</span></p>
<blockquote>
<p>So in the high temperature limit, distillation is equivalent to minimizing <span class="math inline">\(1 / 2\left(z_{i}-v_{i}\right)^{2},\)</span> provided the logits are zero-meaned separately for each transfer case. At lower temperatures, distillation pays much less attention to matching logits that are much more negative than the average. This is potentially advantageous because these logits are almost completely unconstrained by the cost function used for training the cumbersome model so they could be very noisy. On the other hand, the very negative logits may convey useful information about the knowledge acquired by the cumbersome model. Which of these effects dominates is an empirical question. We show that when the distilled model is much too small to capture all of the knowledege in the cumbersome model, intermediate temperatures work best which strongly suggests that ignoring the large negative logits can be helpful.</p>
</blockquote>
<p>若T很大，且logits分布的均值为0时，优化概率交叉熵和logits的平方差是等价的，因此学习软目标的交叉概率和学习logits在此时是等价的。</p>
<p>hinton这篇主要是介绍模型蒸馏的一个思路，蒸馏模型学习的是什么。也是后面研究的基础。
把多个模型的知识提炼给单个的模型，教师模型教学生模型。</p>
</div>
<div id="experiment" class="section level3" number="0.1.5">
<h3><span class="header-section-number">0.1.5</span> Experiment</h3>
<p>这篇文章的实验部分如下</p>
<blockquote>
<p>This net achieved 67 test errors whereas a smaller net with two hidden layers of 800 rectified linear hidden units and no regularization achieved 146 errors. But if the smaller net was regularized solely by adding the additional task of matching the soft targets produced by the large net at a temperature of 20 , it achieved 74 test errors. This shows that soft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations.</p>
</blockquote>
<p>在MNIST这个数据集上，先使用大的网络进行训练测试集错误67个，使用小网络训练测试集错误146个。加入soft targets到目标函数中相当于加入了正则项，测试集的错误的的个数降低到了74个。模型蒸馏确实是使模型的结果变好了</p>
<blockquote>
<p>Table 1 shows that, indeed, our distillation approach is able to extract more useful information from the training set than simply using the hard labels to train a single model. More than <span class="math inline">\(80 \%\)</span> of the improvement in frame classification accuracy achieved by using an ensemble of 10 models is transferred to the distilled model which is similar to the improvement we observed in our preliminary experiments on MNIST. The ensemble gives a smaller improvement on the ultimate objective of WER (on a 23K-word test set) due to the mismatch in the objective function, but again, the improvement in WER achieved by the ensemble is transferred to the distilled model.</p>
</blockquote>
<p>在speech recognition领域中
根据表1的实验结果来看，minst测试集在模型蒸馏的结果上面比baseline的效果是提升的，比10个模型做emsemble的结果略低，说明蒸馏模型确实可以学习到大模型的参数。</p>
<p><img src="figs/distilling_1.png" /></p>
</div>
<div id="结论" class="section level3" number="0.1.6">
<h3><span class="header-section-number">0.1.6</span> 结论</h3>
<blockquote>
<p>We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.</p>
</blockquote>
<p>本文展示了蒸馏的方式包括，从ensemble学习中进行知识蒸馏,让蒸馏模型能够无线接近于集成模型的结果和将大的正则化的模型转移到小模型上面。</p>
</div>
</div>
<div id="distilling-task-speciﬁc-knowledge-from-bert-into-simple-neural-networks" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> Distilling Task-Speciﬁc Knowledge from BERT into Simple Neural Networks</h2>
<p>原文：<a href="https://arxiv.org/abs/1903.12136">Distilling Task-Speciﬁc Knowledge from BERT into Simple Neural Networks</a></p>
<div id="文章知识点总览-1" class="section level3" number="0.2.1">
<h3><span class="header-section-number">0.2.1</span> 文章知识点总览</h3>
<p><strong>模型</strong>：使用单层的bilstm对bert进行蒸馏</p>
<p><strong>目标函数</strong>：教师网络和学生网络logits的mse</p>
<p><strong>模型结构</strong>：</p>
<p><img src="figs/distillingbert_2.png" /></p>
<p>模型效果对比：</p>
<p><img src="figs/distillingbert_3.png" /></p>
<p>下面对每部分要点进行详细解读~</p>
</div>
<div id="abstract" class="section level3" number="0.2.2">
<h3><span class="header-section-number">0.2.2</span> abstract</h3>
<blockquote>
<p>We propose to distill knowledge from BERT, a state-ofthe-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. <span class="citation">(Tang et al. 2019)</span></p>
</blockquote>
<p>这篇文章是对bert蒸馏蒸馏到单层的BiLSTM。也就是之前提到的其中一种蒸馏方式用小的模型对复杂的大模型进行蒸馏。</p>
</div>
<div id="introduction-1" class="section level3" number="0.2.3">
<h3><span class="header-section-number">0.2.3</span> Introduction</h3>
<blockquote>
<p>Our motivation is twofold: we question whether a simple architecture actually lacks representation power for text modeling, and we wish to study effective approaches to transfer knowledge from BERT to a BiLSTM. Concretely, we leverage the knowledge distillation approach (Ba and Caruana, 2014; Hinton et al.,
2015)<span class="math inline">\(,\)</span> where a larger model serves as a teacher and a small model learns to mimic the teacher as a student. This approach is model agnostic, making knowledge transfer possible between BERT and a different neural architecture, such as a single-layer BiLSTM, in our case.</p>
</blockquote>
<p>本文是使用的单层的BiLSTM对bert进行蒸馏，原因有2：作者质疑一个简单的架构是否真的缺乏文本建模的表示能力并且希望能够让BiLSTM更好的学习到bert的性能。具体来说就是hinton提出的模型蒸馏的方式，大的模型作为teacher小模型学着模仿老师当学生。这种方法是模型不可知的，使知识在BERT和不同的神经结构之间转移成为可能，比如本文的单层的BiLSTM。</p>
<blockquote>
<p>To facilitate effective knowledge transfer, however, we often require a large, unlabeled dataset. The teacher model provides the probability logits and estimated labels for these unannotated samples, and the student network learns from the teacher’s outputs. In computer vision, unlabeled images are usually easy to obtain through augmenting the data using rotation, additive noise,and other distortions. However, obtaining additional, even unlabeled samples for a specific task can be difficult in NLP. Traditional data augmentation in NLP is typically task-specific (Wang and Eisner, 2016; Serban et al., 2016) and difficult to extend to other NLP tasks. To this end, we further propose a novel, rule-based textual data augmentation approach for constructing the knowledge transfer set.</p>
</blockquote>
<p>迁移学习一般时在一个非常大的无label的数据集上面。教师模型为这些未标注的样本提供了probability logits和估计标签，学生网络从教师网络的输出中学习。</p>
<p>在cv中，通过使用旋转、加性噪声和其他畸变来进行数据增强是比较容易的。
然而，在NLP任务中做数据增强是相对困难的。传统的NLP数据增强是典型的任务特定的(Wang and Eisner, 2016;Serban等人，2016)，难以扩展到其他NLP任务。为此，本文进一步提出了一种新的基于规则的文本数据增强方法来构建知识转移集。</p>
<blockquote>
<p>We evaluate our approach on three tasks in sentence classification and sentence matching. Experiments show that our knowledge distillation procedure significantly outperforms training the original simpler network alone.</p>
</blockquote>
<p>在三个文本分类任务和文本匹配上评估本文的方法。实验表明本文的知识蒸馏过程明显优于单独训练原始的简单网络。</p>
<blockquote>
<p>With our approach, a shallow BiLSTMbased model achieves results comparable to Embeddings from Language Models (ELMo; Peters et al., 2018 ), but uses around 100 times fewer parameters and performs inference 15 times faster. Therefore, our model becomes a state-of-the-art “small” model for neural NLP.</p>
</blockquote>
<p>本文的基于bilstm的浅层模型可以达到语言模型(ELMo)的效果，但使用的参数约少100倍，推理速度快15倍。因此，本文的模型成为最先进的“小”模型的神经自然语言处理。</p>
</div>
<div id="approach" class="section level3" number="0.2.4">
<h3><span class="header-section-number">0.2.4</span> Approach</h3>
<blockquote>
<p>First, we choose the desired teacher and student models for the knowledge distillation approach. Then, we describe our distillation procedure, which comprises two major components: first, the addition of a logits-regression objective, and second, the construction of a transfer dataset, which augments the training set for more effective knowledge transfer.</p>
</blockquote>
<p>首先，我们选择所需的教师和学生模型的知识提炼方法。然后，我们描述了我们的蒸馏过程，这包括两个主要组成部分:
- 添加一个logits-回归 作为目标函数。</p>
<ul>
<li>构建一个迁移数据集，以增强训练集，以实现更有效的知识迁移。</li>
</ul>
<p>每一层的含义解释的还是非常清楚的</p>
</div>
<div id="model-architecture" class="section level3" number="0.2.5">
<h3><span class="header-section-number">0.2.5</span> Model Architecture</h3>
<p>模型结果详细描述
&gt;For the teacher network, we use the pretrained, fine-tuned BERT (Devlin et al., 2018) model,</p>
<p>教师模型使用的是预训练的bert</p>
<blockquote>
<p>In contrast, our student model is a single-layer BiLSTM with a non-linear classifier. After feeding the input word embeddings into the BiLSTM, the hidden states of the last step in each direction are concatenated and fed to a fully connected layer with rectified linear units (ReLUs), whose output is then passed to a softmax layer for classification (Figure 1). For sentence-pair tasks, we share BiLSTM encoder weights in a siamese architecture between the two sentence encoders, producing sentence vectors <span class="math inline">\(\boldsymbol{h}_{s 1}\)</span> and <span class="math inline">\(\boldsymbol{h}_{s 2}\)</span> (Figure 2). We then apply a standard concatenate-compare operation (Wang et al., 2018) between the two sentence vectors: <span class="math inline">\(f\left(\boldsymbol{h}_{s 1}, \boldsymbol{h}_{s 2}\right)=\left[\boldsymbol{h}_{s 1}, \boldsymbol{h}_{s 2}, \boldsymbol{h}_{s 1} \odot\right.\)</span>
<span class="math inline">\(\left.\boldsymbol{h}_{s 2},\left|\boldsymbol{h}_{s 1}-\boldsymbol{h}_{s 2}\right|\right],\)</span> where <span class="math inline">\(\odot\)</span> denotes elementwise
multiplication. We feed this output to a ReLU-activated classifier.</p>
</blockquote>
<p>学生模型是具有非线性分类器的单层的BiLSTM。输入词嵌入到BiLSTM后，最后一步在每个方向上的隐藏状态被连接并馈送到带有整流线性单元(ReLUs)的全连接层，其输出随后被传递到softmax层进行分类(图1)。</p>
<p><img src="figs/distillingbert.png" /></p>
<p>对于句子对的任务，本文是在一个句子对的encoder中共享孪生BiLSTM encoder的权重，处理成句子两个句子向量 <span class="math inline">\(\boldsymbol{h}_{s 1}\)</span> and <span class="math inline">\(\boldsymbol{h}_{s 2}\)</span>，具体如图2所示</p>
<p><img src="figs/distillingbert_2.png" />
孪生的bilstm对句子匹配问题共享encoder 的权重，其余每个符号看具体的解释。</p>
<blockquote>
<p>It should be emphasized that we restrict the architecture engineering to a minimum to revisit the representation power of BiLSTM itself. We avoid any additional tricks, such as attention and layer normalization.</p>
</blockquote>
<p>严格的使用bilstm结构，没有多余的attention和layernorm</p>
</div>
<div id="distillation-objective" class="section level3" number="0.2.6">
<h3><span class="header-section-number">0.2.6</span> Distillation Objective</h3>
<p>本文的目标函数</p>
<blockquote>
<p>The distillation objective is to penalize the mean-squared-error (MSE) loss between the student network’s logits against the teacher’s logits:</p>
</blockquote>
<p>蒸馏模型的目标函数是教师网络logits和学生网络logits的mse，公示如下：</p>
<p><span class="math display">\[
\mathcal{L}_{\text {distill }}=\left\|\boldsymbol{z}^{(B)}-\boldsymbol{z}^{(S)}\right\|_{2}^{2}
\]</span>
- [ ] 这里为什么用mse，而不是kl散度，kl三度不是更能衡量两个网络的相似性嘛？</p>
<blockquote>
<p>where <span class="math inline">\(z^{(B)}\)</span> and <span class="math inline">\(z^{(S)}\)</span> are the teacher’s and student’s
logits, respectively.</p>
</blockquote>
<p><span class="math inline">\(z^{(B)}\)</span> 和 <span class="math inline">\(z^{(S)}\)</span> 分别是教师网络和学生网络的logits值</p>
<p>Other measures such as cross entropy with soft targets are viable as well (Hinton et al., 2015 ); however, in our preliminary experiments, we found MSE to perform slightly better. At training time, the distilling objective can be used in conjunction with a traditional crossentropy loss against a one-hot label <span class="math inline">\(t,\)</span> given by</p>
<p>其他的测量方法，如带有软目标的交叉熵也是可行的(Hinton等人，2015);然而，本文的初步实验中发现MSE的表现稍好一些。在训练时，可以将蒸馏目标与传统的交叉熵损失结合使用，对一个热标签<span class="math inline">\(t，\)</span>给定</p>
<p><span class="math display">\[
\begin{array}{l}
\mathcal{L}=\alpha \cdot \mathcal{L}_{\mathrm{CE}}+(1-\alpha) \cdot \mathcal{L}_{\text {distill }} \\
=-\alpha \sum_{i} t_{i} \log y_{i}^{(S)}-(1-\alpha)\left\|\boldsymbol{z}^{(B)}-\boldsymbol{z}^{(S)}\right\|_{2}^{2}
\end{array}
\]</span></p>
<p>When distilling with a labeled dataset, the one-hot target <span class="math inline">\(t\)</span> is simply the ground-truth label. When distilling with an unlabeled dataset, we use the predicted label by the teacher, i.e., <span class="math inline">\(t_{i}=1\)</span> if <span class="math inline">\(i=\operatorname{argmax} \boldsymbol{y}^{(B)}\)</span> and 0 otherwise.</p>
<p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。
可以对待训练的Student网络也如此处理</p>
</div>
<div id="data-augmentation-for-distillation" class="section level3" number="0.2.7">
<h3><span class="header-section-number">0.2.7</span> Data Augmentation for Distillation</h3>
<p>蒸馏过程中的数据增强方法</p>
<p>原因是解决标注样本过少的问题，教师网络进行蒸馏的数据量不够</p>
<ul>
<li><p>Masking 使用bert的随机mask进行数据增强（nlpcda那个包中有这个）</p></li>
<li><p>POS-guided word replacement. 新词替换（可能会是语义发生变化）</p></li>
<li><p>n-gram sampling.采样（这个感觉中文不适用啊）</p></li>
</ul>
</div>
<div id="对照实验部分" class="section level3" number="0.2.8">
<h3><span class="header-section-number">0.2.8</span> 对照实验部分</h3>
<p>4个数据集都是常见的文本分类数据集</p>
<blockquote>
<p>We present the results of our models as well as baselines in Table <span class="math inline">\(1 .\)</span> For <span class="math inline">\(\mathrm{QQP},\)</span> we report both <span class="math inline">\(\mathrm{F}_{1}\)</span> and accuracy, since the dataset is slightly unbalanced. Following GLUE, we report the average score of each model on the datasets.</p>
</blockquote>
<p>对比的是F1 和accuracy值，因为样本稍微有些不平衡，从四个数据集上面看，蒸馏模型的效果确实不错，都达到了sota</p>
<p><img src="figs/distillingbert_3.png" /></p>
<p>其他的似乎什么重点了,也不知道数据增强之前和之后的对比实验。只给了增强之后的实验。</p>
</div>
</div>
<div id="patient-knowledge-distillation-for-bert-model-compression" class="section level2" number="0.3">
<h2><span class="header-section-number">0.3</span> Patient Knowledge Distillation for BERT Model Compression</h2>
<p>原文：<a href="https://arxiv.org/pdf/1908.09355.pdf">Patient Knowledge Distillation for BERT Model Compression</a></p>
<div id="文章知识点总览-2" class="section level3" number="0.3.1">
<h3><span class="header-section-number">0.3.1</span> 文章知识点总览</h3>
<p>提出一种Patient Knowledge Distillation方式</p>
<p>训练了一个非常有“耐心” 的学生网络模型，能够学习教师网络的多层知识，不只是最后几层或者前几层，学习的足够充分。</p>
<p><strong>模型结构</strong>：Patient student，蒸馏bert网络的中间层以及最后的输出层，多层蒸馏</p>
<p><img src="figs/distillbert-pk_1.png" /></p>
<p><strong>目标函数</strong>：</p>
<p><span class="math display">\[
L_{P K D}=(1-\alpha) L_{C E}^{s}+\alpha L_{D S}+\beta L_{P T}
\]</span></p>
<p>是向教师网络学习与向正确样本学习以及教师网络的而外的训练损失的加权和，由三部分组成</p>
<ul>
<li><p><span class="math inline">\(L_{C E}\)</span>是学习标注样本的交叉熵</p></li>
<li><p><span class="math inline">\(L_{D S}\)</span>是学习教师网路的交叉熵</p></li>
<li><p><span class="math inline">\(L_{P T}\)</span>教师网络额外的训练损失</p></li>
</ul>
<p><strong>模型结果对比</strong></p>
<p>对中间层进行蒸馏的效果是要好于只对教师网络输出层蒸馏的效果的</p>
<p>直接看下面的实验分析</p>
</div>
<div id="abstract-1" class="section level3" number="0.3.2">
<h3><span class="header-section-number">0.3.2</span> abstract</h3>
<blockquote>
<p>In order to alleviate this resource hunger in large-scale model training,we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). <span class="citation">(Sun et al. 2019)</span></p>
</blockquote>
<p>例如像bert这种重量级的模型是非常吃算力的，因此本文提出了Patient知识蒸馏的方法来压缩原始的原始的庞大的教师网络模型变成一个轻量级的学生模型。</p>
<blockquote>
<p>Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies:</p>
</blockquote>
<p>本文的方法和之前的蒸馏模型的不同之处在于，之前的蒸馏模型（比如hinton的）只对教师网络的输出层进行蒸馏，本文的学生网络会还会耐心的学习教师网络的中间层进行知识的提取。主要方法如下：</p>
<blockquote>
<ol style="list-style-type: lower-roman">
<li>PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers.</li>
</ol>
</blockquote>
<p>PKD-Last：学习最后K层的知识，PKD-Skip：对中间层每隔k层学习一次</p>
<blockquote>
<p>These two patient distillation schemes enable the exploitation of rich information in the teacher’s hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process.</p>
</blockquote>
<p>这两种蒸馏的方式能够充分的学习到教师网络的隐藏层信息，并且能过够通过多层蒸馏的方式模拟出教师网络，从而激发学生网络的“耐心”（个人感觉就是增加学生网络的参数）</p>
<ul>
<li>所有标注数据都叫做ground truth</li>
</ul>
</div>
<div id="introduction-2" class="section level3" number="0.3.3">
<h3><span class="header-section-number">0.3.3</span> Introduction</h3>
<blockquote>
<p>In our approach,the teacher model outputs probability logits and predicts labels for the training samples (extendable to additional unannotated samples), and the student model learns from the teacher network to mimic the teacher’s prediction.</p>
</blockquote>
<p>本文教师网络输出的是probability logits，并且对训练样本进行标签预测(应该用来做数据增强的)，学生模型用来模拟老师的预测标签。</p>
<blockquote>
<p>we adopt a patient learning mechanism: instead of learning parameters from only the last layer of the teacher,we encourage the student model to extract knowledge also from previous layers of the teacher network. We call this ‘Patient Knowledge Distillation’. This patient learner has the advantage of distilling rich information through the deep structure of the teacher network for multi-layer knowledge distillation.</p>
</blockquote>
<p>本文采用了一种patient学习机制，激励学生模型从教师网络模型的多层提取知识而不只是最后一层。</p>
</div>
<div id="patient-knowledge-distillation" class="section level3" number="0.3.4">
<h3><span class="header-section-number">0.3.4</span> Patient Knowledge Distillation</h3>
<blockquote>
<p><strong>Problem Definition</strong> The original large teacher network is represented by a function <span class="math inline">\(f(\mathbf{x} ; \theta)\)</span>, where <span class="math inline">\(\mathrm{x}\)</span> is the input to the network, and <span class="math inline">\(\theta\)</span> denotes the model parameters. The goal of knowledge distillation is to learn a new set of parameters <span class="math inline">\(\theta^{\prime}\)</span> for a shallower student network <span class="math inline">\(g\left(\mathbf{x} ; \theta^{\prime}\right),\)</span> such that the student network achieves similar performance to the teacher, with much lower computational cost. Our strategy is to force the student model to imitate outputs from the teacher model on the training dataset with a defined objective <span class="math inline">\(L_{K D}\)</span>.</p>
</blockquote>
<p><span class="math inline">\(f(\mathbf{x} ; \theta)\)</span>表示大的教师模型，<span class="math inline">\(\mathrm{x}\)</span> 是模型的输入，<span class="math inline">\(\theta\)</span> 表示模型参数。知识蒸馏的目标是让浅层的学生网络<span class="math inline">\(g\left(\mathbf{x} ; \theta^{\prime}\right)\)</span>学习到一组新的参数<span class="math inline">\(\theta^{\prime}\)</span>
学生网络消耗的算力是非常的低的。<span class="math inline">\(L_{K D}\)</span>是本文的目标函数。</p>
<p><strong>目标函数</strong></p>
<blockquote>
<p>Assume <span class="math inline">\(\left\{\mathbf{x}_{i}, \mathbf{y}_{i}\right\}_{i=1}^{N}\)</span> are <span class="math inline">\(N\)</span> training samples, where <span class="math inline">\(\mathbf{x}_{i}\)</span> is the <span class="math inline">\(i\)</span> -th input instance for BERT, and <span class="math inline">\(\mathbf{y}_{i}\)</span> is the corresponding ground-truth label. BERT first computes a contextualized embedding <span class="math inline">\(\mathbf{h}_{i}=\operatorname{BERT}\left(\mathbf{x}_{i}\right) \in \mathbb{R}^{d} .\)</span> Then, a softmax layer <span class="math inline">\(\hat{\mathbf{y}}_{i}=P\left(\mathbf{y}_{i} \mid \mathbf{x}_{i}\right)=\operatorname{softmax}\left(\mathbf{W h}_{i}\right)\)</span> for classifica-
tion is applied to the embedding of BERT output, where <span class="math inline">\(\mathbf{W}\)</span> is a weight matrix to be learned.</p>
</blockquote>
<p>假设<span class="math inline">\(\left\{\mathbf{x}_{i},\mathbf{y}_{i}\right\}_{i=1}^{N}\)</span>是N个训练样本，<span class="math inline">\(\mathbf{x}_{i}\)</span>是bert的低i个输入，<span class="math inline">\(\mathbf{y}_{i}\)</span>是第i个ground-truth label，bert首先计算上下文的词向量
<span class="math inline">\(\mathbf{h}_{i}=\operatorname{BERT}\left(\mathbf{x}_{i}\right) \in \mathbb{R}^{d}\)</span> ，<span class="math inline">\(\hat{\mathbf{y}}_{i}=P\left(\mathbf{y}_{i} \mid \mathbf{x}_{i}\right)=\operatorname{softmax}\left(\mathbf{W h}_{i}\right)\)</span>是softmax层得到的分类输出。<span class="math inline">\(\mathbf{W}\)</span>是学到的参数</p>
<p>To apply knowledge distillation, first we need to train a teacher network. For example, to train a 12-layer BERT-Base as the teacher model, the learned parameters are denoted as:</p>
<p>首先是对12层的BERT-Base进行训练，得到的参数定义如下。</p>
<p><span class="math display">\[
\hat{\theta}^{t}=\arg \min _{\theta} \sum_{i \in[N]} L_{C E}^{t}\left(\mathbf{x}_{i}, \mathbf{y}_{i} ;\left[\theta_{\mathrm{BERT}_{12}}, \mathbf{W}\right]\right)
\]</span></p>
<blockquote>
<p>where the superscript <span class="math inline">\(t\)</span> denotes parameters in the teacher model, <span class="math inline">\([N]\)</span> denotes set <span class="math inline">\(\{1,2, \ldots, N\}\)</span> <span class="math inline">\(L_{C E}^{t}\)</span> denotes the cross-entropy loss for the teacher training, and <span class="math inline">\(\theta_{\mathrm{BERT}_{12}}\)</span> denotes parameters of <span class="math inline">\(\mathrm{BERT}_{12}\)</span></p>
</blockquote>
<p><span class="math inline">\(t\)</span> 是教师网络的参数，<span class="math inline">\(\{1,2, \ldots, N\}\)</span> <span class="math inline">\(L_{C E}^{t}\)</span> 教师网络训练的交叉熵损失函数，<span class="math inline">\(\theta_{\mathrm{BERT}_{12}}\)</span>表示 <span class="math inline">\(\mathrm{BERT}_{12}\)</span>的参数</p>
<p>The output probability for any given input <span class="math inline">\(\mathbf{x}_{i}\)</span> can be formulated as:</p>
<p>对于每个输入的输出概率公式如下</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mathbf{y}}_{i} &amp;=P^{t}\left(\mathbf{y}_{i} \mid \mathbf{x}_{i}\right)=\operatorname{softmax}\left(\frac{\mathbf{W h}_{i}}{T}\right) \\
&amp;=\operatorname{softmax}\left(\frac{\mathbf{W} \cdot \operatorname{BERT}_{12}\left(\mathbf{x}_{i} ; \hat{\theta}^{t}\right)}{T}\right)
\end{aligned}
\]</span></p>
<blockquote>
<p>where <span class="math inline">\(P^{t}(\cdot \mid \cdot)\)</span> denotes the probability output from the teacher. <span class="math inline">\(\hat{\mathbf{y}}_{i}\)</span> is fixed as soft labels, and <span class="math inline">\(T\)</span> is the temperature used in KD, which controls how much to rely on the teacher’s soft predictions. A higher temperature produces a more diverse probability distribution over classes (Hinton et al., 2015). Similarly, let <span class="math inline">\(\theta^{s}\)</span> denote parameters to be learned for the student model, and <span class="math inline">\(P^{s}(\cdot \mid \cdot)\)</span> denote the corresponding probability output from the student model. Thus, the distance between the teacher’s prediction and the student’s prediction can be defined as:</p>
</blockquote>
<p><span class="math inline">\(P^{t}(\cdot \mid \cdot)\)</span>表示教师网络的输出概率，<span class="math inline">\(\hat{\mathbf{y}}_{i}\)</span> 是软label，T是KD的温度参数，决定了在多大程度上依赖于老师网络的软预测。温度越高，分类的概率分布就越多样化，<span class="math inline">\(\theta^{s}\)</span>是学生模型学习的参数，<span class="math inline">\(P^{s}(\cdot \mid \cdot)\)</span>是学生模型的输出概率，因此：教师的预测与学生的预测之间的距离可以定义为:</p>
<p><span class="math display">\[
\begin{array}{r}
L_{D S}=-\sum_{i \in[N]} \sum_{c \in C}\left[P^{t}\left(\mathbf{y}_{i}=c \mid \mathbf{x}_{i} ; \hat{\theta}^{t}\right)\right. \\
\left.\log P^{s}\left(\mathbf{y}_{i}=c \mid \mathbf{x}_{i} ; \theta^{s}\right)\right]
\end{array}
\]</span></p>
<p>这个是学生网络像教师网络学习损失函数</p>
<blockquote>
<p>where <span class="math inline">\(c\)</span> is a class label and <span class="math inline">\(C\)</span> denotes the set of class labels.</p>
</blockquote>
<p><span class="math inline">\(c\)</span>是一个分类标签，<span class="math inline">\(C\)</span>是一个类别的标签</p>
<blockquote>
<p>Besides encouraging the student model to imitate the teacher’s behavior, we can also fine-tune the student model on target tasks, where taskspecific cross-entropy loss is included for model training:</p>
</blockquote>
<p>学生网络除了向教师网络学习之外，还想要像正确的标注数据学习，模型训练中包含了特定任务的交叉熵损失:</p>
<p><span class="math display">\[
\begin{aligned}
L_{C E}^{s}=-\sum_{i \in[N]} &amp; \sum_{c \in C}\left[\mathbb{1}\left[\mathbf{y}_{i}=c\right]\right.\\
&amp;\left.\log P^{s}\left(\mathbf{y}_{i}=c \mid \mathbf{x}_{i} ; \theta^{s}\right)\right]
\end{aligned}
\]</span></p>
<p>Thus, the final objective function for knowledge distillation can be formulated as:</p>
<p>最终本文的目标函数如下，是教师网络和学生网络的距离损失和与真实数据交叉熵的加权和</p>
<p><span class="math display">\[
L_{K D}=(1-\alpha) L_{C E}^{s}+\alpha L_{D S}
\]</span></p>
<p>模型结构如图</p>
<p><img src="figs/distillbert-pk_1.png" /></p>
<p>a图是每两层学习教师模型的一次输出叫PKD-Skip,b图是学生网络学习教师模型最后6层,也叫PKD-last，Trm：transformer。</p>
<blockquote>
<p>Learning from the hidden states of all the tokens is computationally expensive, and may introduce noise. In the original BERT implementation (Devlin et al., 2018), prediction is performed by only using the output from the last layer’s [CLS ] token. In some variants of BERT, like SDNet (Zhu et al., 2018 ), a weighted average of all layers’ [CLS ] embeddings is applied.</p>
</blockquote>
<p>因为从隐藏层学习表征非常的消耗算力，而且有可能会产生噪音。在最初的BERT实现中，预测仅通过使用最后一层的[CLS]的token输出来执行。在BERT的一些变体中，如SDNet，应用了所有层[CLS]嵌入的加权平均。</p>
<blockquote>
<p>In general, the final logit can be computed based on <span class="math inline">\(\mathbf{h}_{\text {final }}=\)</span> <span class="math inline">\(\sum_{j \in[k]} w_{j} \mathbf{h}_{j},\)</span> where <span class="math inline">\(w_{j}\)</span> could be either learned parameters or a pre-defined hyper-parameter, <span class="math inline">\(\mathbf{h}_{j}\)</span> is the embedding of [CLS] from the hidden layer <span class="math inline">\(j,\)</span> and <span class="math inline">\(k\)</span> is the number of hidden layers.</p>
</blockquote>
<p>通常最后几层的logit能够根据<span class="math inline">\(\mathbf{h}_{\text {final }}=\)</span> <span class="math inline">\(\sum_{j \in[k]} w_{j} \mathbf{h}_{j}\)</span>， <span class="math inline">\(w_{j}\)</span> 是可以被学习到的， <span class="math inline">\(\mathbf{h}_{j}\)</span>是[CLS]隐藏层的参数，k表示隐藏层的数量。</p>
<blockquote>
<p>Derived from this, if the compressed model can learn from the representation of [ <span class="math inline">\(\mathrm{CLS}\)</span> ] in the teacher’s intermediate layers for any given input, it has the potential of gaining a generalization ability similar to the teacher model.</p>
</blockquote>
<p>由此可知，对于任何给定的输入，如果压缩模型能够从教师中间层[<span class="math inline">\(\mathrm{CLS}\)</span>]的表示中学习，那么它就有可能获得类似于教师模型的泛化能力。</p>
<blockquote>
<p>Motivated by this, in our Patient-KD framework, the student is cultivated to imitate the representations only for the [CLS] token in the intermediate layers, following the intuition aforementioned that the [CLS] token is important in predicting the final labels. For an input <span class="math inline">\(\mathbf{x}_{i},\)</span> the outputs of the [CLS ] tokens for all the layers are denoted
as:</p>
</blockquote>
<p>基于此，本文的Patient-KD框架中，学生网络是仅模仿中间层中的[CLS]标记的表征，遵循前面提到的[CLS]标记在预测最终标签中很重要的直觉。对于输入<span class="math inline">\(\mathbf{x}_{i}，\)</span>所有层的[CLS]令牌的输出表示为:</p>
<p><span class="math display">\[
\mathbf{h}_{i}=\left[\mathbf{h}_{i, 1}, \mathbf{h}_{i, 2}, \ldots, \mathbf{h}_{i, k}\right]=\operatorname{BERT}_{k}\left(\mathbf{x}_{i}\right) \in \mathbb{R}^{k \times d}
\]</span></p>
<blockquote>
<p>We denote the set of intermediate layers to distill knowledge from as <span class="math inline">\(I_{p t}\)</span>.Take distilling from BERT <span class="math inline">\(_{12}\)</span> to <span class="math inline">\(\operatorname{BERT}_{6}\)</span> as an example. For the PKDSkip strategy, <span class="math inline">\(I_{p t}=\{2,4,6,8,10\} ;\)</span> and for the PKD-Last strategy, <span class="math inline">\(I_{p t}=\{7,8,9,10,11\} .\)</span> Note that <span class="math inline">\(k=5\)</span> for both cases, because the output from the last layer (e.g., Layer 12 for BERT-Base) is omitted since its hidden states are connected to the
softmax layer, which is already included in the KD loss defined in Eqn. (5). In general, for BERT student with <span class="math inline">\(n\)</span> layers, <span class="math inline">\(k\)</span> always equals to <span class="math inline">\(n-1\)</span>. The additional training loss introduced by the patient teacher is defined as the mean-square loss between the normalized hidden states:</p>
</blockquote>
<p>将进行知识蒸馏的中间层表示为 <span class="math inline">\(I_{p t}\)</span>，以将12层bert蒸馏为6层为例。对于中间层的蒸馏策略PKDSkip有<span class="math inline">\(I_{p t}=\{2,4,6,8,10\}\)</span>,对于最后几层的蒸馏策略<span class="math inline">\(I_{p t}=\{7,8,9,10,11\}\)</span>。隐藏层的个数是5，适用于两种情况。因为最后一层的输出(例如，BERT-Base的第12层)被省略，因为它的隐藏状态连接到softmax层，它已经包含在公式（5）中定义的KD损失中。一般来说，对于有<span class="math inline">\(n\)</span>层的BERT学生，<span class="math inline">\(k\)</span>总是等于<span class="math inline">\(n-1\)</span>。由有“耐心”的老师引入的额外训练损失定义为归一化隐藏状态之间的均方损失:</p>
<p><span class="math display">\[
L_{P T}=\sum_{i=1}^{N} \sum_{j=1}^{M}\left\|\frac{\mathbf{h}_{i, j}^{s}}{\left\|\mathbf{h}_{i, j}^{s}\right\|_{2}}-\frac{\mathbf{h}_{i, I_{p t}(j)}^{t}}{\left\|\mathbf{h}_{i, I_{p t}(j)}^{t}\right\|_{2}}\right\|_{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(M\)</span> denotes the number of layers in the student network, <span class="math inline">\(N\)</span> is the number of training samples, and the superscripts <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span> in <span class="math inline">\(\mathbf{h}\)</span> indicate the student and the teacher model, respectively. Combined with the KD loss introduced in Section 3.1 the final objective function can be formulated as:</p>
<p><span class="math inline">\(M\)</span>表示学生网络的层数，<span class="math inline">\(N\)</span>是训练样本数，<span class="math inline">\(\mathbf{h}\)</span>中的上标<span class="math inline">\(s\)</span>和<span class="math inline">\(t\)</span>分别表示学生模型和教师模型。结合上面介绍的KD损失，最终目标函数可表示为:</p>
<p><span class="math display">\[
L_{P K D}=(1-\alpha) L_{C E}^{s}+\alpha L_{D S}+\beta L_{P T}
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is another hyper-parameter that weights the importance of the features for distillation in the intermediate layers.</p>
</div>
<div id="实验部分" class="section level3" number="0.3.5">
<h3><span class="header-section-number">0.3.5</span> 实验部分</h3>
<p>数据集包含情感分类任务，自然语言推理任务,相似度匹配，问答任务，文本识别任务等</p>
<blockquote>
<p>For Sentiment Classification, we test on Stanford Sentiment Treebank (SST-2) (Socher et al., 2013). For Paraphrase Similarity Matching, we use Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005 ) and Quora Question Pairs (QQP) <span class="math inline">\(^{2}\)</span> datasets. For Natural Language Inference, we evaluate on Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2017), QNLI <span class="math inline">\(^{3}\)</span> (Rajpurkar et al., 2016 ), and Recognizing Textual Entailment (RTE).</p>
</blockquote>
<p><img src="figs/distillbert-pk_2.png" /></p>
<p>从表1的结果中能够看出多层的方法在除了MRPC之外的任务上都能达到比较最好的效果</p>
<p><img src="figs/distillbert-pk_3.png" /></p>
<p>从图2的效果上来看，pkd的效果是好于kd的效果的，也就是说加上了对中间层的蒸馏效果更好。</p>
<p><img src="figs/distillbert-pk_4.png" /></p>
<p>PKD-SKIP的效果是好于PKD-LAST的</p>
<p><img src="figs/distillbert-pk_5.png" /></p>
<p>在RACE数据集上，多层的PKD-skip效果也是最接近于teacher的</p>
<p><img src="figs/distillbert-pk_6.png" /></p>
<p>从表5上看，增加bert到bert-large作为teacher进行蒸馏， 得到的学生模型的效果并没有很大的提升。</p>
<p>为什么？</p>
</div>
</div>
<div id="distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter" class="section level2" number="0.4">
<h2><span class="header-section-number">0.4</span> DistilBERT a distilled version of BERT smaller faster cheaper and lighter</h2>
<p>原文：<a href="https://arxiv.org/pdf/1908.09355.pdf">DistilBERT, a distilled version of BERT: smaller,
faster, cheaper and lighter</a></p>
<div id="文章知识点总览-3" class="section level3" number="0.4.1">
<h3><span class="header-section-number">0.4.1</span> 文章知识点总览</h3>
<p>文章很短，直接往下看吧</p>
</div>
<div id="abstract-2" class="section level3" number="0.4.2">
<h3><span class="header-section-number">0.4.2</span> abstract</h3>
<blockquote>
<p>While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by <span class="math inline">\(40 \%,\)</span> while retaining <span class="math inline">\(97 \%\)</span> of its language understanding capabilities and being <span class="math inline">\(60 \%\)</span> faster. <span class="citation">(Sanh et al. 2019)</span></p>
</blockquote>
<p>之前的蒸馏模型往往使用在特定的任务上面，本文在预训练阶段进行模型蒸馏，能使bert模型缩小40%，保留模型的97%的能力，速度提高了60%。（看上去不错！）</p>
<blockquote>
<p>To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses.</p>
</blockquote>
<p>为了利用大模型在训练前学到的归纳偏差，本文引入了结合语言建模、蒸馏和余弦距离损失的三重损失。（响起了trible kill）</p>
</div>
<div id="introdiction" class="section level3" number="0.4.3">
<h3><span class="header-section-number">0.4.3</span> introdiction</h3>
<p><img src="figs/DistilBERT_victor_1.png" /></p>
<p>这个图表示一些预训练模型的参数量级，可以看到DistilBERT这个量级对比其他预训练模型还是非常低的</p>
<blockquote>
<p>We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices.</p>
</blockquote>
<p>模型居然还能在一定设备上面跑</p>
</div>
<div id="distilbert-a-distilled-version-of-bert" class="section level3" number="0.4.4">
<h3><span class="header-section-number">0.4.4</span> DistilBERT a distilled version of BERT</h3>
<p>bert的蒸馏版本</p>
<blockquote>
<p>Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of <span class="math inline">\(2 .\)</span> Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers.</p>
</blockquote>
<p>student model 具有和Bert相同的结构，当模型层数减少1/2时，移除了Token type embedding 和pooler. Transformer 架构中线性层和LN层，可以充分地优化模型结构，本文研究表明最后一层张量维度的变化对计算效率影响 比层数变化要小，所以作者关注于减少模型层数。在初始化方面，作者从teacher model中每两层选择一层做初始化，蒸馏应用了Liu et al. [2019] 提出的 BERT 模型训练最佳实践。语料和 Bert 使用的一致。</p>
<p>本文的重点放在了减少网络层数上面，也是每两层</p>
<blockquote>
<p>Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.</p>
</blockquote>
<p>除了前面描述的优化和架构选择之外，本文训练过程中是为子网络寻找正确的初始化方法以使其收敛。利用教师网络和学生网络的共同维度，从教师网络中每两层抽取一层进行初始化，和上一篇中间层蒸馏是一样的。</p>
<p>每隔两层抽一层进行蒸馏，可以有效的减少模型的层数</p>
<p><img src="figs/DistilBERT_victor_2.png" /></p>
<p>表1的结果蒸馏效果确实还不错
表2的结果在下游任务上面也还可以，符合说的效果达到97%
表3是参数量级，差不多减少了40%吧比bert-base</p>
<ul>
<li>个人感觉这篇文章是上一篇的一部分，好像没什么创新，实验做的还算充分</li>
</ul>
</div>
</div>
<div id="参考文献" class="section level2 unnumbered" number="">
<h2>参考文献</h2>
<div id="refs" class="references hanging-indent">
<div>
<p>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network,” March.</p>
</div>
<div>
<p>Sanh, V., L. Debut, J. Chaumond, and T. Wolf. 2019. “DistilBERT, a Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter.”</p>
</div>
<div>
<p>Sun, S., Y. Cheng, Z. Gan, and J. Liu. 2019. “Patient Knowledge Distillation for Bert Model Compression.”</p>
</div>
<div>
<p>Tang, R., Y. Lu, L. Liu, L. Mou, O. Vechtomova, and J. Lin. 2019. “Distilling Task-Specific Knowledge from Bert into Simple Neural Networks.”</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
